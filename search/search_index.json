{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Credit: Randall Munroe (xkcd.com/2054) Welcome to Environmental Incentives' Metrics Service Line Wiki! This wiki is intended for all EI staff to better understand the services of the Metrics service line and for Metrics service line experts as an important standard reference. Don't build a giant house of cards! We can help you develop solutions that last. All EI Staff \u00b6 If you're here to better understand how the Metrics service line can support your project or internal initiative, you might want to check out our portfolio of projects , better understand how we can work with you , or review one of these helpful introductions to technologies and services available to you: Data Analysis Tool Development Databases Visualization Mapping and Spatial Analysis Metrics Service Line Experts \u00b6 Metrics service line experts will want to familiarize themselves with all of the content here. This site provides helpful training resources, captures best practices for developing metrics products, and describes our standard consulting process for internal and external clients. If you will be contributing to the development of this site, please reference this guidance . The Brown Bag \u00b6 If you missed the brown bag introduction to this wiki, check it out here .","title":"Home"},{"location":"#all-ei-staff","text":"If you're here to better understand how the Metrics service line can support your project or internal initiative, you might want to check out our portfolio of projects , better understand how we can work with you , or review one of these helpful introductions to technologies and services available to you: Data Analysis Tool Development Databases Visualization Mapping and Spatial Analysis","title":"All EI Staff"},{"location":"#metrics-service-line-experts","text":"Metrics service line experts will want to familiarize themselves with all of the content here. This site provides helpful training resources, captures best practices for developing metrics products, and describes our standard consulting process for internal and external clients. If you will be contributing to the development of this site, please reference this guidance .","title":"Metrics Service Line Experts"},{"location":"#the-brown-bag","text":"If you missed the brown bag introduction to this wiki, check it out here .","title":"The Brown Bag"},{"location":"development-guidance/","text":"EI Development Guidance \u00b6 This was produced with mkdocs. For full documentation visit mkdocs.org . Contents \u00b6 Home Portfolio Metrics Services Metrics Design Philosophy Project Planning Git Development Deployment (non-tech: how do we deliver products?) Data Management (non-tech: database overview) Data Science (non-tech: types of data analysis) Spatial Analysis (non-tech: types of spatial data analysis) Data Visualization (non-tech: visualization options) Packages News/Blog How to Maintain this Site Additional Resources Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Installations \u00b6 mkdocs and the material theme must be installed: pip install mkdocs pip install mkdocs-material","title":"Development Guide"},{"location":"development-guidance/#ei-development-guidance","text":"This was produced with mkdocs. For full documentation visit mkdocs.org .","title":"EI Development Guidance"},{"location":"development-guidance/#contents","text":"Home Portfolio Metrics Services Metrics Design Philosophy Project Planning Git Development Deployment (non-tech: how do we deliver products?) Data Management (non-tech: database overview) Data Science (non-tech: types of data analysis) Spatial Analysis (non-tech: types of spatial data analysis) Data Visualization (non-tech: visualization options) Packages News/Blog How to Maintain this Site Additional Resources","title":"Contents"},{"location":"development-guidance/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"development-guidance/#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"development-guidance/#installations","text":"mkdocs and the material theme must be installed: pip install mkdocs pip install mkdocs-material","title":"Installations"},{"location":"notes/","text":"Tips & Tricks \u00b6 Customs Used \u00b6 To identify a user-provided value, wrap it in <> within a code block: git commit <updated-file> Highlight package names, files, code, and cli commands in code blocks: rasterio Markdown Reference \u00b6 Here is a handy reference for writing in markdown, provided by Typora. Converting Notebooks \u00b6 To add a notebook to the site, us nbconvert to convert the notebook to markdown. Save the output with the desired filename and into a new directory. The new directory will include the markdown file and assets within a *_files directory. Cut and paste the new directory into the ei-dev docs/ folder in the desired location. Open the Anaconda Prompt window within the notebook's root directory Run the following command (a new directory will be created if needed): jupyter nbconvert <notebook.ipynb> --to markdown --output-dir='<rel path to new dirname>' --output <desired-filename> For example jupyter nbconvert my_notebook.ipynb --to markdown --output-dir='../md-notebooks' --output my_markdown Cut and paste the newly created directory into the ei-dev docs/ folder in the desired location. Syntax highlighting is done using the codehilite package (part of standard markdown library) and Pygments (part of standard Python library). However, codehilite must be enabled in the mkdocs.yml file. See documentation for more. Changes made to the notebook will not be reflected in the markdown files, and you'll have to overwrite the folder you created the first time. If you change the name of the outputs, update the mkdocs.yml folder. Linking To and Within Documents \u00b6 To link to another page, simply provide the relative path to the page as a markdown file. You can link to a section if you add a '#' after the filename. Sections should be all lower-case with '-' in place of spaces (make sure all headers are unique as well). Ignore any punctuation in the header. If in the same page, omit the page name. See also documentation . [here's a link to another document](foldername/filename.md#header-name) [here's a link to a head in this document](#header-name) Images and Gifs \u00b6 You can either link to images and gifs online or save locally. If saving locally, store in assets/ within the section directory (e.g., consulting-process/ ). The alternative text (within the square brackets) is read by accessibility services. See also documentation . Use web links whenever possible. ![here's an image](assets/gif.gif) YouTube Videos \u00b6 To include Youtube videos, use the iframe provided under the 'Share' option. Any HTML pasted directly within a markdown file will be rendered in the site. Forms \u00b6 Google forms (and probably many other web forms) can be embedded as HTML within an iframe. Extensions \u00b6 Markdown and Mkdocs both offer extensions for rendering markdown. Note that the extensions may need to be configured for Typora as well as Mkdocs for consistency between editing and serving. Material has a few extensions as well. Check these out in their project documentation pages to see if the functionality would be helpful. The site currently uses the Material theme extension. Some interesting extensions: Admonition \u00b6 Provides callout style boxes, including collapsible boxes, for example: Warning Don't do this really bad thing! For your information... ...I changed the title of this note. See documentation for more. Interactive Visualizations \u00b6 Maps, dashboards, and other interactive visualizations should be passed in using the iframe. See Plotly guidance here . HTML Tags \u00b6 Any HTML written in the document will get passed as pure HTML when served. If the standard markdown isn't sufficient, use pure HTML. For example, to get the credit for the comic on the home page to be right adjusted, I typed this line (don't include block quotes if you want the actual text rendered rather than the code) : 1 < div style = \"text-align: right\" > Credit: Randall Munroe (xkcd.com/2054) </ div > A few tags to consider: Details \u00b6 This tag allows for drop-down type hiding of content. 1 2 3 4 < detail > < summary > The text when collapsed goes here </ summary > < p > The text that gets hidden goes here </ p > </ detail > It's best to work within the source code mode ( ctrl+/ ), as Markdown treats this strangely if trying to pass markdown between the detail tags. Click me! This would be the first paragraph of content Here's some more content with a link Note that the details tag is styled by the extension admonition with additional styling. I couldn't prevent this behavior by removing the admonition extension from my yaml file, which may be a bug. However, adding custom CSS may help override this behavior. PyMdown \u00b6 A ton of functionality in this package of extensions , including highlighting text, inline comments and editing, math equations, and tasklists (though not interactive tasklists). JavaScript \u00b6 You can add custom JavaScript to MkDocs provided you're ok with the script running on every page. In the mkdocs.yml file, add a line that points to the location of the JavaScript file relative to the docs/ folder: 1 2 extra_javascript : - javascripts/scripts.js The script will automatically be included on every page of the document, no need to add a script tag. Here's an example that I've implemented on the site. I want all external links to open in a new window, and internal links to open in the same window. I could express each link in html, but it would be easier to update all links written in standard markdown (which doesn't have this functionality out-of-the-box). Here's the content of the JavaScript file: 1 2 3 4 5 6 7 var links = document . links ; for ( var i = 0 , linksLength = links . length ; i < linksLength ; i ++ ) { if ( links [ i ]. hostname != window . location . hostname ) { links [ i ]. target = '_blank' ; } } This script gets a list of all links in the document, checks whether the hostname is the same as the hostname of the current window (i.e., the hostname of my website), and then updates the target property of each link to \"_blank\" whenever the link is external. Note you can also add JavaScript (and CSS) by defining a custom theme . CSS \u00b6 See also JavaScript above, but basically just include in the mkdocs.yml file the following: 1 2 3 extra_css : - stylesheets/<my_css.css> - stylesheets/<more_css.css>","title":"Tips & Tricks"},{"location":"notes/#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"notes/#customs-used","text":"To identify a user-provided value, wrap it in <> within a code block: git commit <updated-file> Highlight package names, files, code, and cli commands in code blocks: rasterio","title":"Customs Used"},{"location":"notes/#markdown-reference","text":"Here is a handy reference for writing in markdown, provided by Typora.","title":"Markdown Reference"},{"location":"notes/#converting-notebooks","text":"To add a notebook to the site, us nbconvert to convert the notebook to markdown. Save the output with the desired filename and into a new directory. The new directory will include the markdown file and assets within a *_files directory. Cut and paste the new directory into the ei-dev docs/ folder in the desired location. Open the Anaconda Prompt window within the notebook's root directory Run the following command (a new directory will be created if needed): jupyter nbconvert <notebook.ipynb> --to markdown --output-dir='<rel path to new dirname>' --output <desired-filename> For example jupyter nbconvert my_notebook.ipynb --to markdown --output-dir='../md-notebooks' --output my_markdown Cut and paste the newly created directory into the ei-dev docs/ folder in the desired location. Syntax highlighting is done using the codehilite package (part of standard markdown library) and Pygments (part of standard Python library). However, codehilite must be enabled in the mkdocs.yml file. See documentation for more. Changes made to the notebook will not be reflected in the markdown files, and you'll have to overwrite the folder you created the first time. If you change the name of the outputs, update the mkdocs.yml folder.","title":"Converting Notebooks"},{"location":"notes/#linking-to-and-within-documents","text":"To link to another page, simply provide the relative path to the page as a markdown file. You can link to a section if you add a '#' after the filename. Sections should be all lower-case with '-' in place of spaces (make sure all headers are unique as well). Ignore any punctuation in the header. If in the same page, omit the page name. See also documentation . [here's a link to another document](foldername/filename.md#header-name) [here's a link to a head in this document](#header-name)","title":"Linking To and Within Documents"},{"location":"notes/#images-and-gifs","text":"You can either link to images and gifs online or save locally. If saving locally, store in assets/ within the section directory (e.g., consulting-process/ ). The alternative text (within the square brackets) is read by accessibility services. See also documentation . Use web links whenever possible. ![here's an image](assets/gif.gif)","title":"Images and Gifs"},{"location":"notes/#youtube-videos","text":"To include Youtube videos, use the iframe provided under the 'Share' option. Any HTML pasted directly within a markdown file will be rendered in the site.","title":"YouTube Videos"},{"location":"notes/#forms","text":"Google forms (and probably many other web forms) can be embedded as HTML within an iframe.","title":"Forms"},{"location":"notes/#extensions","text":"Markdown and Mkdocs both offer extensions for rendering markdown. Note that the extensions may need to be configured for Typora as well as Mkdocs for consistency between editing and serving. Material has a few extensions as well. Check these out in their project documentation pages to see if the functionality would be helpful. The site currently uses the Material theme extension. Some interesting extensions:","title":"Extensions"},{"location":"notes/#admonition","text":"Provides callout style boxes, including collapsible boxes, for example: Warning Don't do this really bad thing! For your information... ...I changed the title of this note. See documentation for more.","title":"Admonition"},{"location":"notes/#interactive-visualizations","text":"Maps, dashboards, and other interactive visualizations should be passed in using the iframe. See Plotly guidance here .","title":"Interactive Visualizations"},{"location":"notes/#html-tags","text":"Any HTML written in the document will get passed as pure HTML when served. If the standard markdown isn't sufficient, use pure HTML. For example, to get the credit for the comic on the home page to be right adjusted, I typed this line (don't include block quotes if you want the actual text rendered rather than the code) : 1 < div style = \"text-align: right\" > Credit: Randall Munroe (xkcd.com/2054) </ div > A few tags to consider:","title":"HTML Tags"},{"location":"notes/#details","text":"This tag allows for drop-down type hiding of content. 1 2 3 4 < detail > < summary > The text when collapsed goes here </ summary > < p > The text that gets hidden goes here </ p > </ detail > It's best to work within the source code mode ( ctrl+/ ), as Markdown treats this strangely if trying to pass markdown between the detail tags. Click me! This would be the first paragraph of content Here's some more content with a link Note that the details tag is styled by the extension admonition with additional styling. I couldn't prevent this behavior by removing the admonition extension from my yaml file, which may be a bug. However, adding custom CSS may help override this behavior.","title":"Details"},{"location":"notes/#pymdown","text":"A ton of functionality in this package of extensions , including highlighting text, inline comments and editing, math equations, and tasklists (though not interactive tasklists).","title":"PyMdown"},{"location":"notes/#javascript","text":"You can add custom JavaScript to MkDocs provided you're ok with the script running on every page. In the mkdocs.yml file, add a line that points to the location of the JavaScript file relative to the docs/ folder: 1 2 extra_javascript : - javascripts/scripts.js The script will automatically be included on every page of the document, no need to add a script tag. Here's an example that I've implemented on the site. I want all external links to open in a new window, and internal links to open in the same window. I could express each link in html, but it would be easier to update all links written in standard markdown (which doesn't have this functionality out-of-the-box). Here's the content of the JavaScript file: 1 2 3 4 5 6 7 var links = document . links ; for ( var i = 0 , linksLength = links . length ; i < linksLength ; i ++ ) { if ( links [ i ]. hostname != window . location . hostname ) { links [ i ]. target = '_blank' ; } } This script gets a list of all links in the document, checks whether the hostname is the same as the hostname of the current window (i.e., the hostname of my website), and then updates the target property of each link to \"_blank\" whenever the link is external. Note you can also add JavaScript (and CSS) by defining a custom theme .","title":"JavaScript"},{"location":"notes/#css","text":"See also JavaScript above, but basically just include in the mkdocs.yml file the following: 1 2 3 extra_css : - stylesheets/<my_css.css> - stylesheets/<more_css.css>","title":"CSS"},{"location":"under-construction/","text":"We're getting to it... \u00b6","title":"We're getting to it..."},{"location":"under-construction/#were-getting-to-it","text":"","title":"We're getting to it..."},{"location":"additional-resources/interview-guide/","text":"Interview Guide \u00b6 An editable interview guide is available here ; download a version of this before proceeding. Talking to real people will help us better understand our potential users and the problems they face in their efforts to meet our conservation targets. This guide is intended to help you plan and facilitate the interview. There are two sections to the interview: (1) understanding the persona and (2) understanding their problems. Within each section, a table with \u2018question topics\u2019 and \u2018example questions\u2019 is provided. Edit the example questions within each question topic prior to the interview. Use these questions as a guide, but feel free to improvise or skip questions as you see fit. Discussion during the interview should relate to our area of interest (conservation targets), but should not be leading. People are generally agreeable\u2014if you ask whether they think pollinators are important, they will probably say yes. Better to ask what their priorities are and see if they mention the problems or solutions you're testing. Towards the end of the interview, you may ask more leading questions to see why they did or did not mention our area of interest or value proposition in their earlier responses. Before conducting the interview, you should (1) develop a persona based on available data sources and (2) brainstorm the types of problems they would face in relation to your program scope, vision or conservation targets. To ensure your personas are well-developed, make sure they meet the following criteria: Real \u2013 based on actual evidence (secondary research, interviews, observations, collected data) Exact \u2013 includes sufficient detail that the persona \u2018feels\u2019 like a real person Actionable \u2013 represents people that would actually use the MBHE Clear \u2013 described well enough to be understood by others on the team Testable \u2013 evidence can be collected to substantiate and improve the persona over time Fill out the table in the interview guide for the persona to be interviewed to ensure your persona is ready and to brainstorm specific questions to ask.","title":"Interview Guide"},{"location":"additional-resources/interview-guide/#interview-guide","text":"An editable interview guide is available here ; download a version of this before proceeding. Talking to real people will help us better understand our potential users and the problems they face in their efforts to meet our conservation targets. This guide is intended to help you plan and facilitate the interview. There are two sections to the interview: (1) understanding the persona and (2) understanding their problems. Within each section, a table with \u2018question topics\u2019 and \u2018example questions\u2019 is provided. Edit the example questions within each question topic prior to the interview. Use these questions as a guide, but feel free to improvise or skip questions as you see fit. Discussion during the interview should relate to our area of interest (conservation targets), but should not be leading. People are generally agreeable\u2014if you ask whether they think pollinators are important, they will probably say yes. Better to ask what their priorities are and see if they mention the problems or solutions you're testing. Towards the end of the interview, you may ask more leading questions to see why they did or did not mention our area of interest or value proposition in their earlier responses. Before conducting the interview, you should (1) develop a persona based on available data sources and (2) brainstorm the types of problems they would face in relation to your program scope, vision or conservation targets. To ensure your personas are well-developed, make sure they meet the following criteria: Real \u2013 based on actual evidence (secondary research, interviews, observations, collected data) Exact \u2013 includes sufficient detail that the persona \u2018feels\u2019 like a real person Actionable \u2013 represents people that would actually use the MBHE Clear \u2013 described well enough to be understood by others on the team Testable \u2013 evidence can be collected to substantiate and improve the persona over time Fill out the table in the interview guide for the persona to be interviewed to ensure your persona is ready and to brainstorm specific questions to ask.","title":"Interview Guide"},{"location":"additional-resources/persona-guide/","text":"Persona Development Guide \u00b6 Personas include (1) a name, (2) a photo, (3) a screening question that will help distinguish those that fit the persona and those that don\u2019t and (4) a description of relevant motivations and constraints. You can use a template or simply write down one or two paragraphs. The persona profile should fit on a single page or PowerPoint slide. The goal is to personalize the stakeholder so you understand their perspective and allow you to advocate on their behalf while developing solutions. Here is a quick and simple process: Start by dumping a list of all the personas you think might be relevant. List them by name, such as \u2018Fred the Farmer\u2019. Ultimately, you\u2019ll need to focus on the ones that are most relevant (you can\u2019t make everyone happy), but for now more is more*. Next, rank the personas and group redundant ones. Who do you need to appeal to first to achieve your vision? Who is most likely to help or hurt your cause? Draft screening questions for each priority persona. This is one sentence that \u2018defines\u2019 who the persona is and allows you to determine if someone fits the bill (e.g., do you depend entirely on farm income for your livelihood?) Write down at least 5 people that fit the persona. You don\u2019t have to know these people, but they must exist. Use one of their photos for the persona profile. Now spend less than 30 minutes per persona to start a draft. You will find that you may be very ignorant and stereotypical at this point\u2014that\u2019s fine, overcoming that is what this exercise is all about! Next you will research and refine your personas. * Sometimes, it's best to use a different pattern for categorizing personas instead of people, which can carry connotations that you don't mean to include. For example, categorize them by their shoes (wingtips, steel toes, and tevas), where they go on vacation (snow birds, summer in france, timeshares), or something else relevant to your project. Example \u00b6 This example was used in our effort to encourage farmers to participate in a cost-share conservation project (the Monarch Habitat Exchange). Acknowledgements \u00b6 This guide was adapted from Alex Cowan's excellent guide, which you can find here .","title":"Persona Development Guide"},{"location":"additional-resources/persona-guide/#persona-development-guide","text":"Personas include (1) a name, (2) a photo, (3) a screening question that will help distinguish those that fit the persona and those that don\u2019t and (4) a description of relevant motivations and constraints. You can use a template or simply write down one or two paragraphs. The persona profile should fit on a single page or PowerPoint slide. The goal is to personalize the stakeholder so you understand their perspective and allow you to advocate on their behalf while developing solutions. Here is a quick and simple process: Start by dumping a list of all the personas you think might be relevant. List them by name, such as \u2018Fred the Farmer\u2019. Ultimately, you\u2019ll need to focus on the ones that are most relevant (you can\u2019t make everyone happy), but for now more is more*. Next, rank the personas and group redundant ones. Who do you need to appeal to first to achieve your vision? Who is most likely to help or hurt your cause? Draft screening questions for each priority persona. This is one sentence that \u2018defines\u2019 who the persona is and allows you to determine if someone fits the bill (e.g., do you depend entirely on farm income for your livelihood?) Write down at least 5 people that fit the persona. You don\u2019t have to know these people, but they must exist. Use one of their photos for the persona profile. Now spend less than 30 minutes per persona to start a draft. You will find that you may be very ignorant and stereotypical at this point\u2014that\u2019s fine, overcoming that is what this exercise is all about! Next you will research and refine your personas. * Sometimes, it's best to use a different pattern for categorizing personas instead of people, which can carry connotations that you don't mean to include. For example, categorize them by their shoes (wingtips, steel toes, and tevas), where they go on vacation (snow birds, summer in france, timeshares), or something else relevant to your project.","title":"Persona Development Guide"},{"location":"additional-resources/persona-guide/#example","text":"This example was used in our effort to encourage farmers to participate in a cost-share conservation project (the Monarch Habitat Exchange).","title":"Example"},{"location":"additional-resources/persona-guide/#acknowledgements","text":"This guide was adapted from Alex Cowan's excellent guide, which you can find here .","title":"Acknowledgements"},{"location":"additional-resources/product-definition/","text":"Product Definition \u00b6 An editable version of the Product Definition is available here . Download a copy before continuing. Product definition is the process of documenting the proposed solution in broad terms. The purpose of a product definition is to align expectations, formulate broad concepts, clarify roles, and establish a development timeline. In the next step, Product Specification , you will have a chance to provide more specifics. At the product definition stage, you still have considerable flexibility in how you will deliver the final solution. Use the guidance below to complete the product definition. The product definition for this website is available here as an example. Solution Description \u00b6 Provide a 1-2 sentence description of the proposed solution. If you'd like, include a positioning statement: For [User Name], who wants/needs to [statement of need or opportunity], this [product category] will [primary value proposition]. Unlike [next best alternative], this tool [primary difference]. Goal & Objectives \u00b6 Goal \u00b6 The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully. Objectives \u00b6 Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives Users \u00b6 Primary \u00b6 Primary users are the subset of your boundary partners who will directly interact with the tool or solution. Think of these like your customers. You might want to group users with personas . For each primary user identified, complete the table below. If you've already completed the scoping exercise , you will have a starting point. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Add a row to the table for each problem scenario. Secondary \u00b6 Secondary users don't directly use the tool or solution, but are affected by it. They may provide data inputs or make decisions from the outputs. Repeat the process you went through for primary users. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Uses \u00b6 Uses describe the conditions under which the tool is used and the purpose for using the tool in that case. They should be fairly broad and together encompass the list of uses you envision for the tool or solution. If you're familiar with user stories, you can think of a use case as an 'epic' user story. During product specification you will enumerate more specific user stories to motivate development of features. However, if it's helpful, you can frame use cases like user stories as \"As user X, I want to Y so I can Z.\" This helps you think through each use case (Y) as specific to a user (X) who is motivated to achieve some outcome (Z). Product Sketch \u00b6 The six panels in this section allow you to get visual with how you expect the solution to work in practice. Visualizing can really help you avoid pitfalls and identify necessary features. Don't feel restricted to this six-panel format, feel free to get creative. Storyboards and Journey Maps are both great options for this sketch. Jot down any insights you had. You can move this section to the back if breaks up the flow of the document too much. Design Principles & Constraints \u00b6 List design principles and constraints. Consider integration, hosting, support, adaptive management and deployment but don't get into details yet. Your client may have constraints related to software licensing, budget, etc. Listing them here will make sure you don't come back with a solution that isn't feasible. Conditions of Satisfaction \u00b6 Set expectations for the tool by clarifying how you will know this tool is successful. Approach \u00b6 Provide a general description of the proposed work plan. Discuss roles, timelines, and resources required.","title":"Product Definition"},{"location":"additional-resources/product-definition/#product-definition","text":"An editable version of the Product Definition is available here . Download a copy before continuing. Product definition is the process of documenting the proposed solution in broad terms. The purpose of a product definition is to align expectations, formulate broad concepts, clarify roles, and establish a development timeline. In the next step, Product Specification , you will have a chance to provide more specifics. At the product definition stage, you still have considerable flexibility in how you will deliver the final solution. Use the guidance below to complete the product definition. The product definition for this website is available here as an example.","title":"Product Definition"},{"location":"additional-resources/product-definition/#solution-description","text":"Provide a 1-2 sentence description of the proposed solution. If you'd like, include a positioning statement: For [User Name], who wants/needs to [statement of need or opportunity], this [product category] will [primary value proposition]. Unlike [next best alternative], this tool [primary difference].","title":"Solution Description"},{"location":"additional-resources/product-definition/#goal-objectives","text":"","title":"Goal &amp; Objectives"},{"location":"additional-resources/product-definition/#goal","text":"The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully.","title":"Goal"},{"location":"additional-resources/product-definition/#objectives","text":"Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives","title":"Objectives"},{"location":"additional-resources/product-definition/#users","text":"","title":"Users"},{"location":"additional-resources/product-definition/#primary","text":"Primary users are the subset of your boundary partners who will directly interact with the tool or solution. Think of these like your customers. You might want to group users with personas . For each primary user identified, complete the table below. If you've already completed the scoping exercise , you will have a starting point. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Add a row to the table for each problem scenario.","title":"Primary"},{"location":"additional-resources/product-definition/#secondary","text":"Secondary users don't directly use the tool or solution, but are affected by it. They may provide data inputs or make decisions from the outputs. Repeat the process you went through for primary users. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better?","title":"Secondary"},{"location":"additional-resources/product-definition/#uses","text":"Uses describe the conditions under which the tool is used and the purpose for using the tool in that case. They should be fairly broad and together encompass the list of uses you envision for the tool or solution. If you're familiar with user stories, you can think of a use case as an 'epic' user story. During product specification you will enumerate more specific user stories to motivate development of features. However, if it's helpful, you can frame use cases like user stories as \"As user X, I want to Y so I can Z.\" This helps you think through each use case (Y) as specific to a user (X) who is motivated to achieve some outcome (Z).","title":"Uses"},{"location":"additional-resources/product-definition/#product-sketch","text":"The six panels in this section allow you to get visual with how you expect the solution to work in practice. Visualizing can really help you avoid pitfalls and identify necessary features. Don't feel restricted to this six-panel format, feel free to get creative. Storyboards and Journey Maps are both great options for this sketch. Jot down any insights you had. You can move this section to the back if breaks up the flow of the document too much.","title":"Product Sketch"},{"location":"additional-resources/product-definition/#design-principles-constraints","text":"List design principles and constraints. Consider integration, hosting, support, adaptive management and deployment but don't get into details yet. Your client may have constraints related to software licensing, budget, etc. Listing them here will make sure you don't come back with a solution that isn't feasible.","title":"Design Principles &amp; Constraints"},{"location":"additional-resources/product-definition/#conditions-of-satisfaction","text":"Set expectations for the tool by clarifying how you will know this tool is successful.","title":"Conditions of Satisfaction"},{"location":"additional-resources/product-definition/#approach","text":"Provide a general description of the proposed work plan. Discuss roles, timelines, and resources required.","title":"Approach"},{"location":"additional-resources/scoping/","text":"Scoping Canvas \u00b6 This scoping exercise will help you better understand the problem context and kick-start the process of identifying solutions. This is best done together in person, if possible, and typically requires at least an hour but can take half a day or more, depending on how invested you are in building out the Situation Model (feel free to sub in a systems map or journey map if preferred). If you've already developed some of these elements, great! Bring along a copy and start with a quick review. Find a large whiteboard and divide it up into sections as illustrated below (don't label each box as large as shown below, you'll need room to write). You can also find an editable version of this scoping canvas here . Instructions for filling in each section are provided below. Instructions \u00b6 Work through the sections in approximately this order, but don't hesitate to jump around if that's where the conversation flows. Scope \u00b6 A project\u2019s scope defines what the project intends to affect. \u201cPlace-based\u201d projects have a geographic scope and include efforts to conserve or effectively manage ecoregions, priority areas, or protected areas. \u201cThematic-based\u201d projects include efforts to address specific conservation targets, threats, opportunities, or enabling conditions and generally have a corresponding thematic scope. Thematic-based projects may also define a geographic scope that spatially describes a project area and might reference specific elements of biodiversity or a specific threat. (CMP 2013). I recommend erring on the inclusive side here. You will continue to get more specific throughout this process, for now try to represent the entire problem space. The scope should include the scope for the program, not just the specific tactic, tool, or feature you're considering. Vision \u00b6 In addition to defining the scope, it is also necessary to decide on a clear and common vision \u2013 a description of the desired state or ultimate condition that you are working to achieve. Your vision can be summarized in a vision statement, which meets the criteria of being relatively general, visionary, and brief. (CMP 2013) It's important to be agnostic about the solution within the vision statement. The vision is not for everyone to be using the tool your thinking about developing. Just describe the long-term outcomes that you're trying to achieve in sufficient detail that if you time-traveled to a time in which your strategy was successful, you'd know you were there. See Step 1: Start at the End of our metrics design philosophy for help if you get stuck. Map It! \u00b6 Create a visual representation of the problem space. A Situation Model is a great option, which consists of one or more conservation targets, threats, and drivers. Interventions should be mapped where appropriate. However, you can do a systems map, journey map, or whatever approach you choose. See Step 2: Map It of our metrics design philosophy for inspiration. The components of a Situation Model are defined below: Conservation Targets \u00b6 Conservation targets are specific species or ecological systems/habitats that are chosen to represent and encompass the full suite of biodiversity in the project area for place-based conservation or the focus of a thematic program (CMP 2013). If you've already been through the process of developing a situation model, you should have a list of conservation targets to draw from. You may also already know which target is the focus of today's effort. For expediency, you can focus on the conservation target you are, well, targeting. If you're not sure, include them all. Place the conservation targets on the far right side of the Situation Model diagram. Threats \u00b6 Direct threats are primarily human activities that immediately degrade a conservation target (e.g., unsustainable fishing, unsustainable hunting, oil drilling, construction of roads, industrial wastewater, or introduction of exotic invasive species), but they can be natural phenomena altered by human activities (e.g., increase in extreme storm events or increased evaporation due to global climate change) or in rare cases, natural phenomena whose impact is increased by other human activities (e.g., a potential tsunami that threatens the last remaining population of an Asian rhino). (CMP 2013) Feel free to list all threats, but what we really need is the threat that is mapped to the key intervention point that strategy will attack. If you haven't chosen a key intervention point yet, go ahead and build out the entire thing. If you have, just build out the relevant parts. List threats vertically to the left of the conservation target . Drivers \u00b6 From the Conservation Measures Partnership, drivers are: key factors that drive the direct threats and ultimately influence your conservation targets. These include indirect threats (also known as root causes and drivers), opportunities, and enabling conditions (CMP 2013). If you already have a situation model, just copy over the relevant bits. Otherwise, list the key drivers and make connections where there are relationships. Place drivers in boxes, mapped to their relevant threats, and group like drivers. Draw lines to connect related drivers. Leave a bit of room for interventions. Interventions \u00b6 Finally, map the primary existing interventions (e.g., strategies or tactics employed by boundary partners or strategic partners in your problem space). These are typically captured in hexagonal polygons. Connect them to the relevant threats or drivers. Take a step back and determine which driver you would like to intervene on. This is your key intervention point. Circle the key intervention point(s) (or leverage point or pain point, etc.). This is where your group has chosen to focus, either with the entire program or with this specific tactic, tool, or feature. Stakeholders \u00b6 Stakeholders include everyone that is involved in the problem space or might be affected by your selected intervention. That may be a lot of people, which is why we'll focus in on Boundary Partners here. List stakeholders and underline boundary partners. Boundary partners are people or organizations you can directly influence. Stakeholders you can't directly influence, but can work with directly, are called Strategic Partners . You might put a star next to their name to distinguish them. Feel free to group stakeholders to maintain a manageable list, but at the same time identify an actual human being who would fit in that group. If a boundary partner is described as 'permitting staff at state agencies', find a real human being in a permitting department at a state in your geographic scope as an example. If you can't do that, your boundary partner probably doesn't actually exist. See Step 3: Develop Empathy of our metrics design philosophy for a lot more detail on this process. Problems \u00b6 If you're coming from the Open Standards for the Practice of Conservation, everything so far has been very familiar. This is where we take a bit of a departure. Instead of jumping straight to listing strategies, we're going to think about how we can solve problems or create opportunities for the people whose behavior we need to change to achieve our conservation goal. Our strategy will be shaped by our understanding of these problems (but we actually won't define a specific strategy until the Product Definition step). Identify the most important boundary partner in your list. Ask yourself, what do we need them to do (or to stop doing) to achieve our conservation goal? In other words, what behaviors do we need them to exhibit in order that our vision is achieved? You might frame these as behaviors you'd 'expect to see', 'like to see', and 'love to see'. Now, write down the problems (or jobs to be done) that your boundary partners face in doing what you need them to do . Maybe they don't know where to get technical information, they don't have time to do it, they can't identify the priority areas to work, etc. If you - when you - talk to them about your proposed solution, they should relate to these problem statements - if not, these are not real problems (or jobs to be done) and solving them won't change their behavior. See Step 4: Define the Problem for helpful guidance on this prompt. Alternatives \u00b6 From the list of problems you've identified, have any of these problems been solved already? You may seek to solve them more expediently, which is great, but knowing what the alternatives are will allow you to evaluate your proposed solution against a real baseline. List the currently existing alternative solutions. How Might We? \u00b6 Now you can start to brainstorm solutions. \"How Might We?\" is a way of framing design questions that help you convert your problem statements into opportunities. If, for example, your boundary partners don't know where to find key information, how might we get that information to them? List the 'How Might We?' questions that address the problems you think will be most impactful for your boundary partners. Again, you'll take the time to craft your solutions during the Product Definition step. This will just give you a head start. Considerations/Constraints \u00b6 As your thinking about problems and potential solutions, you'll probably identify a few constraints on what you're able to do to solve the problem, or just some things to keep in mind going forward. Jot down any considerations and/or constraints so you don't forget. Research Questions \u00b6 This space is for recording questions that you don't have the answer to...yet. It might help to have an expert available during the scoping meeting so you don't get hung up on a question, but for those you can't address during the scoping meeting, write down the research questions and plan to get it answered before developing the Product Definition. References \u00b6 CMP 2013","title":"Scoping Canvas"},{"location":"additional-resources/scoping/#scoping-canvas","text":"This scoping exercise will help you better understand the problem context and kick-start the process of identifying solutions. This is best done together in person, if possible, and typically requires at least an hour but can take half a day or more, depending on how invested you are in building out the Situation Model (feel free to sub in a systems map or journey map if preferred). If you've already developed some of these elements, great! Bring along a copy and start with a quick review. Find a large whiteboard and divide it up into sections as illustrated below (don't label each box as large as shown below, you'll need room to write). You can also find an editable version of this scoping canvas here . Instructions for filling in each section are provided below.","title":"Scoping Canvas"},{"location":"additional-resources/scoping/#instructions","text":"Work through the sections in approximately this order, but don't hesitate to jump around if that's where the conversation flows.","title":"Instructions"},{"location":"additional-resources/scoping/#scope","text":"A project\u2019s scope defines what the project intends to affect. \u201cPlace-based\u201d projects have a geographic scope and include efforts to conserve or effectively manage ecoregions, priority areas, or protected areas. \u201cThematic-based\u201d projects include efforts to address specific conservation targets, threats, opportunities, or enabling conditions and generally have a corresponding thematic scope. Thematic-based projects may also define a geographic scope that spatially describes a project area and might reference specific elements of biodiversity or a specific threat. (CMP 2013). I recommend erring on the inclusive side here. You will continue to get more specific throughout this process, for now try to represent the entire problem space. The scope should include the scope for the program, not just the specific tactic, tool, or feature you're considering.","title":"Scope"},{"location":"additional-resources/scoping/#vision","text":"In addition to defining the scope, it is also necessary to decide on a clear and common vision \u2013 a description of the desired state or ultimate condition that you are working to achieve. Your vision can be summarized in a vision statement, which meets the criteria of being relatively general, visionary, and brief. (CMP 2013) It's important to be agnostic about the solution within the vision statement. The vision is not for everyone to be using the tool your thinking about developing. Just describe the long-term outcomes that you're trying to achieve in sufficient detail that if you time-traveled to a time in which your strategy was successful, you'd know you were there. See Step 1: Start at the End of our metrics design philosophy for help if you get stuck.","title":"Vision"},{"location":"additional-resources/scoping/#map-it","text":"Create a visual representation of the problem space. A Situation Model is a great option, which consists of one or more conservation targets, threats, and drivers. Interventions should be mapped where appropriate. However, you can do a systems map, journey map, or whatever approach you choose. See Step 2: Map It of our metrics design philosophy for inspiration. The components of a Situation Model are defined below:","title":"Map It!"},{"location":"additional-resources/scoping/#conservation-targets","text":"Conservation targets are specific species or ecological systems/habitats that are chosen to represent and encompass the full suite of biodiversity in the project area for place-based conservation or the focus of a thematic program (CMP 2013). If you've already been through the process of developing a situation model, you should have a list of conservation targets to draw from. You may also already know which target is the focus of today's effort. For expediency, you can focus on the conservation target you are, well, targeting. If you're not sure, include them all. Place the conservation targets on the far right side of the Situation Model diagram.","title":"Conservation Targets"},{"location":"additional-resources/scoping/#threats","text":"Direct threats are primarily human activities that immediately degrade a conservation target (e.g., unsustainable fishing, unsustainable hunting, oil drilling, construction of roads, industrial wastewater, or introduction of exotic invasive species), but they can be natural phenomena altered by human activities (e.g., increase in extreme storm events or increased evaporation due to global climate change) or in rare cases, natural phenomena whose impact is increased by other human activities (e.g., a potential tsunami that threatens the last remaining population of an Asian rhino). (CMP 2013) Feel free to list all threats, but what we really need is the threat that is mapped to the key intervention point that strategy will attack. If you haven't chosen a key intervention point yet, go ahead and build out the entire thing. If you have, just build out the relevant parts. List threats vertically to the left of the conservation target .","title":"Threats"},{"location":"additional-resources/scoping/#drivers","text":"From the Conservation Measures Partnership, drivers are: key factors that drive the direct threats and ultimately influence your conservation targets. These include indirect threats (also known as root causes and drivers), opportunities, and enabling conditions (CMP 2013). If you already have a situation model, just copy over the relevant bits. Otherwise, list the key drivers and make connections where there are relationships. Place drivers in boxes, mapped to their relevant threats, and group like drivers. Draw lines to connect related drivers. Leave a bit of room for interventions.","title":"Drivers"},{"location":"additional-resources/scoping/#interventions","text":"Finally, map the primary existing interventions (e.g., strategies or tactics employed by boundary partners or strategic partners in your problem space). These are typically captured in hexagonal polygons. Connect them to the relevant threats or drivers. Take a step back and determine which driver you would like to intervene on. This is your key intervention point. Circle the key intervention point(s) (or leverage point or pain point, etc.). This is where your group has chosen to focus, either with the entire program or with this specific tactic, tool, or feature.","title":"Interventions"},{"location":"additional-resources/scoping/#stakeholders","text":"Stakeholders include everyone that is involved in the problem space or might be affected by your selected intervention. That may be a lot of people, which is why we'll focus in on Boundary Partners here. List stakeholders and underline boundary partners. Boundary partners are people or organizations you can directly influence. Stakeholders you can't directly influence, but can work with directly, are called Strategic Partners . You might put a star next to their name to distinguish them. Feel free to group stakeholders to maintain a manageable list, but at the same time identify an actual human being who would fit in that group. If a boundary partner is described as 'permitting staff at state agencies', find a real human being in a permitting department at a state in your geographic scope as an example. If you can't do that, your boundary partner probably doesn't actually exist. See Step 3: Develop Empathy of our metrics design philosophy for a lot more detail on this process.","title":"Stakeholders"},{"location":"additional-resources/scoping/#problems","text":"If you're coming from the Open Standards for the Practice of Conservation, everything so far has been very familiar. This is where we take a bit of a departure. Instead of jumping straight to listing strategies, we're going to think about how we can solve problems or create opportunities for the people whose behavior we need to change to achieve our conservation goal. Our strategy will be shaped by our understanding of these problems (but we actually won't define a specific strategy until the Product Definition step). Identify the most important boundary partner in your list. Ask yourself, what do we need them to do (or to stop doing) to achieve our conservation goal? In other words, what behaviors do we need them to exhibit in order that our vision is achieved? You might frame these as behaviors you'd 'expect to see', 'like to see', and 'love to see'. Now, write down the problems (or jobs to be done) that your boundary partners face in doing what you need them to do . Maybe they don't know where to get technical information, they don't have time to do it, they can't identify the priority areas to work, etc. If you - when you - talk to them about your proposed solution, they should relate to these problem statements - if not, these are not real problems (or jobs to be done) and solving them won't change their behavior. See Step 4: Define the Problem for helpful guidance on this prompt.","title":"Problems"},{"location":"additional-resources/scoping/#alternatives","text":"From the list of problems you've identified, have any of these problems been solved already? You may seek to solve them more expediently, which is great, but knowing what the alternatives are will allow you to evaluate your proposed solution against a real baseline. List the currently existing alternative solutions.","title":"Alternatives"},{"location":"additional-resources/scoping/#how-might-we","text":"Now you can start to brainstorm solutions. \"How Might We?\" is a way of framing design questions that help you convert your problem statements into opportunities. If, for example, your boundary partners don't know where to find key information, how might we get that information to them? List the 'How Might We?' questions that address the problems you think will be most impactful for your boundary partners. Again, you'll take the time to craft your solutions during the Product Definition step. This will just give you a head start.","title":"How Might We?"},{"location":"additional-resources/scoping/#considerationsconstraints","text":"As your thinking about problems and potential solutions, you'll probably identify a few constraints on what you're able to do to solve the problem, or just some things to keep in mind going forward. Jot down any considerations and/or constraints so you don't forget.","title":"Considerations/Constraints"},{"location":"additional-resources/scoping/#research-questions","text":"This space is for recording questions that you don't have the answer to...yet. It might help to have an expert available during the scoping meeting so you don't get hung up on a question, but for those you can't address during the scoping meeting, write down the research questions and plan to get it answered before developing the Product Definition.","title":"Research Questions"},{"location":"additional-resources/scoping/#references","text":"CMP 2013","title":"References"},{"location":"data-management/database-overview/","text":"Do you (or your client) have data stored across so many files you can't remember where they are? Are you scrambling to pull together information when requested, constantly months behind in reporting, or finding yourself dumping data from multiple sources into a single spreadsheet for analysis? If so, it might be time for a database. However, there are complexities to consider when setting up and managing a database. This tutorial will introduce you to databases , describe available database solutions , and help you select the right database for your needs . Introduction to Databases \u00b6 A database is simply an organized store for information. Data are stored as rows and columns in tables; relationships are defined between tables (hence the name, \"relational database\"). You might have tables for customers, products, and orders. Customers place orders which include products. The database helps you get the right products to the right customers. How the data are stored is often less important than how the data are accessed. The primary reason you're developing a database is probably to make data easier to access, analyze and report. When choosing and designing a database, you'll need to pay as much attention to how the user will interface with the database as the database solution itself. If you're simply looking for data analysis and visualization (think: Tableau), check out our Introduction to Data Visualization . Interacting with a database \u00b6 Most users will interact with a database through a navigation form. The navigation form surfaces the important functionality of the database, including entering data, querying data, and generating reports. Here's an example navigation form for a bakery's order database created in Microsoft Access. The navigation form is set up after the database is configured by the database administrator (DBA; this may be you, someone on the Metrics Service Line, or an outside contractor). The navigation form--as well as other forms, queries, and reports that are made available to the user by the DBA--make data accessible to users while protecting the integrity of the database Forms \u00b6 Data is entered into the database primarily through forms (it can be imported as well). Forms can be made available through the Navigation Form or through individual forms designed and made available by the DBA. Queries \u00b6 Queries allow you to search for and update records in the database. Want to know how many orders were placed yesterday? How many blueberry muffins you've sold this year? You'll need a query for that. Reports \u00b6 Reports are queries or collections of queries that can be batched together and run periodically. You might have a standard report that is produced each quarter or a ready-made report to provide real time information to your boss when requested. Reports must also be set up and made available by the DBA. A database solution involves multiple components \u00b6 A database solution (aka database management system; DBMS) will include these key components Database Software and Hosting Platform : The database software runs the database. The database and the database software will be hosted either on a local server or on the cloud. Database Management Client : The database management client is the interface that the DBA will use to configure the database and manage access for users. The database client can reduce, but doesn't necessarily eliminate, the need to use SQL (Structured Query Language) to interact with the database (however many DBAs will prefer using SQL anyway). User Interface and Analysis Platform : The user interface allows users to access the data while the analysis platform allows the user to gain insights from the data. Often, these are combined. To illustrate, below are three options for a database solution that you might consider. Example 1: Microsoft Access \u00b6 Microsoft Access is an all-in-one database solution from the '90s. It lacks many features that some would deem critical, but for lightweight and desktop-based database solutions, it's hard to beat the out-of-the-box functionality for low-tech users. Similar, more modern options are available as cloud-based, subscription services. Example 2: Microsoft Azure SQL Database + SQL Server Management Studio + Microsoft PowerApps \u00b6 A cloud-based solution that offers great customization, if you're tech-savvy enough to configure and maintain it. You don't have to be a software engineer, but don't expect it to be easy. Amazon, IBM and Oracle offer similar solutions. Example 3: MySQL + MySQL Workbench + Custom Web Application \u00b6 This open source solution may be your go-to if you're building a website and working with a web developer. Many open source solutions are available. Project Spotlight: The Registry for the Monarch Butterfly Habitat Exchange was built on this stack. Database Options \u00b6 There are many choices when it comes to databases, each with its own advantages and disadvantages. All-in-one, low tech solutions \u00b6 These are your options if you don't have a tech guru on the team and don't have enough budget to hire one. Free(ish) and Mostly Desktop-Based \u00b6 Microsoft Access and OpenOffice Base are good entry-level options. OpenOffice Base is free but has fewer features and less support than Microsoft Access. Microsoft Excel , especially if paired with custom VBA Forms and Modules, can even serve as a database but has limitations. If you'd prefer a cloud-based solution, Google Sheets paired with Google Forms might even work. Cheap(ish) Cloud-based Subscription Services \u00b6 The need for data storage and analytics solutions to serve low-tech users has led to a boom in cloud-based solutions like AirTable , QuickBase , TeamDesk , Knack , and Sonadier . The price scales with increasing storage capacity and number of users, from cheap ($5/month) to not-so-cheap ($500/month or more). The data visualization powerhouse Tableau offers a data management add-on for $5/mo that might meet your needs if you're mostly focused on visualization and are already a Tableau subscriber. Salesforce and other CRMs (Customer Relationship Management software) also combine data management and analytics in a way that could serve as a solution for your needs. Finally, one notable desktop application is Notion , which is a do-everything app with a cult following that provides some database-like functionality. Solutions for the more technical crowd \u00b6 Venture into this territory and you'll want to make sure you have the technical chops on board to configure and maintain these options. Open Source Database Software \u00b6 PostgreSQL , MySQL , and MariaDB are examples of open source (free) database solutions. While these are robust, scalable database solutions used by many large corporations, they are not designed for your everyday desk jockey. These open source solutions are often paired with a web-based interface. If you are working with a web developer and have the budget for them to design the forms, queries and reports you need you'll likely choose one of these open source options. Don't forget you'll probably need to hire them again when you want to make changes or when the software reaches its End Of Life (EOL). Cloud-based Scalable Services \u00b6 Cloud-based options are offered by Amazon RCD , Microsoft Azure , IBM Db2 and Oracle Cloud Services . Your data now lives on the cloud. Price scales with use, and entry-level tiers can be very cheap, but you will have to provide a credit card and make sure the charges don't get out of hand. To get to your data, you'll need an application--either a custom web app or a desktop app you build using a service like Microsoft PowerApps , FileMaker , Oracle Forms , or Appian . These \"low code\" application platforms allow you to develop custom solutions without a team of software engineers, but do require some serious training. Enterprise Database Solutions \u00b6 Microsoft, IBM, and Oracle all sell enterprise database solutions. Expect to pay over $1,000 per month. You can get Microsoft SQL Server Express for free but make sure its limited set of features will work for you. Database Clients \u00b6 For these more technical solutions, you'll need a database management client. Some database solutions come with a preferred client, such as Microsoft's SQL Server Management Studio and Azure Data Studio , Oracle's SQL Developer , PostreSQL's pgAdmin , and MySQL's MySQL Workbench . However, you're not stuck with these options, some of these can be used for other databases, while a number of popular free alternative options exist including SQuirrel SLQ , DataGrip , or even VSCode with mssql extension (for MySQL databases; however see Azure Data Studio as a better solution built from the VSCode platform). For Python \u00b6 If you're working in Python, know that there are APIs for most of the open source and cloud-based database solutions. For example, psycopg2 is the API for Postrgres and Ibm-db is the API for IBM's DB2 cloud-based database. I'd recommend playing around with SQLite (sqlite3 package), which comes with the base Python distribution, to familiarize yourself with using databases in your applications. You can access and manipulate your database in a Jupyter Notebook using the appropriate API and pandas . Use the pandas.read_sql command to read the results of a SQL query into a pandas dataframe for further analysis. How to Choose? \u00b6 With so many options, how do you choose? This section will help you pick the right one. Budget \u00b6 Consider both the cost of the software itself and the cost of configuring and maintaining the database. Scalability \u00b6 Unless you're dealing with *Big Data*, you won't need to worry about the database growing too large to fit on a single server. The only likely exception would be if attempting to store geospatial data within a relational database outside of ArcGIS. If you go with a cloud-based solution you can scale as much as you want, provided you can pay for it. Hosting & Integration \u00b6 If the database will be hosted and managed by the client or their website manager, your options will likely be limited to those that integrate with their existing systems. Work with them when selecting a database and during database design. In most other cases, the database will live on the client's local server (unless you picked a cloud-based alternative). Maintenance & Support \u00b6 Who will maintain the database over time? If it's the client, make sure they are comfortable with the solution. Who will provide support if something crashes or the client needs a new type of report developed? If it's you, make sure you are comfortable with the solution. Keep in mind that paying a contractor to set up a cool web-based database and interface will likely require paying a contractor to maintain that database and interface periodically over time. Final Thoughts \u00b6 Setting up a database is a big step in the maturation of any data-driven program. The right database solution will keep your data safe and secure while also providing access to the data for analysis and reporting. When choosing a database, the best strategy is to keep it as simple as possible. Important! If you've decided that you're not the right person to decide which database to use, that's ok. While someone with more technical experience will be better able to select the right database solution, they may not know enough about your program to design the database and the forms, queries and reports you need. You're the expert on your program, stay involved in the process to make sure the final solution meets your needs!","title":"Database Overview"},{"location":"data-management/database-overview/#introduction-to-databases","text":"A database is simply an organized store for information. Data are stored as rows and columns in tables; relationships are defined between tables (hence the name, \"relational database\"). You might have tables for customers, products, and orders. Customers place orders which include products. The database helps you get the right products to the right customers. How the data are stored is often less important than how the data are accessed. The primary reason you're developing a database is probably to make data easier to access, analyze and report. When choosing and designing a database, you'll need to pay as much attention to how the user will interface with the database as the database solution itself. If you're simply looking for data analysis and visualization (think: Tableau), check out our Introduction to Data Visualization .","title":"Introduction to Databases"},{"location":"data-management/database-overview/#interacting-with-a-database","text":"Most users will interact with a database through a navigation form. The navigation form surfaces the important functionality of the database, including entering data, querying data, and generating reports. Here's an example navigation form for a bakery's order database created in Microsoft Access. The navigation form is set up after the database is configured by the database administrator (DBA; this may be you, someone on the Metrics Service Line, or an outside contractor). The navigation form--as well as other forms, queries, and reports that are made available to the user by the DBA--make data accessible to users while protecting the integrity of the database","title":"Interacting with a database"},{"location":"data-management/database-overview/#forms","text":"Data is entered into the database primarily through forms (it can be imported as well). Forms can be made available through the Navigation Form or through individual forms designed and made available by the DBA.","title":"Forms"},{"location":"data-management/database-overview/#queries","text":"Queries allow you to search for and update records in the database. Want to know how many orders were placed yesterday? How many blueberry muffins you've sold this year? You'll need a query for that.","title":"Queries"},{"location":"data-management/database-overview/#reports","text":"Reports are queries or collections of queries that can be batched together and run periodically. You might have a standard report that is produced each quarter or a ready-made report to provide real time information to your boss when requested. Reports must also be set up and made available by the DBA.","title":"Reports"},{"location":"data-management/database-overview/#a-database-solution-involves-multiple-components","text":"A database solution (aka database management system; DBMS) will include these key components Database Software and Hosting Platform : The database software runs the database. The database and the database software will be hosted either on a local server or on the cloud. Database Management Client : The database management client is the interface that the DBA will use to configure the database and manage access for users. The database client can reduce, but doesn't necessarily eliminate, the need to use SQL (Structured Query Language) to interact with the database (however many DBAs will prefer using SQL anyway). User Interface and Analysis Platform : The user interface allows users to access the data while the analysis platform allows the user to gain insights from the data. Often, these are combined. To illustrate, below are three options for a database solution that you might consider.","title":"A database solution involves multiple components"},{"location":"data-management/database-overview/#example-1-microsoft-access","text":"Microsoft Access is an all-in-one database solution from the '90s. It lacks many features that some would deem critical, but for lightweight and desktop-based database solutions, it's hard to beat the out-of-the-box functionality for low-tech users. Similar, more modern options are available as cloud-based, subscription services.","title":"Example 1: Microsoft Access"},{"location":"data-management/database-overview/#example-2-microsoft-azure-sql-database-sql-server-management-studio-microsoft-powerapps","text":"A cloud-based solution that offers great customization, if you're tech-savvy enough to configure and maintain it. You don't have to be a software engineer, but don't expect it to be easy. Amazon, IBM and Oracle offer similar solutions.","title":"Example 2: Microsoft Azure SQL Database + SQL Server Management Studio + Microsoft PowerApps"},{"location":"data-management/database-overview/#example-3-mysql-mysql-workbench-custom-web-application","text":"This open source solution may be your go-to if you're building a website and working with a web developer. Many open source solutions are available. Project Spotlight: The Registry for the Monarch Butterfly Habitat Exchange was built on this stack.","title":"Example 3: MySQL + MySQL Workbench + Custom Web Application"},{"location":"data-management/database-overview/#database-options","text":"There are many choices when it comes to databases, each with its own advantages and disadvantages.","title":"Database Options"},{"location":"data-management/database-overview/#all-in-one-low-tech-solutions","text":"These are your options if you don't have a tech guru on the team and don't have enough budget to hire one.","title":"All-in-one, low tech solutions"},{"location":"data-management/database-overview/#freeish-and-mostly-desktop-based","text":"Microsoft Access and OpenOffice Base are good entry-level options. OpenOffice Base is free but has fewer features and less support than Microsoft Access. Microsoft Excel , especially if paired with custom VBA Forms and Modules, can even serve as a database but has limitations. If you'd prefer a cloud-based solution, Google Sheets paired with Google Forms might even work.","title":"Free(ish) and Mostly Desktop-Based"},{"location":"data-management/database-overview/#cheapish-cloud-based-subscription-services","text":"The need for data storage and analytics solutions to serve low-tech users has led to a boom in cloud-based solutions like AirTable , QuickBase , TeamDesk , Knack , and Sonadier . The price scales with increasing storage capacity and number of users, from cheap ($5/month) to not-so-cheap ($500/month or more). The data visualization powerhouse Tableau offers a data management add-on for $5/mo that might meet your needs if you're mostly focused on visualization and are already a Tableau subscriber. Salesforce and other CRMs (Customer Relationship Management software) also combine data management and analytics in a way that could serve as a solution for your needs. Finally, one notable desktop application is Notion , which is a do-everything app with a cult following that provides some database-like functionality.","title":"Cheap(ish) Cloud-based Subscription Services"},{"location":"data-management/database-overview/#solutions-for-the-more-technical-crowd","text":"Venture into this territory and you'll want to make sure you have the technical chops on board to configure and maintain these options.","title":"Solutions for the more technical crowd"},{"location":"data-management/database-overview/#open-source-database-software","text":"PostgreSQL , MySQL , and MariaDB are examples of open source (free) database solutions. While these are robust, scalable database solutions used by many large corporations, they are not designed for your everyday desk jockey. These open source solutions are often paired with a web-based interface. If you are working with a web developer and have the budget for them to design the forms, queries and reports you need you'll likely choose one of these open source options. Don't forget you'll probably need to hire them again when you want to make changes or when the software reaches its End Of Life (EOL).","title":"Open Source Database Software"},{"location":"data-management/database-overview/#cloud-based-scalable-services","text":"Cloud-based options are offered by Amazon RCD , Microsoft Azure , IBM Db2 and Oracle Cloud Services . Your data now lives on the cloud. Price scales with use, and entry-level tiers can be very cheap, but you will have to provide a credit card and make sure the charges don't get out of hand. To get to your data, you'll need an application--either a custom web app or a desktop app you build using a service like Microsoft PowerApps , FileMaker , Oracle Forms , or Appian . These \"low code\" application platforms allow you to develop custom solutions without a team of software engineers, but do require some serious training.","title":"Cloud-based Scalable Services"},{"location":"data-management/database-overview/#enterprise-database-solutions","text":"Microsoft, IBM, and Oracle all sell enterprise database solutions. Expect to pay over $1,000 per month. You can get Microsoft SQL Server Express for free but make sure its limited set of features will work for you.","title":"Enterprise Database Solutions"},{"location":"data-management/database-overview/#database-clients","text":"For these more technical solutions, you'll need a database management client. Some database solutions come with a preferred client, such as Microsoft's SQL Server Management Studio and Azure Data Studio , Oracle's SQL Developer , PostreSQL's pgAdmin , and MySQL's MySQL Workbench . However, you're not stuck with these options, some of these can be used for other databases, while a number of popular free alternative options exist including SQuirrel SLQ , DataGrip , or even VSCode with mssql extension (for MySQL databases; however see Azure Data Studio as a better solution built from the VSCode platform).","title":"Database Clients"},{"location":"data-management/database-overview/#for-python","text":"If you're working in Python, know that there are APIs for most of the open source and cloud-based database solutions. For example, psycopg2 is the API for Postrgres and Ibm-db is the API for IBM's DB2 cloud-based database. I'd recommend playing around with SQLite (sqlite3 package), which comes with the base Python distribution, to familiarize yourself with using databases in your applications. You can access and manipulate your database in a Jupyter Notebook using the appropriate API and pandas . Use the pandas.read_sql command to read the results of a SQL query into a pandas dataframe for further analysis.","title":"For Python"},{"location":"data-management/database-overview/#how-to-choose","text":"With so many options, how do you choose? This section will help you pick the right one.","title":"How to Choose?"},{"location":"data-management/database-overview/#budget","text":"Consider both the cost of the software itself and the cost of configuring and maintaining the database.","title":"Budget"},{"location":"data-management/database-overview/#scalability","text":"Unless you're dealing with *Big Data*, you won't need to worry about the database growing too large to fit on a single server. The only likely exception would be if attempting to store geospatial data within a relational database outside of ArcGIS. If you go with a cloud-based solution you can scale as much as you want, provided you can pay for it.","title":"Scalability"},{"location":"data-management/database-overview/#hosting-integration","text":"If the database will be hosted and managed by the client or their website manager, your options will likely be limited to those that integrate with their existing systems. Work with them when selecting a database and during database design. In most other cases, the database will live on the client's local server (unless you picked a cloud-based alternative).","title":"Hosting &amp; Integration"},{"location":"data-management/database-overview/#maintenance-support","text":"Who will maintain the database over time? If it's the client, make sure they are comfortable with the solution. Who will provide support if something crashes or the client needs a new type of report developed? If it's you, make sure you are comfortable with the solution. Keep in mind that paying a contractor to set up a cool web-based database and interface will likely require paying a contractor to maintain that database and interface periodically over time.","title":"Maintenance &amp; Support"},{"location":"data-management/database-overview/#final-thoughts","text":"Setting up a database is a big step in the maturation of any data-driven program. The right database solution will keep your data safe and secure while also providing access to the data for analysis and reporting. When choosing a database, the best strategy is to keep it as simple as possible. Important! If you've decided that you're not the right person to decide which database to use, that's ok. While someone with more technical experience will be better able to select the right database solution, they may not know enough about your program to design the database and the forms, queries and reports you need. You're the expert on your program, stay involved in the process to make sure the final solution meets your needs!","title":"Final Thoughts"},{"location":"data-management/notes/","text":"Database Design Conceptual Data Model: must reflect the actual and possible states of the outside world. For example, if people can have two phone numbers, this must be possible. ER-Model Unified Modelling Language Schema/Logical Data Model: actual implementation of the conceptual data model in the chosen database technology. Normalization: According to wikipeda, \"each elementary 'fact' is recorded in one place so that insertions, updates, and deletions automatically maintain consistency\". Physical database design: affects performance, scalability, recovery, security, etc. Security Views","title":"Notes"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/","text":"Machine Learning Tutorial \u00b6 This tutorial is intended to illustrate a typical workflow for machine learning to solve a land use and land cover classification problem. Land Use and Land Cover classifications are used to identify the dominant land cover or land use type in an area. We use the Naive Bayes and Random Forest classifiers, as implemented within scikit-learn library. This tutorial borrows heavily from the very helpful tutorial developed by Chris Holden and updated by Patrick Gray. Also used: rasterio , geopandas , numpy , pandas , shapely , and matplotlib . The Challenge \u00b6 Our client required a rapid approach for evaluating the benefits of conservation projects to Mule Deer. We proposed using Ecological State and Transition Models (STMs) as the basis for the evaluation. The NRCS has developed STMs for the dominant ecological sites within the region, however only a subset of the region was mapped. We use vegetation data (provided by the Rangelands App ) and other environmental variables to predict STM for the unmapped areas of the range. Import Statements \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import rasterio from rasterio.mask import mask from rasterio.plot import show from rasterio.plot import show_hist from rasterio.windows import Window from rasterio.plot import reshape_as_raster , reshape_as_image import geopandas as gpd import numpy as np from shapely.geometry import mapping from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn import preprocessing from sklearn.pipeline import make_pipeline from sklearn.utils import resample import matplotlib.pyplot as plt # ipywidgets is used to create a progress bar from ipywidgets import IntProgress from IPython.display import display 1 % matplotlib inline Import Data \u00b6 We'll be using supervised classification techniques. We'll need both labels and predictor features to train the models. Our labels will come from the mapped STM derived from NRCS data. To begin, we'll use vegetation data as predictors, including cover estimates for trees, shrubs, perennial grasses and forbs, and bare ground. 1 2 3 4 5 # CHANGE TO VEG_COVER_PATH # Read in features and training data train_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\train_clip_utm.tif' data = rasterio . open ( train_path ) data . crs # Check the projection, all features must share a projection 1 CRS.from_dict(init='epsg:26913') 1 2 3 labels_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\labels.shp' labels = gpd . read_file ( labels_path ) labels . crs 1 {'init': 'epsg:26913'} 1 len ( labels ) 1 6136 We have 6,136 labeled polygons, represented within a geopandas dataframe, while our predictor features are represented in a rasterio raster. Both share the same projection. Explore Data and Preparation Steps \u00b6 To train the classifiers, we'll need to associate our vector data (labels as polygons) with our raster pixels (predictor features). We'll accomplish this with the rasterio mask function. The mask function will essentially clip (or mask) our raster with each polygon. First, we'll want to extract the geometry of each feature in the labels shapefile to GeoJSON format. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # this generates a list of shapely geometries geoms = labels . geometry . values # let's grab a single shapely geometry to check geometry = geoms [ 0 ] print ( \"This is a shapely polygon\" ) print ( type ( geometry )) print ( geometry ) # transform to GeoJSON format (note 'mapping' is in the shapely namespace) feature = [ mapping ( geometry )] # can also do this using polygon.__geo_interface__ print ( \"This is the same polygon in GeoJSON format\" ) print ( type ( feature )) print ( feature ) 1 2 3 4 5 6 This is a shapely polygon <class 'shapely.geometry.polygon.Polygon'> POLYGON ((155090.8039999995 4317864.1029, 155095.9642000003 4317847.2246, 155096.9452999998 4317767.406400001, 155079.9972999999 4317764.4924, 155013.6201999998 4317772.8774, 154946.0262000002 4317762.761399999, 154906.4253000002 4317765.351500001, 154878.2922999999 4317781.742900001, 154874.5252 4317788.509099999, 154877.2857999997 4317829.973099999, 154890.6249000002 4317873.230799999, 154892.2852999996 4317898.202400001, 154873.1852000002 4318020.8462, 154866.6454999996 4318032.8059, 154861.3509 4318094.8554, 154880.1355999997 4318314.824999999, 154876.2439000001 4318350.6874, 154879.5504999999 4318400.631100001, 154884.4243000001 4318410.842, 154890.0175999999 4318527.3519, 154897.2582 4318573.0209, 154890.3213 4318594.527100001, 154894.7226999998 4318598.269300001, 154913.1912000002 4318593.0416, 154946.1224999996 4318553.260600001, 154952.6897 4318463.052999999, 154967.4232000001 4318401.3926, 154951.2766000004 4318220.3751, 154975.3370000003 4318141.542099999, 155019.5067999996 4318081.9695, 155041.7379000001 4318038.8718, 155041.0279000001 4317965.691199999, 155054.3926999997 4317914.6457, 155080.8075000001 4317871.2906, 155090.8039999995 4317864.1029)) This is the same polygon in GeoJSON format <class 'list'> [{'type': 'Polygon', 'coordinates': (((155090.80399999954, 4317864.1029), (155095.96420000028, 4317847.2246), (155096.9452999998, 4317767.406400001), (155079.99729999993, 4317764.4924), (155013.62019999977, 4317772.8774), (154946.0262000002, 4317762.761399999), (154906.42530000024, 4317765.351500001), (154878.29229999986, 4317781.742900001), (154874.52520000003, 4317788.509099999), (154877.28579999972, 4317829.973099999), (154890.62490000017, 4317873.230799999), (154892.28529999964, 4317898.202400001), (154873.18520000018, 4318020.8462000005), (154866.64549999963, 4318032.8059), (154861.35089999996, 4318094.8554), (154880.1355999997, 4318314.824999999), (154876.24390000012, 4318350.6874), (154879.5504999999, 4318400.631100001), (154884.42430000007, 4318410.842), (154890.0175999999, 4318527.3519), (154897.25820000004, 4318573.0209), (154890.32129999995, 4318594.5271000005), (154894.7226999998, 4318598.269300001), (154913.19120000023, 4318593.0416), (154946.1224999996, 4318553.260600001), (154952.6897, 4318463.052999999), (154967.42320000008, 4318401.3926), (154951.27660000045, 4318220.3751), (154975.3370000003, 4318141.542099999), (155019.50679999962, 4318081.9695), (155041.73790000007, 4318038.8718), (155041.0279000001, 4317965.691199999), (155054.39269999973, 4317914.6457), (155080.8075000001, 4317871.2906), (155090.80399999954, 4317864.1029)),)}] Now let's extract the raster values within each polygon using the rasterio mask() function . 1 out_image , out_transform = mask ( data , feature , crop = True ) 1 out_image . shape 1 (6, 32, 10) 1 show ( out_image [ 0 ]) 1 <matplotlib.axes._subplots.AxesSubplot at 0xbea5c5b70> As you can see, the features raster was clipped to a single polygon. There are 6 bands and 32x10 pixels. We'll repeat this process for all 6,136 polygons to build our dataset. We'll also need to clean this raster up a bit before we use it in training. We'll explore all of this next. But first, we'll be doing a lot of memory intensive work so we'll close the dataset for now. 1 data . close () Build the Training Data for sckit-learn \u00b6 We'll repeat the above process for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels. Note that the column 'MuleDeer_1' in the labels geodataframe has the label we're after. TODO: change to column with label instead of numeric. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 %% time # Lets create a progress bar as this step can take some time p_bar = IntProgress ( min = 0 , max = len ( geoms ), description = 'Processing' ) display ( p_bar ) # Set up arrays X = np . array ([], dtype = np . int8 ) . reshape ( 0 , 6 ) # pixels for training y = np . array ([], dtype = np . string_ ) # labels for training # extract the raster values within the polygon with rasterio . open ( train_path ) as src : band_count = src . count for index , geom in enumerate ( geoms ): # Note geoms was created above feature = [ mapping ( geom )] # the mask function returns an array of the raster pixels within this feature out_image , out_transform = mask ( src , feature , crop = True ) # eliminate all the pixels with 0 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image [:, ~ np . all ( out_image == 0 , axis = 0 )] # eliminate all the pixels with 255 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image_trimmed [:, ~ np . all ( out_image_trimmed == 255 , axis = 0 )] # reshape the array to [pixel count, bands] out_image_reshaped = out_image_trimmed . reshape ( - 1 , band_count ) # append the labels to the y array y = np . append ( y , [ labels [ \"MuleDeer_1\" ][ index ]] * out_image_reshaped . shape [ 0 ]) # ??? # stack the pixels onto the pixel array X = np . vstack (( X , out_image_reshaped )) # increment the progress bar p_bar . value += 1 1 2 3 4 IntProgress ( value = 0 , description = 'Processing' , max = 6136 ) Wall time : 25 min 33 s Save the output \u00b6 We'll save the output as a numpy array to avoid the long process of rebuilding the features in the future. This will also allow us to share this analysis with others without them needing access to the input data. If this were not a tutorial, I might have ended the notebook here and started a new one for the remainder of the analysis. I've commented out the load statements below, uncomment to load in the saved features and labels. 1 2 3 # Save features and labels np . save ( 'E:/lulc-features.npy' , X ) np . save ( 'E:/lulc-labels.npy' , y ) 1 2 3 # Load in data X = np . load ( 'E:/lulc-features.npy' ) # fill in path y = np . load ( 'E:/lulc-labels.npy' ) Splitting the data for testing \u00b6 In order to evaluate the accuracy of our model, we'll reserve a subset of the data for testing. The train_test_split function allows us to quickly and randomly subset our data for this purpose. 1 2 # split out 30% of data for testing. Random state set for reproducibility. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) Pairing y with X \u00b6 Now that we have the image we want to classify (X_train) and the land cover labels (y_train), let's check to make sure they match in size so we can feed them to our models. 1 2 3 4 5 6 7 st_names = np . unique ( y_train ) print ( 'The training data include {n} classes: {classes} \\n ' . format ( n = st_names . size , classes = st_names )) # We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels print ( 'Our X matrix is sized: {sz} ' . format ( sz = X_train . shape )) print ( 'Our y array is sized: {sz} ' . format ( sz = y_train . shape )) 1 2 3 4 5 The training data include 5 classes : [ 'Encroached Shrub' 'Loamy Bottom' 'P-J' 'Perennial Shrub' 'Wet/Salt Meadow' ] Our X matrix is sized : ( 4755679 , 6 ) Our y array is sized : ( 4755679 ,) That looks good. We have 5 classes (i.e., our STM names); 6 predictor features (i.e., the 6 bands in our X matrix, now flattened; and both the X and y array are the same length. We'll treat these vegetation cover values as spectral signatures, and plot each to make sure they're actually separable since all we're going by in this classification is pixel values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 fig , ax = plt . subplots ( 1 , 3 , figsize = [ 20 , 8 ]) # bands are numbered 1 through 6 following GDAL convention band_count = np . arange ( 1 , 7 ) classes = np . unique ( y_train ) for class_type in classes : band_intensity = np . mean ( X_train [ y_train == class_type , :], axis = 0 ) ax [ 0 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 1 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 2 ] . plot ( band_count , band_intensity , label = class_type ) #plot them as lines # Add some axis labels # Add some axis labels ax [ 0 ] . set_xlabel ( 'Band #' ) ax [ 0 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_xlabel ( 'Band #' ) ax [ 2 ] . set_ylabel ( 'Reflectance Value' ) ax [ 2 ] . set_xlabel ( 'Band #' ) ax [ 1 ] . legend ( loc = \"upper right\" ) # Add a title ax [ 0 ] . set_title ( 'Band Intensities Full Overview' ) ax [ 1 ] . set_title ( 'Band Intensities Lower Ref Subset' ) ax [ 2 ] . set_title ( 'Band Intensities Higher Ref Subset' ) plt . show () Looks like each class will be easily separable. This will be a helper function to convert class labels into indices so we're predicting to integers instead of strings. TODO: switch labels and numbers 1 2 3 4 5 6 7 def str_class_to_int ( class_array ): class_array [ class_array == 'P-J' ] = 1 class_array [ class_array == 'Perennial Shrub' ] = 2 class_array [ class_array == 'Encroached Shrub' ] = 3 class_array [ class_array == 'Loamy Bottom' ] = 4 class_array [ class_array == 'Wet/Salt Meadow' ] = 5 return ( class_array . astype ( int )) Training the Classifier \u00b6 Now that we have our X matrix of feature inputs (the vegetation cover bands) and our y array (the labels), we can train our model. Visit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn . 1 2 gnb = GaussianNB () gnb . fit ( X_train , y_train ) 1 GaussianNB(priors=None, var_smoothing=1e-09) It's that simple to train a classifier in sckit-learn . The hard part is often validation and interpretation. Validation \u00b6 To see how well our classifier worked, we could use the test data we partioned earlier. However, we may want to adjust the model if our results are not as accurate as we'd like. This could lead to overfitting by 'leaking' information from the test set into our training of the model. Overfitting will hurt the performance of our model on predicting novel data, and will lead to inflated accuracy metrics. So how do we evaluate our model at this stage? Cross-validation . There are a few options for cross-validation, but for our purposes k-fold cross validation will work. 1 2 # 5-fold cross validation scores = cross_val_score ( gnb , X_train , y_train , cv = 5 ) scores stores the results of computing the score 5 consecutive times (with different splits each time) 1 scores 1 array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The mean score and the 95% confidence interval of the score estimate are hence given by: 1 print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.59 (+/- 0.002 ) Improving Model Accuracy \u00b6 Standardizing Values \u00b6 The accuracy is not as good as we'd like. How can we improve the accuracy of the model? One option is to standardize the data so that each of the features are similar in magnitude. This avoids some higher values from overwhelming lower values. It is often necessary to complete this step, depending on the model used. sklearn provides a preprocessing module that facilitate this scaling. 1 2 3 # Hide warnings for converting ints to floats (or save X, y as float64 type) import warnings warnings . filterwarnings ( \"ignore\" ) 1 2 gnb = make_pipeline ( preprocessing . StandardScaler (), GaussianNB ()) cross_val_score ( gnb , X_train , y_train , cv = 5 ) 1 array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The accuracy is exactly the same! This is because the Gaussian Naive Bayes is robust to scaling. In essence Gaussian Naive Bayes performs standardization internally . Balancing Classes \u00b6 Unbalanced classes can also impact the accuracy of a model. Imagine you have data on a rare disease that only 0.01% of people have. A simple model that predicts everyone does not have the disease would be right 99.99% of the time! To get a model that can actually predict when people do have the disease, you'll need to address the imbalanced classes. Again, this is an issue depending on the model used. We'll try a different type of model later in the tutorial that is more robust to imbalanced data (Random Forests). To balance the data, you can either down-sample the over-represented classes or up-sample the under-represented classes. How unbalanced are the classes now? \u00b6 Let's quickly plot the number of each label so we know how unbalanced the data are. 1 2 3 import pandas as pd df_plot = pd . DataFrame ( y_train ) df_plot [ 0 ] . value_counts () . plot ( kind = 'bar' ) 1 <matplotlib.axes._subplots.AxesSubplot at 0x22899b1eda0> We'll try up-sampling first so we don't reduce the number of data points too much. How much to upsample? We'll resample each of the less represented features with replacement to get the number of features contained in the most represented class. 1 df_plot [ 0 ] . value_counts () 1 2 3 4 5 6 P-J 2877442 Perennial Shrub 1388305 Encroached Shrub 375820 Loamy Bottom 85357 Wet/Salt Meadow 28755 Name: 0, dtype: int64 This will be easier with a pandas DataFrame, so let's convert our data to a DataFrame. 1 2 3 4 5 6 df = pd . concat ( [ pd . DataFrame ( y_train , columns = [ 'label' ]), pd . DataFrame ( X_train )], axis = 1 ) . set_index ( 'label' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 label P-J 18 19 12 12 11 13 P-J 9 9 7 8 7 13 P-J 18 24 21 22 20 20 Perennial Shrub 14 18 16 12 14 13 P-J 4 5 9 9 7 7 1 2 3 4 df_shrub = resample ( df . loc [ 'Perennial Shrub' ], replace = True , n_samples = df . index . value_counts () . max ()) df_shrub . shape [ 0 ] 1 2877442 Now we have the same number of Perennial Shrub classes as P-J classes, our dominant class type. Let's repeat for the remaining features. We'll loop through our labels and concatenate the results to the most represented class so that this step is robust to changes in which features we explore. 1 df . index . value_counts () . max () 1 2877442 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Start by subsetting the most represented class max_class = df . index . value_counts () . idxmax () df_upsampled = df . loc [ max_class ] labels = list ( df . index . unique ()) labels . remove ( max_class ) # For each label, resample the features to balance classes and append # to the empty dataframe for label in labels : df_temp = resample ( df . loc [ label ], replace = True , n_samples = df . index . value_counts () . max () ) df_upsampled = pd . concat ([ df_upsampled , df_temp ], axis = 0 ) df_upsampled . index . value_counts () 1 2 3 4 5 6 P-J 2877442 Encroached Shrub 2877442 Perennial Shrub 2877442 Wet/Salt Meadow 2877442 Loamy Bottom 2877442 Name: label, dtype: int64 Now, we can split out our X and y data again and re-train the model on the more balanced classes. 1 2 X_train_upsampled = df_upsampled . reset_index () . drop ( 'label' , axis = 1 ) y_train_upsampled = df_upsampled . index . values 1 2 3 4 # 5-fold cross validation scores = cross_val_score ( gnb , X_train_upsampled , y_train_upsampled , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.24 (+/- 0.001 ) Our accuracy took a bit of a nosedive, as now we're probably over-representing those less common classes on the landscape. For the purposes of our model, it may actually be better to simply ignore those more rare classes instead of over-representing them. We could try downsampling instead, but it will likely not help our accuracy very much. Instead, let's try a different model, Random Forests. Alternative Model: Random Forests \u00b6 Random Forests is robust to unscaled and unbalanced data, making it a good option for this classification problem out-of-the-box. It's not a bad idea to scale the data as we did earlier, but since our data is well scaled (percent cover data from 0 - 100%), we'll skip this step. Acknowledgements to this tutorial used in developing this section. 1 2 3 4 5 6 7 8 %% time from sklearn.ensemble import RandomForestClassifier # Initialize our model with 10 estimators to limit processing time rfc = RandomForestClassifier ( n_estimators = 10 , random_state = 0 ) # 5-fold cross validation scores = cross_val_score ( rfc , X_train , y_train , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 2 Accuracy : 0.58 (+/- 0.001 ) Wall time : 3 h 13 min 13 s Our accuracy (57%) is slightly less than our accuracy with the unbalanced classes using Naive Bayes (59%) but comparable. Confusion Matrix \u00b6 We can visualize how well we're classifying each class (and where the model is getting confused) using a confusion matrix . The code for the confusion matrix is copied from the linked documentation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels def plot_confusion_matrix ( y_true , y_pred , classes , normalize = False , title = None , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if not title : if normalize : title = 'Normalized confusion matrix' else : title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix ( y_true , y_pred ) # Only use the labels that appear in the data classes = unique_labels ( y_true , y_pred ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) fig , ax = plt . subplots ( figsize = ( 15 , 10 )) im = ax . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) ax . figure . colorbar ( im , ax = ax ) # We want to show all ticks... ax . set ( xticks = np . arange ( cm . shape [ 1 ]), yticks = np . arange ( cm . shape [ 0 ]), # ... and label them with the respective list entries xticklabels = classes , yticklabels = classes , title = title , ylabel = 'True label' , xlabel = 'Predicted label' ) # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 45 , ha = \"right\" , rotation_mode = \"anchor\" ) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i in range ( cm . shape [ 0 ]): for j in range ( cm . shape [ 1 ]): ax . text ( j , i , format ( cm [ i , j ], fmt ), ha = \"center\" , va = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) fig . tight_layout () return ax 1 2 # Use the model to predict on the X_train dataset y_pred = gnb . fit ( X_train , y_train ) . predict ( X_train ) 1 2 class_names = unique_labels ( y_train , y_pred ) np . set_printoptions ( precision = 2 ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.90e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.91e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.17e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.71e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] The confusion matrix indicates that the model is catogorizing most pixels as P-J and some as Perennial Shrub, with just a few Wet Meadow predictions. It ignores Encroached Shrub and Loamy Bottom. Focusing on the final row of the matrix, you can see that the Wet/Salt Meadow is being categorized as P-J 75% of the time, Perennial Shrub 24% of the time, and its correct label only 1% of the time. 1 y_pred_rfc = rfc . fit ( X_train , y_pred ) . predict ( X_train ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred_rfc , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.87e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.85e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.06e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.69e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] Interestingly, the confusion matrix for both the Naive Bayes and Random Forests model is the same. This may signal that we've done about as well as we could with these features. We could incorporate additional features through the same process as we went through before, considering topography, soils, precipitation, and spatial measures using distances or moving windows. We should also carefully consider feature selection to optimize the bias-variance tradeoffs. Feature engingeering and feature selection is beyond the scope of this tutorial. A classification report will provide reportable metrics of model accuracy to allow us document the performance of the model. It reports precision, recall and the F1 score. Precision is the ability of the model to avoid false positives. Recall is the ability to identify the feature correctly (notice it is equivalent to where the diagonal axis of the confusion matrix). The F1 score is a harmonic mean of precision and recall. 1 2 from sklearn.metrics import classification_report print ( classification_report ( y_train , y_pred , target_names = class_names )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 C : \\ Users \\ Erik \\ Anaconda3 \\ lib \\ site - packages \\ sklearn \\ metrics \\ classification . py : 1143 : UndefinedMetricWarning : Precision and F - score are ill - defined and being set to 0 . 0 in labels with no predicted samples . 'precision' , 'predicted' , average , warn_for ) precision recall f1 - score support Encroached Shrub 0 . 00 0 . 00 0 . 00 375820 Loamy Bottom 0 . 00 0 . 00 0 . 00 85357 P - J 0 . 62 0 . 89 0 . 73 2877442 Perennial Shrub 0 . 40 0 . 19 0 . 26 1388305 Wet / Salt Meadow 0 . 02 0 . 01 0 . 02 28755 micro avg 0 . 59 0 . 59 0 . 59 4755679 macro avg 0 . 21 0 . 22 0 . 20 4755679 weighted avg 0 . 49 0 . 59 0 . 52 4755679 \u200b Predicting on the Image \u00b6 With our classifier fit, we can now proceed by trying to classify the entire image. 1 2 3 4 5 6 7 8 9 range_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\range_clip_utm.tif' with rasterio . open ( range_path ) as src : profile = src . profile # the src profile will be used to save the output later img = src . read () # Take our full iamge and reshape into long 2d array (nrow * ncol, nband) for classification print ( img . shape ) reshaped_img = reshape_as_image ( img ) print ( reshaped_img . shape ) 1 2 (6, 9412, 7341) (9412, 7341, 6) Now we can predict for each pixel in our image. 1 2 3 4 class_prediction = gnb . predict ( reshaped_img . reshape ( - 1 , 6 )) # Reshape our classification map back into a 2d matrix so we can visualize it class_prediction = class_prediction . reshape ( reshaped_img [:, :, 0 ] . shape ) Because we used labels as strings we will want to convert them to numpy array with integers using the helper function we made earlier. 1 2 3 # This function converts the class prediction to ints from strings because it was originally created as a string # See template notebook for more class_prediction = str_class_to_int ( class_prediction ) Visualize the Results \u00b6 First we'll make a colormap so we can adjust the colors of each class to more logical colors. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def color_stretch ( image , index ): colors = image [:, :, index ] . astype ( np . float64 ) for b in range ( colors . shape [ 2 ]): colors [:, :, b ] = rasterio . plot . adjust_band ( colors [:, :, b ]) return colors # find the highest pixel value in the prediction image n = int ( np . max ( class_prediction )) # next setup a colormap for our map colors = dict (( ( 1 , ( 34 , 139 , 34 , 255 )), # Forest Green - PJ ( 2 , ( 139 , 69 , 19 , 255 )), # Brown - Perennial Shrub ( 3 , ( 48 , 156 , 214 , 255 )), # Blue - Encroached Shrub ( 4 , ( 244 , 164 , 96 , 255 )), # Tan - Loamy Bottom ( 5 , ( 206 , 224 , 196 , 255 )), # Lime - Grass Meadow )) # Put 0 - 255 as float 0 - 1 for k in colors : v = colors [ k ] _v = [ _v / 255.0 for _v in v ] colors [ k ] = _v index_colors = [ colors [ key ] if key in colors else ( 255 , 255 , 255 , 0 ) for key in range ( 0 , n + 1 )] cmap = plt . matplotlib . colors . ListedColormap ( index_colors , 'Classification' , n + 1 ) 1 show ( class_prediction ) 1 <matplotlib.axes._subplots.AxesSubplot at 0xbeb7d2278> 1 2 3 4 5 6 7 8 9 with rasterio . Env (): profile . update ( dtype = rasterio . uint8 , count = 1 , compress = 'lzw' ) with rasterio . open ( r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\example.tif' , 'w' , ** profile ) as dst : dst . write ( class_prediction . astype ( rasterio . uint8 ), 1 ) Improving our Model Accuracy \u00b6 In the following sections (TBD), we'll improve upon our model, evaluate accuracy, and explore different classifiers * Normalize values * Split into training and testing data * Balance classes in training data * Introduce additional features (elevation, precip, soil temperature, aspect, moving window stats, imagery) * Explore different classifiers * Feature Importance * Parameter tuning * Correlations Evaluation Accuracy 1","title":"Machine Learning Tutorial"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#machine-learning-tutorial","text":"This tutorial is intended to illustrate a typical workflow for machine learning to solve a land use and land cover classification problem. Land Use and Land Cover classifications are used to identify the dominant land cover or land use type in an area. We use the Naive Bayes and Random Forest classifiers, as implemented within scikit-learn library. This tutorial borrows heavily from the very helpful tutorial developed by Chris Holden and updated by Patrick Gray. Also used: rasterio , geopandas , numpy , pandas , shapely , and matplotlib .","title":"Machine Learning Tutorial"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#the-challenge","text":"Our client required a rapid approach for evaluating the benefits of conservation projects to Mule Deer. We proposed using Ecological State and Transition Models (STMs) as the basis for the evaluation. The NRCS has developed STMs for the dominant ecological sites within the region, however only a subset of the region was mapped. We use vegetation data (provided by the Rangelands App ) and other environmental variables to predict STM for the unmapped areas of the range.","title":"The Challenge"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#import-statements","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import rasterio from rasterio.mask import mask from rasterio.plot import show from rasterio.plot import show_hist from rasterio.windows import Window from rasterio.plot import reshape_as_raster , reshape_as_image import geopandas as gpd import numpy as np from shapely.geometry import mapping from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn import preprocessing from sklearn.pipeline import make_pipeline from sklearn.utils import resample import matplotlib.pyplot as plt # ipywidgets is used to create a progress bar from ipywidgets import IntProgress from IPython.display import display 1 % matplotlib inline","title":"Import Statements"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#import-data","text":"We'll be using supervised classification techniques. We'll need both labels and predictor features to train the models. Our labels will come from the mapped STM derived from NRCS data. To begin, we'll use vegetation data as predictors, including cover estimates for trees, shrubs, perennial grasses and forbs, and bare ground. 1 2 3 4 5 # CHANGE TO VEG_COVER_PATH # Read in features and training data train_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\train_clip_utm.tif' data = rasterio . open ( train_path ) data . crs # Check the projection, all features must share a projection 1 CRS.from_dict(init='epsg:26913') 1 2 3 labels_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\labels.shp' labels = gpd . read_file ( labels_path ) labels . crs 1 {'init': 'epsg:26913'} 1 len ( labels ) 1 6136 We have 6,136 labeled polygons, represented within a geopandas dataframe, while our predictor features are represented in a rasterio raster. Both share the same projection.","title":"Import Data"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#explore-data-and-preparation-steps","text":"To train the classifiers, we'll need to associate our vector data (labels as polygons) with our raster pixels (predictor features). We'll accomplish this with the rasterio mask function. The mask function will essentially clip (or mask) our raster with each polygon. First, we'll want to extract the geometry of each feature in the labels shapefile to GeoJSON format. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # this generates a list of shapely geometries geoms = labels . geometry . values # let's grab a single shapely geometry to check geometry = geoms [ 0 ] print ( \"This is a shapely polygon\" ) print ( type ( geometry )) print ( geometry ) # transform to GeoJSON format (note 'mapping' is in the shapely namespace) feature = [ mapping ( geometry )] # can also do this using polygon.__geo_interface__ print ( \"This is the same polygon in GeoJSON format\" ) print ( type ( feature )) print ( feature ) 1 2 3 4 5 6 This is a shapely polygon <class 'shapely.geometry.polygon.Polygon'> POLYGON ((155090.8039999995 4317864.1029, 155095.9642000003 4317847.2246, 155096.9452999998 4317767.406400001, 155079.9972999999 4317764.4924, 155013.6201999998 4317772.8774, 154946.0262000002 4317762.761399999, 154906.4253000002 4317765.351500001, 154878.2922999999 4317781.742900001, 154874.5252 4317788.509099999, 154877.2857999997 4317829.973099999, 154890.6249000002 4317873.230799999, 154892.2852999996 4317898.202400001, 154873.1852000002 4318020.8462, 154866.6454999996 4318032.8059, 154861.3509 4318094.8554, 154880.1355999997 4318314.824999999, 154876.2439000001 4318350.6874, 154879.5504999999 4318400.631100001, 154884.4243000001 4318410.842, 154890.0175999999 4318527.3519, 154897.2582 4318573.0209, 154890.3213 4318594.527100001, 154894.7226999998 4318598.269300001, 154913.1912000002 4318593.0416, 154946.1224999996 4318553.260600001, 154952.6897 4318463.052999999, 154967.4232000001 4318401.3926, 154951.2766000004 4318220.3751, 154975.3370000003 4318141.542099999, 155019.5067999996 4318081.9695, 155041.7379000001 4318038.8718, 155041.0279000001 4317965.691199999, 155054.3926999997 4317914.6457, 155080.8075000001 4317871.2906, 155090.8039999995 4317864.1029)) This is the same polygon in GeoJSON format <class 'list'> [{'type': 'Polygon', 'coordinates': (((155090.80399999954, 4317864.1029), (155095.96420000028, 4317847.2246), (155096.9452999998, 4317767.406400001), (155079.99729999993, 4317764.4924), (155013.62019999977, 4317772.8774), (154946.0262000002, 4317762.761399999), (154906.42530000024, 4317765.351500001), (154878.29229999986, 4317781.742900001), (154874.52520000003, 4317788.509099999), (154877.28579999972, 4317829.973099999), (154890.62490000017, 4317873.230799999), (154892.28529999964, 4317898.202400001), (154873.18520000018, 4318020.8462000005), (154866.64549999963, 4318032.8059), (154861.35089999996, 4318094.8554), (154880.1355999997, 4318314.824999999), (154876.24390000012, 4318350.6874), (154879.5504999999, 4318400.631100001), (154884.42430000007, 4318410.842), (154890.0175999999, 4318527.3519), (154897.25820000004, 4318573.0209), (154890.32129999995, 4318594.5271000005), (154894.7226999998, 4318598.269300001), (154913.19120000023, 4318593.0416), (154946.1224999996, 4318553.260600001), (154952.6897, 4318463.052999999), (154967.42320000008, 4318401.3926), (154951.27660000045, 4318220.3751), (154975.3370000003, 4318141.542099999), (155019.50679999962, 4318081.9695), (155041.73790000007, 4318038.8718), (155041.0279000001, 4317965.691199999), (155054.39269999973, 4317914.6457), (155080.8075000001, 4317871.2906), (155090.80399999954, 4317864.1029)),)}] Now let's extract the raster values within each polygon using the rasterio mask() function . 1 out_image , out_transform = mask ( data , feature , crop = True ) 1 out_image . shape 1 (6, 32, 10) 1 show ( out_image [ 0 ]) 1 <matplotlib.axes._subplots.AxesSubplot at 0xbea5c5b70> As you can see, the features raster was clipped to a single polygon. There are 6 bands and 32x10 pixels. We'll repeat this process for all 6,136 polygons to build our dataset. We'll also need to clean this raster up a bit before we use it in training. We'll explore all of this next. But first, we'll be doing a lot of memory intensive work so we'll close the dataset for now. 1 data . close ()","title":"Explore Data and Preparation Steps"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#build-the-training-data-for-sckit-learn","text":"We'll repeat the above process for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels. Note that the column 'MuleDeer_1' in the labels geodataframe has the label we're after. TODO: change to column with label instead of numeric. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 %% time # Lets create a progress bar as this step can take some time p_bar = IntProgress ( min = 0 , max = len ( geoms ), description = 'Processing' ) display ( p_bar ) # Set up arrays X = np . array ([], dtype = np . int8 ) . reshape ( 0 , 6 ) # pixels for training y = np . array ([], dtype = np . string_ ) # labels for training # extract the raster values within the polygon with rasterio . open ( train_path ) as src : band_count = src . count for index , geom in enumerate ( geoms ): # Note geoms was created above feature = [ mapping ( geom )] # the mask function returns an array of the raster pixels within this feature out_image , out_transform = mask ( src , feature , crop = True ) # eliminate all the pixels with 0 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image [:, ~ np . all ( out_image == 0 , axis = 0 )] # eliminate all the pixels with 255 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image_trimmed [:, ~ np . all ( out_image_trimmed == 255 , axis = 0 )] # reshape the array to [pixel count, bands] out_image_reshaped = out_image_trimmed . reshape ( - 1 , band_count ) # append the labels to the y array y = np . append ( y , [ labels [ \"MuleDeer_1\" ][ index ]] * out_image_reshaped . shape [ 0 ]) # ??? # stack the pixels onto the pixel array X = np . vstack (( X , out_image_reshaped )) # increment the progress bar p_bar . value += 1 1 2 3 4 IntProgress ( value = 0 , description = 'Processing' , max = 6136 ) Wall time : 25 min 33 s","title":"Build the Training Data for sckit-learn"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#save-the-output","text":"We'll save the output as a numpy array to avoid the long process of rebuilding the features in the future. This will also allow us to share this analysis with others without them needing access to the input data. If this were not a tutorial, I might have ended the notebook here and started a new one for the remainder of the analysis. I've commented out the load statements below, uncomment to load in the saved features and labels. 1 2 3 # Save features and labels np . save ( 'E:/lulc-features.npy' , X ) np . save ( 'E:/lulc-labels.npy' , y ) 1 2 3 # Load in data X = np . load ( 'E:/lulc-features.npy' ) # fill in path y = np . load ( 'E:/lulc-labels.npy' )","title":"Save the output"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#splitting-the-data-for-testing","text":"In order to evaluate the accuracy of our model, we'll reserve a subset of the data for testing. The train_test_split function allows us to quickly and randomly subset our data for this purpose. 1 2 # split out 30% of data for testing. Random state set for reproducibility. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 )","title":"Splitting the data for testing"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#pairing-y-with-x","text":"Now that we have the image we want to classify (X_train) and the land cover labels (y_train), let's check to make sure they match in size so we can feed them to our models. 1 2 3 4 5 6 7 st_names = np . unique ( y_train ) print ( 'The training data include {n} classes: {classes} \\n ' . format ( n = st_names . size , classes = st_names )) # We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels print ( 'Our X matrix is sized: {sz} ' . format ( sz = X_train . shape )) print ( 'Our y array is sized: {sz} ' . format ( sz = y_train . shape )) 1 2 3 4 5 The training data include 5 classes : [ 'Encroached Shrub' 'Loamy Bottom' 'P-J' 'Perennial Shrub' 'Wet/Salt Meadow' ] Our X matrix is sized : ( 4755679 , 6 ) Our y array is sized : ( 4755679 ,) That looks good. We have 5 classes (i.e., our STM names); 6 predictor features (i.e., the 6 bands in our X matrix, now flattened; and both the X and y array are the same length. We'll treat these vegetation cover values as spectral signatures, and plot each to make sure they're actually separable since all we're going by in this classification is pixel values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 fig , ax = plt . subplots ( 1 , 3 , figsize = [ 20 , 8 ]) # bands are numbered 1 through 6 following GDAL convention band_count = np . arange ( 1 , 7 ) classes = np . unique ( y_train ) for class_type in classes : band_intensity = np . mean ( X_train [ y_train == class_type , :], axis = 0 ) ax [ 0 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 1 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 2 ] . plot ( band_count , band_intensity , label = class_type ) #plot them as lines # Add some axis labels # Add some axis labels ax [ 0 ] . set_xlabel ( 'Band #' ) ax [ 0 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_xlabel ( 'Band #' ) ax [ 2 ] . set_ylabel ( 'Reflectance Value' ) ax [ 2 ] . set_xlabel ( 'Band #' ) ax [ 1 ] . legend ( loc = \"upper right\" ) # Add a title ax [ 0 ] . set_title ( 'Band Intensities Full Overview' ) ax [ 1 ] . set_title ( 'Band Intensities Lower Ref Subset' ) ax [ 2 ] . set_title ( 'Band Intensities Higher Ref Subset' ) plt . show () Looks like each class will be easily separable. This will be a helper function to convert class labels into indices so we're predicting to integers instead of strings. TODO: switch labels and numbers 1 2 3 4 5 6 7 def str_class_to_int ( class_array ): class_array [ class_array == 'P-J' ] = 1 class_array [ class_array == 'Perennial Shrub' ] = 2 class_array [ class_array == 'Encroached Shrub' ] = 3 class_array [ class_array == 'Loamy Bottom' ] = 4 class_array [ class_array == 'Wet/Salt Meadow' ] = 5 return ( class_array . astype ( int ))","title":"Pairing y with X"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#training-the-classifier","text":"Now that we have our X matrix of feature inputs (the vegetation cover bands) and our y array (the labels), we can train our model. Visit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn . 1 2 gnb = GaussianNB () gnb . fit ( X_train , y_train ) 1 GaussianNB(priors=None, var_smoothing=1e-09) It's that simple to train a classifier in sckit-learn . The hard part is often validation and interpretation.","title":"Training the Classifier"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#validation","text":"To see how well our classifier worked, we could use the test data we partioned earlier. However, we may want to adjust the model if our results are not as accurate as we'd like. This could lead to overfitting by 'leaking' information from the test set into our training of the model. Overfitting will hurt the performance of our model on predicting novel data, and will lead to inflated accuracy metrics. So how do we evaluate our model at this stage? Cross-validation . There are a few options for cross-validation, but for our purposes k-fold cross validation will work. 1 2 # 5-fold cross validation scores = cross_val_score ( gnb , X_train , y_train , cv = 5 ) scores stores the results of computing the score 5 consecutive times (with different splits each time) 1 scores 1 array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The mean score and the 95% confidence interval of the score estimate are hence given by: 1 print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.59 (+/- 0.002 )","title":"Validation"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#improving-model-accuracy","text":"","title":"Improving Model Accuracy"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#standardizing-values","text":"The accuracy is not as good as we'd like. How can we improve the accuracy of the model? One option is to standardize the data so that each of the features are similar in magnitude. This avoids some higher values from overwhelming lower values. It is often necessary to complete this step, depending on the model used. sklearn provides a preprocessing module that facilitate this scaling. 1 2 3 # Hide warnings for converting ints to floats (or save X, y as float64 type) import warnings warnings . filterwarnings ( \"ignore\" ) 1 2 gnb = make_pipeline ( preprocessing . StandardScaler (), GaussianNB ()) cross_val_score ( gnb , X_train , y_train , cv = 5 ) 1 array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The accuracy is exactly the same! This is because the Gaussian Naive Bayes is robust to scaling. In essence Gaussian Naive Bayes performs standardization internally .","title":"Standardizing Values"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#balancing-classes","text":"Unbalanced classes can also impact the accuracy of a model. Imagine you have data on a rare disease that only 0.01% of people have. A simple model that predicts everyone does not have the disease would be right 99.99% of the time! To get a model that can actually predict when people do have the disease, you'll need to address the imbalanced classes. Again, this is an issue depending on the model used. We'll try a different type of model later in the tutorial that is more robust to imbalanced data (Random Forests). To balance the data, you can either down-sample the over-represented classes or up-sample the under-represented classes.","title":"Balancing Classes"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#how-unbalanced-are-the-classes-now","text":"Let's quickly plot the number of each label so we know how unbalanced the data are. 1 2 3 import pandas as pd df_plot = pd . DataFrame ( y_train ) df_plot [ 0 ] . value_counts () . plot ( kind = 'bar' ) 1 <matplotlib.axes._subplots.AxesSubplot at 0x22899b1eda0> We'll try up-sampling first so we don't reduce the number of data points too much. How much to upsample? We'll resample each of the less represented features with replacement to get the number of features contained in the most represented class. 1 df_plot [ 0 ] . value_counts () 1 2 3 4 5 6 P-J 2877442 Perennial Shrub 1388305 Encroached Shrub 375820 Loamy Bottom 85357 Wet/Salt Meadow 28755 Name: 0, dtype: int64 This will be easier with a pandas DataFrame, so let's convert our data to a DataFrame. 1 2 3 4 5 6 df = pd . concat ( [ pd . DataFrame ( y_train , columns = [ 'label' ]), pd . DataFrame ( X_train )], axis = 1 ) . set_index ( 'label' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 label P-J 18 19 12 12 11 13 P-J 9 9 7 8 7 13 P-J 18 24 21 22 20 20 Perennial Shrub 14 18 16 12 14 13 P-J 4 5 9 9 7 7 1 2 3 4 df_shrub = resample ( df . loc [ 'Perennial Shrub' ], replace = True , n_samples = df . index . value_counts () . max ()) df_shrub . shape [ 0 ] 1 2877442 Now we have the same number of Perennial Shrub classes as P-J classes, our dominant class type. Let's repeat for the remaining features. We'll loop through our labels and concatenate the results to the most represented class so that this step is robust to changes in which features we explore. 1 df . index . value_counts () . max () 1 2877442 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Start by subsetting the most represented class max_class = df . index . value_counts () . idxmax () df_upsampled = df . loc [ max_class ] labels = list ( df . index . unique ()) labels . remove ( max_class ) # For each label, resample the features to balance classes and append # to the empty dataframe for label in labels : df_temp = resample ( df . loc [ label ], replace = True , n_samples = df . index . value_counts () . max () ) df_upsampled = pd . concat ([ df_upsampled , df_temp ], axis = 0 ) df_upsampled . index . value_counts () 1 2 3 4 5 6 P-J 2877442 Encroached Shrub 2877442 Perennial Shrub 2877442 Wet/Salt Meadow 2877442 Loamy Bottom 2877442 Name: label, dtype: int64 Now, we can split out our X and y data again and re-train the model on the more balanced classes. 1 2 X_train_upsampled = df_upsampled . reset_index () . drop ( 'label' , axis = 1 ) y_train_upsampled = df_upsampled . index . values 1 2 3 4 # 5-fold cross validation scores = cross_val_score ( gnb , X_train_upsampled , y_train_upsampled , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.24 (+/- 0.001 ) Our accuracy took a bit of a nosedive, as now we're probably over-representing those less common classes on the landscape. For the purposes of our model, it may actually be better to simply ignore those more rare classes instead of over-representing them. We could try downsampling instead, but it will likely not help our accuracy very much. Instead, let's try a different model, Random Forests.","title":"How unbalanced are the classes now?"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#alternative-model-random-forests","text":"Random Forests is robust to unscaled and unbalanced data, making it a good option for this classification problem out-of-the-box. It's not a bad idea to scale the data as we did earlier, but since our data is well scaled (percent cover data from 0 - 100%), we'll skip this step. Acknowledgements to this tutorial used in developing this section. 1 2 3 4 5 6 7 8 %% time from sklearn.ensemble import RandomForestClassifier # Initialize our model with 10 estimators to limit processing time rfc = RandomForestClassifier ( n_estimators = 10 , random_state = 0 ) # 5-fold cross validation scores = cross_val_score ( rfc , X_train , y_train , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 2 Accuracy : 0.58 (+/- 0.001 ) Wall time : 3 h 13 min 13 s Our accuracy (57%) is slightly less than our accuracy with the unbalanced classes using Naive Bayes (59%) but comparable.","title":"Alternative Model: Random Forests"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#confusion-matrix","text":"We can visualize how well we're classifying each class (and where the model is getting confused) using a confusion matrix . The code for the confusion matrix is copied from the linked documentation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels def plot_confusion_matrix ( y_true , y_pred , classes , normalize = False , title = None , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if not title : if normalize : title = 'Normalized confusion matrix' else : title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix ( y_true , y_pred ) # Only use the labels that appear in the data classes = unique_labels ( y_true , y_pred ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) fig , ax = plt . subplots ( figsize = ( 15 , 10 )) im = ax . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) ax . figure . colorbar ( im , ax = ax ) # We want to show all ticks... ax . set ( xticks = np . arange ( cm . shape [ 1 ]), yticks = np . arange ( cm . shape [ 0 ]), # ... and label them with the respective list entries xticklabels = classes , yticklabels = classes , title = title , ylabel = 'True label' , xlabel = 'Predicted label' ) # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 45 , ha = \"right\" , rotation_mode = \"anchor\" ) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i in range ( cm . shape [ 0 ]): for j in range ( cm . shape [ 1 ]): ax . text ( j , i , format ( cm [ i , j ], fmt ), ha = \"center\" , va = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) fig . tight_layout () return ax 1 2 # Use the model to predict on the X_train dataset y_pred = gnb . fit ( X_train , y_train ) . predict ( X_train ) 1 2 class_names = unique_labels ( y_train , y_pred ) np . set_printoptions ( precision = 2 ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.90e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.91e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.17e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.71e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] The confusion matrix indicates that the model is catogorizing most pixels as P-J and some as Perennial Shrub, with just a few Wet Meadow predictions. It ignores Encroached Shrub and Loamy Bottom. Focusing on the final row of the matrix, you can see that the Wet/Salt Meadow is being categorized as P-J 75% of the time, Perennial Shrub 24% of the time, and its correct label only 1% of the time. 1 y_pred_rfc = rfc . fit ( X_train , y_pred ) . predict ( X_train ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred_rfc , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.87e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.85e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.06e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.69e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] Interestingly, the confusion matrix for both the Naive Bayes and Random Forests model is the same. This may signal that we've done about as well as we could with these features. We could incorporate additional features through the same process as we went through before, considering topography, soils, precipitation, and spatial measures using distances or moving windows. We should also carefully consider feature selection to optimize the bias-variance tradeoffs. Feature engingeering and feature selection is beyond the scope of this tutorial. A classification report will provide reportable metrics of model accuracy to allow us document the performance of the model. It reports precision, recall and the F1 score. Precision is the ability of the model to avoid false positives. Recall is the ability to identify the feature correctly (notice it is equivalent to where the diagonal axis of the confusion matrix). The F1 score is a harmonic mean of precision and recall. 1 2 from sklearn.metrics import classification_report print ( classification_report ( y_train , y_pred , target_names = class_names )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 C : \\ Users \\ Erik \\ Anaconda3 \\ lib \\ site - packages \\ sklearn \\ metrics \\ classification . py : 1143 : UndefinedMetricWarning : Precision and F - score are ill - defined and being set to 0 . 0 in labels with no predicted samples . 'precision' , 'predicted' , average , warn_for ) precision recall f1 - score support Encroached Shrub 0 . 00 0 . 00 0 . 00 375820 Loamy Bottom 0 . 00 0 . 00 0 . 00 85357 P - J 0 . 62 0 . 89 0 . 73 2877442 Perennial Shrub 0 . 40 0 . 19 0 . 26 1388305 Wet / Salt Meadow 0 . 02 0 . 01 0 . 02 28755 micro avg 0 . 59 0 . 59 0 . 59 4755679 macro avg 0 . 21 0 . 22 0 . 20 4755679 weighted avg 0 . 49 0 . 59 0 . 52 4755679 \u200b","title":"Confusion Matrix"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#predicting-on-the-image","text":"With our classifier fit, we can now proceed by trying to classify the entire image. 1 2 3 4 5 6 7 8 9 range_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\range_clip_utm.tif' with rasterio . open ( range_path ) as src : profile = src . profile # the src profile will be used to save the output later img = src . read () # Take our full iamge and reshape into long 2d array (nrow * ncol, nband) for classification print ( img . shape ) reshaped_img = reshape_as_image ( img ) print ( reshaped_img . shape ) 1 2 (6, 9412, 7341) (9412, 7341, 6) Now we can predict for each pixel in our image. 1 2 3 4 class_prediction = gnb . predict ( reshaped_img . reshape ( - 1 , 6 )) # Reshape our classification map back into a 2d matrix so we can visualize it class_prediction = class_prediction . reshape ( reshaped_img [:, :, 0 ] . shape ) Because we used labels as strings we will want to convert them to numpy array with integers using the helper function we made earlier. 1 2 3 # This function converts the class prediction to ints from strings because it was originally created as a string # See template notebook for more class_prediction = str_class_to_int ( class_prediction )","title":"Predicting on the Image"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#visualize-the-results","text":"First we'll make a colormap so we can adjust the colors of each class to more logical colors. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def color_stretch ( image , index ): colors = image [:, :, index ] . astype ( np . float64 ) for b in range ( colors . shape [ 2 ]): colors [:, :, b ] = rasterio . plot . adjust_band ( colors [:, :, b ]) return colors # find the highest pixel value in the prediction image n = int ( np . max ( class_prediction )) # next setup a colormap for our map colors = dict (( ( 1 , ( 34 , 139 , 34 , 255 )), # Forest Green - PJ ( 2 , ( 139 , 69 , 19 , 255 )), # Brown - Perennial Shrub ( 3 , ( 48 , 156 , 214 , 255 )), # Blue - Encroached Shrub ( 4 , ( 244 , 164 , 96 , 255 )), # Tan - Loamy Bottom ( 5 , ( 206 , 224 , 196 , 255 )), # Lime - Grass Meadow )) # Put 0 - 255 as float 0 - 1 for k in colors : v = colors [ k ] _v = [ _v / 255.0 for _v in v ] colors [ k ] = _v index_colors = [ colors [ key ] if key in colors else ( 255 , 255 , 255 , 0 ) for key in range ( 0 , n + 1 )] cmap = plt . matplotlib . colors . ListedColormap ( index_colors , 'Classification' , n + 1 ) 1 show ( class_prediction ) 1 <matplotlib.axes._subplots.AxesSubplot at 0xbeb7d2278> 1 2 3 4 5 6 7 8 9 with rasterio . Env (): profile . update ( dtype = rasterio . uint8 , count = 1 , compress = 'lzw' ) with rasterio . open ( r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\example.tif' , 'w' , ** profile ) as dst : dst . write ( class_prediction . astype ( rasterio . uint8 ), 1 )","title":"Visualize the Results"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#improving-our-model-accuracy","text":"In the following sections (TBD), we'll improve upon our model, evaluate accuracy, and explore different classifiers * Normalize values * Split into training and testing data * Balance classes in training data * Introduce additional features (elevation, precip, soil temperature, aspect, moving window stats, imagery) * Explore different classifiers * Feature Importance * Parameter tuning * Correlations Evaluation Accuracy 1","title":"Improving our Model Accuracy"},{"location":"data-visualization/dashboards/","text":"Dashboards \u00b6","title":"Dashboards"},{"location":"data-visualization/dashboards/#dashboards","text":"","title":"Dashboards"},{"location":"data-visualization/data-visualization/","text":"Intro to Data Visualization \u00b6 Data visualization is the art of telling a story, with data, visually. A good storyteller will consider their audience, provide context, focus on what's important, and deliver the payoff in the end. As will a great visual data storyteller. This tutorial will help you on your path to being a great visual data storyteller, and provide suggested technologies to make your ideas come to life. There are thousands of resources available to you to improve your visualization skills, including online classes , YouTube videos, web articles, blogs , books , competitions , etc. You can even get an advanced degree in data visualization if you're so inclined. What follows is simply a collection of the best concepts and advice I've encountered. I encourage you to explore other sources, experiment, and even throw this right out if it doesn't speak to you. The process \u00b6 Most stories - at least the ones worth listening to - begin with a question. How did you get that scar? Great story actually\u2026 Once you've identified a data visualization need, consider the people who make up your audience: for which questions will they need answers? List all the questions you can think of and highlight the most important. You might find it useful to group similar questions. Next, inventory the data sources at your disposal. You will, after all, need data to create a data visualization. As is typically true of working with data, you may spend a disproportionate amount of your time collecting and cleaning the data you want to use. If the data you want is not attainable, you'll need to reframe your approach. Before you start cleaning data and creating visuals, stop . Step away from the computer. Find a sunny patio or a dimly lit coffee shop. Get out a piece of paper (I prefer the gridded engineering graph paper). Grab your pencils (colored pencils if you like - although I recommend designing first in black and white before judiciously adding color). Sketch . Pick a question from your list that is the most interesting to you. Given your understanding of the data available, how would you answer that question with a data visual? Which type of chart might work best? Sketch a few alternatives to see how they compare. Pick another question and repeat. Sketching first will save you time by helping you explore the story you want to tell and how you want to tell it. You can more quickly prototype ideas on paper than in any visualization software. You also won't feel hamstrung by the limitations of the software, or your limitations in working with the software. Skipping this step is how we end up with so many of the same boring bar charts in Excel's standard color scheme. Now is a good time to share your sketches with members of your intended audience to get their reactions. This is also a good time to co-create visuals with others. You can work together to pick the most important questions to answer and update the sketches to more intuitively answer them. With your draft visuals in hand, you should now have a clearer understanding of what type of visualization to develop and which visualization platform to use. Prepare and explore your data to see if you can discover new insights. Quickly test your sketches . Do the data work with (or against) your sketch? If your visuals will be dynamic (as with a dashboard, for example) test out different slices of the data or create random data to test edge cases. With many rounds of iteration, you will develop your visualizations . Next, strip away distraction and focus on the key message. This is where you make sure you deliver that payoff - the information your audience needs as succinctly, intuitively, and insightfully as possible. See the techniques and approaches sections for suggestions. Finally, deliver your visualization. To borrow from Paul Valery, \"a work of art is never finished, merely abandoned\" . The process, succinctly \u00b6 List the key questions of your audience Inventory data sources Sketch potential visuals Share your sketches with members of your audience Prepare and explore the data Test your sketches Develop your visualizations through iteration Strip away distraction Deliver Types of visualizations \u00b6 There's no official taxonomy of data visualizations, but here's a quick overview of a few important visualization types in my opinion: Chart : your standard static data visualization. The static chart's primary habitat is in reports and other static media. See also graphs. Infographic : a thematic collection of multiple static visualizations, often presented as a poster or banner. Dashboard : a thematic collection of one or more dynamic visualizations, updated at some regular interval. Data Story : a presentation of data that marries narrative with data visualization. See some great examples under Inspiration. Any of the above can support interactivity, animations, and/or maps. A static map would be considered a chart (just ask any sailor). By static I mean the underlying data do not change; data change with dynamic visualizations - which provides a unique challenge for visualizations. Visualization tools \u00b6 There are dozens (hundreds?) of visualization tools available to you. If you don't already have a preferred solution, here are a few suggestions. This list focuses on tools with a free tier. Of course, there's always Excel or Google Sheets - nothing wrong with these options - but I'm assuming you've already been introduced. Tableau : these folks are visualization nerds and incorporate by default the latest in visualization science in your visuals - which makes you look smart. Tableau Public is free but, you guessed it, makes your visualization public (so don't choose this solution if you're working with sensitive data, or get a paid account). Power BI : Power BI is a free desktop tool with an online service to allow you to publish your visuals and create dashboards. If your team is already using MS Office, it\u2019s a good choice for its easy integration with the Microsoft ecosystem and ability to share visuals within your organization. Google Charts & Data Studio : this is Google's answer to Microsoft's Power BI. Slick and intuitive, you can very quickly learn to create visuals and dashboards in an afternoon that look like they took a week. Plotly & Dash : Plotly's open-source Chart Studio allows you to upload data and quickly create interactive web-ready charts from their standard library. However, this solution really sings if you're able to use the Dash API with Python. Use this solution to develop and deploy analytic web apps, like interactive dashboards. Infogram : an option for creating infographics, Infogram allows up to 10 projects at the free tier. See also Visme , Adobe Illustrator , and Inkscape . Matplotlib & Seaborn : for Python developers, these are your go-to libraries for data visualization. ArcGIS StoryMaps : StoryMaps include straightforward interactive maps and the eponymous Story Map , a platform for creating data stories that typically (but don't have to!) include maps and other spatial components. More tips on visualization \u00b6 If you're wondering which visualization to use for a given task, start with this Visualization Vocabulary . Techniques \u00b6 Try out some of these techniques to improve your visualizations. Highlighting Transition Guidance (layering the story, point by point) Ordering Messaging Clarity Reduce but don't avoid complexity Interaction How to tell a story \u00b6 These are approaches to telling your data story. Change Over Time Drill Down Zoom Out Contrast Intersections Factors Outliers Communicating Uncertainty Inspiration \u00b6 How did I create an entire post about data visualization without a single data visualization? Check these out: Galleries \u00b6 Tableau Public Gallery Makeover Monday Gallery Dash App Gallery Philosophies \u00b6 Data Humanism Some great examples \u00b6 The Avalanche at Snow Falls (NYT) What's really warming the world (Bloomberg) Bussed Out (The Guardian)","title":"Data Visualization"},{"location":"data-visualization/data-visualization/#intro-to-data-visualization","text":"Data visualization is the art of telling a story, with data, visually. A good storyteller will consider their audience, provide context, focus on what's important, and deliver the payoff in the end. As will a great visual data storyteller. This tutorial will help you on your path to being a great visual data storyteller, and provide suggested technologies to make your ideas come to life. There are thousands of resources available to you to improve your visualization skills, including online classes , YouTube videos, web articles, blogs , books , competitions , etc. You can even get an advanced degree in data visualization if you're so inclined. What follows is simply a collection of the best concepts and advice I've encountered. I encourage you to explore other sources, experiment, and even throw this right out if it doesn't speak to you.","title":"Intro to Data Visualization"},{"location":"data-visualization/data-visualization/#the-process","text":"Most stories - at least the ones worth listening to - begin with a question. How did you get that scar? Great story actually\u2026 Once you've identified a data visualization need, consider the people who make up your audience: for which questions will they need answers? List all the questions you can think of and highlight the most important. You might find it useful to group similar questions. Next, inventory the data sources at your disposal. You will, after all, need data to create a data visualization. As is typically true of working with data, you may spend a disproportionate amount of your time collecting and cleaning the data you want to use. If the data you want is not attainable, you'll need to reframe your approach. Before you start cleaning data and creating visuals, stop . Step away from the computer. Find a sunny patio or a dimly lit coffee shop. Get out a piece of paper (I prefer the gridded engineering graph paper). Grab your pencils (colored pencils if you like - although I recommend designing first in black and white before judiciously adding color). Sketch . Pick a question from your list that is the most interesting to you. Given your understanding of the data available, how would you answer that question with a data visual? Which type of chart might work best? Sketch a few alternatives to see how they compare. Pick another question and repeat. Sketching first will save you time by helping you explore the story you want to tell and how you want to tell it. You can more quickly prototype ideas on paper than in any visualization software. You also won't feel hamstrung by the limitations of the software, or your limitations in working with the software. Skipping this step is how we end up with so many of the same boring bar charts in Excel's standard color scheme. Now is a good time to share your sketches with members of your intended audience to get their reactions. This is also a good time to co-create visuals with others. You can work together to pick the most important questions to answer and update the sketches to more intuitively answer them. With your draft visuals in hand, you should now have a clearer understanding of what type of visualization to develop and which visualization platform to use. Prepare and explore your data to see if you can discover new insights. Quickly test your sketches . Do the data work with (or against) your sketch? If your visuals will be dynamic (as with a dashboard, for example) test out different slices of the data or create random data to test edge cases. With many rounds of iteration, you will develop your visualizations . Next, strip away distraction and focus on the key message. This is where you make sure you deliver that payoff - the information your audience needs as succinctly, intuitively, and insightfully as possible. See the techniques and approaches sections for suggestions. Finally, deliver your visualization. To borrow from Paul Valery, \"a work of art is never finished, merely abandoned\" .","title":"The process"},{"location":"data-visualization/data-visualization/#the-process-succinctly","text":"List the key questions of your audience Inventory data sources Sketch potential visuals Share your sketches with members of your audience Prepare and explore the data Test your sketches Develop your visualizations through iteration Strip away distraction Deliver","title":"The process, succinctly"},{"location":"data-visualization/data-visualization/#types-of-visualizations","text":"There's no official taxonomy of data visualizations, but here's a quick overview of a few important visualization types in my opinion: Chart : your standard static data visualization. The static chart's primary habitat is in reports and other static media. See also graphs. Infographic : a thematic collection of multiple static visualizations, often presented as a poster or banner. Dashboard : a thematic collection of one or more dynamic visualizations, updated at some regular interval. Data Story : a presentation of data that marries narrative with data visualization. See some great examples under Inspiration. Any of the above can support interactivity, animations, and/or maps. A static map would be considered a chart (just ask any sailor). By static I mean the underlying data do not change; data change with dynamic visualizations - which provides a unique challenge for visualizations.","title":"Types of visualizations"},{"location":"data-visualization/data-visualization/#visualization-tools","text":"There are dozens (hundreds?) of visualization tools available to you. If you don't already have a preferred solution, here are a few suggestions. This list focuses on tools with a free tier. Of course, there's always Excel or Google Sheets - nothing wrong with these options - but I'm assuming you've already been introduced. Tableau : these folks are visualization nerds and incorporate by default the latest in visualization science in your visuals - which makes you look smart. Tableau Public is free but, you guessed it, makes your visualization public (so don't choose this solution if you're working with sensitive data, or get a paid account). Power BI : Power BI is a free desktop tool with an online service to allow you to publish your visuals and create dashboards. If your team is already using MS Office, it\u2019s a good choice for its easy integration with the Microsoft ecosystem and ability to share visuals within your organization. Google Charts & Data Studio : this is Google's answer to Microsoft's Power BI. Slick and intuitive, you can very quickly learn to create visuals and dashboards in an afternoon that look like they took a week. Plotly & Dash : Plotly's open-source Chart Studio allows you to upload data and quickly create interactive web-ready charts from their standard library. However, this solution really sings if you're able to use the Dash API with Python. Use this solution to develop and deploy analytic web apps, like interactive dashboards. Infogram : an option for creating infographics, Infogram allows up to 10 projects at the free tier. See also Visme , Adobe Illustrator , and Inkscape . Matplotlib & Seaborn : for Python developers, these are your go-to libraries for data visualization. ArcGIS StoryMaps : StoryMaps include straightforward interactive maps and the eponymous Story Map , a platform for creating data stories that typically (but don't have to!) include maps and other spatial components.","title":"Visualization tools"},{"location":"data-visualization/data-visualization/#more-tips-on-visualization","text":"If you're wondering which visualization to use for a given task, start with this Visualization Vocabulary .","title":"More tips on visualization"},{"location":"data-visualization/data-visualization/#techniques","text":"Try out some of these techniques to improve your visualizations. Highlighting Transition Guidance (layering the story, point by point) Ordering Messaging Clarity Reduce but don't avoid complexity Interaction","title":"Techniques"},{"location":"data-visualization/data-visualization/#how-to-tell-a-story","text":"These are approaches to telling your data story. Change Over Time Drill Down Zoom Out Contrast Intersections Factors Outliers Communicating Uncertainty","title":"How to tell a story"},{"location":"data-visualization/data-visualization/#inspiration","text":"How did I create an entire post about data visualization without a single data visualization? Check these out:","title":"Inspiration"},{"location":"data-visualization/data-visualization/#galleries","text":"Tableau Public Gallery Makeover Monday Gallery Dash App Gallery","title":"Galleries"},{"location":"data-visualization/data-visualization/#philosophies","text":"Data Humanism","title":"Philosophies"},{"location":"data-visualization/data-visualization/#some-great-examples","text":"The Avalanche at Snow Falls (NYT) What's really warming the world (Bloomberg) Bussed Out (The Guardian)","title":"Some great examples"},{"location":"deployment/deployment-overview/","text":"Getting Tools to Users \u00b6 This page is under development Tools are only useful if they are used. Getting tools in the hands of your users, while ensuring a good user experience with tech support and the development of new features, is at least half the battle. A number of options exist for deploying tools. The deployment environment should be decided early on in tool development. Here's a short list of options (and important supporting technologies) for deploying tools. Feel free to get creative and invent your own solutions as well. Data Visualization & Dashboards Plot.ly Bokeh Cufflinks Folium (maps for dashboards) Dash Tableau Ipywidgets, ipyleaflet, \u2026 Microsoft PowerApps PowerBI Jupyter Notebook + Binder Google Colab Web Deployment Flask Django Streamlit Heroku GitHub Pages Linode Digital Ocean Google Apps (Firebase, AppSheet, App Engine) Desktop Deployment Tkinter Cron Tab PyOxidizer Excel + VBA Command Line Tools Databases & Back-ends SQLLite PostgreSQL MySQL Google Sheets AWS S3 Environments Docker Geographic Data & Mapping Pyproj (for projection conversion) Google Earth Engine Custom script tools using ESRI ArcGIS API (arcpy) ESRI ArcGIS Story Map ESRI ArcGIS Apps Comparing the options \u00b6 Dash vs. Bokeh \u00b6 Dash and Bokeh provide similar capabilities for developing dashboards. The first step will be selecting which package to learn. Dash is maintained by Plot.ly, whereas Bokeh is maintained by Anaconda. Based on a little bit of research , it appears that Dash is a better first choice for building Dashboards. It has slightly less functionality but is 100% python and well documented with rapid development of new features. It offers a free and paid version. Dash also offers a deployment server (paid) and uses Flask as a backend, which is another advantage in deployment. Dash recommends Heroku for deployment if not using the Dash server. Ipywidgets \u00b6 Plotly plays well with ipywidgets in Notebooks, which provides a nice place to do data exploration or play with dashboard designs. You can pass chart design elements as interactions to test different marker sizes, opacities, background colors, map projections, etc. without interacting with code. See Jon Mease\u2019s talk at PyData 2018 . Folium \u00b6 Folium is a python wrapper for Leaflet, a popular JavaScript web map package. It creates HTML maps which conveniently run in Notebooks natively (ipyleaflet is an alternative if using widget integration in a map, but Folium has good interactions without the need for widgets). When combined with Dash, it allows for maps integrated into dashboards. Heroku \u00b6 Heroku is a hosting service for python web apps. It is a fairly easy to use service with a free option and paid options for higher traffic. We\u2019ll use Heroku for our web apps for now. Flask also provides a list of web deployment options . OpenShift allows one free app. The Magic Mix \u00b6 Heroku, Postgres, Flask, Dash, Plot.ly, Git The above technologies are integrated in the Heroku platform for web app deployment. Mastering these technologies will provide an important platform for quickly building web apps in the future.","title":"Deployment Overview"},{"location":"deployment/deployment-overview/#getting-tools-to-users","text":"This page is under development Tools are only useful if they are used. Getting tools in the hands of your users, while ensuring a good user experience with tech support and the development of new features, is at least half the battle. A number of options exist for deploying tools. The deployment environment should be decided early on in tool development. Here's a short list of options (and important supporting technologies) for deploying tools. Feel free to get creative and invent your own solutions as well. Data Visualization & Dashboards Plot.ly Bokeh Cufflinks Folium (maps for dashboards) Dash Tableau Ipywidgets, ipyleaflet, \u2026 Microsoft PowerApps PowerBI Jupyter Notebook + Binder Google Colab Web Deployment Flask Django Streamlit Heroku GitHub Pages Linode Digital Ocean Google Apps (Firebase, AppSheet, App Engine) Desktop Deployment Tkinter Cron Tab PyOxidizer Excel + VBA Command Line Tools Databases & Back-ends SQLLite PostgreSQL MySQL Google Sheets AWS S3 Environments Docker Geographic Data & Mapping Pyproj (for projection conversion) Google Earth Engine Custom script tools using ESRI ArcGIS API (arcpy) ESRI ArcGIS Story Map ESRI ArcGIS Apps","title":"Getting Tools to Users"},{"location":"deployment/deployment-overview/#comparing-the-options","text":"","title":"Comparing the options"},{"location":"deployment/deployment-overview/#dash-vs-bokeh","text":"Dash and Bokeh provide similar capabilities for developing dashboards. The first step will be selecting which package to learn. Dash is maintained by Plot.ly, whereas Bokeh is maintained by Anaconda. Based on a little bit of research , it appears that Dash is a better first choice for building Dashboards. It has slightly less functionality but is 100% python and well documented with rapid development of new features. It offers a free and paid version. Dash also offers a deployment server (paid) and uses Flask as a backend, which is another advantage in deployment. Dash recommends Heroku for deployment if not using the Dash server.","title":"Dash vs. Bokeh"},{"location":"deployment/deployment-overview/#ipywidgets","text":"Plotly plays well with ipywidgets in Notebooks, which provides a nice place to do data exploration or play with dashboard designs. You can pass chart design elements as interactions to test different marker sizes, opacities, background colors, map projections, etc. without interacting with code. See Jon Mease\u2019s talk at PyData 2018 .","title":"Ipywidgets"},{"location":"deployment/deployment-overview/#folium","text":"Folium is a python wrapper for Leaflet, a popular JavaScript web map package. It creates HTML maps which conveniently run in Notebooks natively (ipyleaflet is an alternative if using widget integration in a map, but Folium has good interactions without the need for widgets). When combined with Dash, it allows for maps integrated into dashboards.","title":"Folium"},{"location":"deployment/deployment-overview/#heroku","text":"Heroku is a hosting service for python web apps. It is a fairly easy to use service with a free option and paid options for higher traffic. We\u2019ll use Heroku for our web apps for now. Flask also provides a list of web deployment options . OpenShift allows one free app.","title":"Heroku"},{"location":"deployment/deployment-overview/#the-magic-mix","text":"Heroku, Postgres, Flask, Dash, Plot.ly, Git The above technologies are integrated in the Heroku platform for web app deployment. Mastering these technologies will provide an important platform for quickly building web apps in the future.","title":"The Magic Mix"},{"location":"deployment/google-api/","text":"Google API \u00b6 Google provides an API for interacting with Google Drive and its suite of office products (Sheets, Docs, Slides, etc.). An API is an 'application programming interface'. Simply put, it's the interpreter between your program and Google's products. If you want to write into a Google Doc, you can use the API to do it. Google Sheets is actually a quite powerful backend data store for web-based applications. If you're deploying to Heroku and don't have the need to configure a Postgres database, Google Sheets can get the job done quite well. That said, it's not free of issues, so beware before building anything mission critical. You can create up to 12 projects on Google Drive using the API for free. Setting up access \u00b6 This tutorial describes how to set up a project through Google's API Console and get the credentials you'll need to access it. Basically Go to the Google API Console Create a new project Click Enable API. Search for and enable the Google Drive API. Search for and enable the Google Sheets API. Create credentials for a Web Server to access Application Data. Name the service account and grant it a Project Role of Editor Download the JSON file Copy the JSON file into your project directory under a subdirectory called secrets/ In the spreadsheet on Google Drive, click the Share button and paste the client email from the JSON file into the People field to give it edit rights. Hit Send. Credit: Greg Baugues, twilio.com Interacting with the spreadsheet \u00b6 Now you can access the spreadsheet from your script. You'll need both the gspread and oauth2client package. Note that oauth2client is deprecated, see google-auth or oauthlib , gspread may be superseded by pygsheets . conda install the required packages. Use the code below to get access to the spreadsheet. 1 2 3 4 5 6 7 8 9 10 11 import gspread from oauth2client.service_account import ServiceAccountCredentials scope = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] creds = ServiceAccountCredentials . from_json_keyfile_name ( '<path to JSON credentials>' , scope ) client = gspread . authorize ( creds ) Refer to the 'gspread` docs for help reading and writing to spreadsheets. Basic gspread commands \u00b6 Use this code to read a spreadsheet as a pandas data frame (include code from above to authenticate). Note that a spreadsheet is synonymous with an Excel workbook, and a worksheets is still a worksheet. 1 2 3 4 5 6 import pandas as pd wks = client . open ( '<spreadsheet name>' ) . worksheet ( '<worksheet name>' ) data = hours_wks . get_all_values () headers = data . pop ( 0 ) df = pd . DataFrame ( data , columns = headers ) To save a csv in an existing workbook (note this will overwrite the entire workbook): 1 2 3 out_file = client . open ( '<spreadsheet name>' ) '.id data = ( '<data.csv>' ) client . import_csv ( out_file , data )","title":"Google API"},{"location":"deployment/google-api/#google-api","text":"Google provides an API for interacting with Google Drive and its suite of office products (Sheets, Docs, Slides, etc.). An API is an 'application programming interface'. Simply put, it's the interpreter between your program and Google's products. If you want to write into a Google Doc, you can use the API to do it. Google Sheets is actually a quite powerful backend data store for web-based applications. If you're deploying to Heroku and don't have the need to configure a Postgres database, Google Sheets can get the job done quite well. That said, it's not free of issues, so beware before building anything mission critical. You can create up to 12 projects on Google Drive using the API for free.","title":"Google API"},{"location":"deployment/google-api/#setting-up-access","text":"This tutorial describes how to set up a project through Google's API Console and get the credentials you'll need to access it. Basically Go to the Google API Console Create a new project Click Enable API. Search for and enable the Google Drive API. Search for and enable the Google Sheets API. Create credentials for a Web Server to access Application Data. Name the service account and grant it a Project Role of Editor Download the JSON file Copy the JSON file into your project directory under a subdirectory called secrets/ In the spreadsheet on Google Drive, click the Share button and paste the client email from the JSON file into the People field to give it edit rights. Hit Send. Credit: Greg Baugues, twilio.com","title":"Setting up access"},{"location":"deployment/google-api/#interacting-with-the-spreadsheet","text":"Now you can access the spreadsheet from your script. You'll need both the gspread and oauth2client package. Note that oauth2client is deprecated, see google-auth or oauthlib , gspread may be superseded by pygsheets . conda install the required packages. Use the code below to get access to the spreadsheet. 1 2 3 4 5 6 7 8 9 10 11 import gspread from oauth2client.service_account import ServiceAccountCredentials scope = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] creds = ServiceAccountCredentials . from_json_keyfile_name ( '<path to JSON credentials>' , scope ) client = gspread . authorize ( creds ) Refer to the 'gspread` docs for help reading and writing to spreadsheets.","title":"Interacting with the spreadsheet"},{"location":"deployment/google-api/#basic-gspread-commands","text":"Use this code to read a spreadsheet as a pandas data frame (include code from above to authenticate). Note that a spreadsheet is synonymous with an Excel workbook, and a worksheets is still a worksheet. 1 2 3 4 5 6 import pandas as pd wks = client . open ( '<spreadsheet name>' ) . worksheet ( '<worksheet name>' ) data = hours_wks . get_all_values () headers = data . pop ( 0 ) df = pd . DataFrame ( data , columns = headers ) To save a csv in an existing workbook (note this will overwrite the entire workbook): 1 2 3 out_file = client . open ( '<spreadsheet name>' ) '.id data = ( '<data.csv>' ) client . import_csv ( out_file , data )","title":"Basic gspread commands"},{"location":"deployment/heroku/","text":"Heroku \u00b6 Heroku is an app hosting platform that supports multiple programming languages, including python. To get the most out of it, you'll need to develop a user interface for your app with a package like streamlit , dash , django or flask . Anything that renders your application in HTML will suffice. Heroku loads all of the software (i.e., python, the packages you need, and your scripts) into a container on its servers in what they call a 'dyno'. That way, your users can run the software without having to load it on their own machine. Heroku is similar to Shiny Apps, if you're used to the R ecosystem. It's also worth noting that Heroku does not store data for you. In other words, you won't be able to access the .csv files or .png files you have been passing to your program. You'll need to find a cloud-based hosting service like Google Drive or Amazon S3 for those assets. Tutorial \u00b6 This tutorial will walk you through the process of deploying a streamlit app on Heroku. You'll need to set up a virtual environment , understand the basics of git, and be comfortable working from the command line. You should also review the Google API tutorial to set up Google Sheets as a back-end. Step-by-step \u00b6 Create a new project folder Open an Anaconda Prompt window within the project folder Initialize git ( git init ) Create a .gitignore file ( code .gitignore ) Add (one per line): *.pyc , secrets/ , and any other files or folders you want ignored (e.g., notebooks/ , data/ if you have will be working in jupyter notebooks or with local data while you build your app) Create a virtual environment ( conda create -n <project-name> python ) Activate the environment ( conda activate <project-name> ) Install any packages you'll need using the conda package manager Build your project locally, working within the project's environment Create a requirements.txt file. To do this, you will first need to install pip into your environment ( install pip ). Then use the command pip freeze > requirements.txt . As an alternative, you can simply list the top-level libraries you are using (i.e., the packages you import in your script). If, when deploying to Heroku, you get an error that a package cannot be found, delete that package from requirements.txt . I've found fewer issues with just listing top-level libraries, but you risk version issues cropping up later. Optionally, create a runtime.txt file to tell Heroku which python version to install. Only certain versions are supported by Heroku, so check the documentation first. Use the command python -V > runtime.txt , or type in the python version you'd prefer Heroku install. Create a Procfile, which tells Heroku how to actually run your app. The Procfile is run when the app is initialized in each session. Type code Profile to create a new file and add the command Heroku will use to run your app. It's different for each technology, but for a streamlit app the text is sh setup.sh && streamlit run <my_app.py> . The app will run from the root folder, so make sure if your script is stored in a folder, you include it in the path (e.g, sh setup && streamlit run <scripts/my_app.py> ). Also, make sure that the relative paths in your script are relative to the root folder, NOT the script itself. For a streamlit app, you'll need to create a setup.sh file. Create a new file code setup.sh and paste the below into it (note language is bash): 1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir -p ~/.streamlit/ echo \"\\ [general]\\n\\ email = \\\"your-email@domain.com\\\"\\n\\ \" > ~/.streamlit/credentials.toml echo \"\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT \\n\\ \" > ~/.streamlit/config.toml Create a Procfile for local development, if needed: code Procfile.windows web: streamlit run scripts/<my_app.py> runserver 0.0.0.0:5000 Commit changes to git ( git add . , git commit -m \"message\" ) Create the Heroku app: heroku create <app name> where <app name> will serve as the base of the url and the name of the app on Heroku's dashboard Deploy the code to Heroku: git push heroku master Spin up one dyno: heroku ps:scale web=1 (use heroku ps:scale web=0 ) to shut it down. Use heroku ps to see how many dynos are running. Open the app: heroku open . If the app isn't up yet, or something looks wrong, check the logs with heroku logs --tail . You can close the logs with ctrl+c . If you need to run the app locally, you can use the command heroku local web -f Procfile.windows . If you make changes to the app, commit everything to git and then push it back up to Heroku. Every time you push to Heroku, it rebuilds the dyno from scratch, which can take some time depending on how many packages you need to load, so try to do that sparingly. If you have API tokens or other sensitive information, you'll need to configure Heroku's configuration variables. This is equivalent to setting an environment variable on your local machine. These variables will only be exposed at runtime, and won't be accessible to your users. Simply type heroku config:set <VAR_NAME> = <VAR_VALUE> . For JSON blobs, it's easiest to add them through Heroku's online interface (under the 'Settings' of the app.) You can paste in the prettified JSON as a new variable there. See the Google API page for more info on accessing those variables within your scripts. Your app should now be up and running on Heroku. If you're on a free tier, it may take 10-20 seconds for the app to load each time because the server needs to spin it up. It will be active for 30 minutes, but will sleep again after that time. You can upgrade to a Hobby license for $7 per dyno per month. Update an existing app \u00b6 Updating an existing app is as simple as running git push heroku master from the project's root directory. However, if you have cloned the repository (or you, say, deployed it last time from a different computer), you'll need to establish the remote connection with heroku first. Type heroku login to login (a browser window will open) and then heroku git:remote -a <app-name> to connect to the app. The <app-name> can be found on your heroku page . Notes \u00b6 Note that in recent updates the setup.sh may not be required, instead you can use this command in the Procfile: web: streamlit run --server.enableCORS false my_script.py (but I haven't tested it).","title":"Heroku"},{"location":"deployment/heroku/#heroku","text":"Heroku is an app hosting platform that supports multiple programming languages, including python. To get the most out of it, you'll need to develop a user interface for your app with a package like streamlit , dash , django or flask . Anything that renders your application in HTML will suffice. Heroku loads all of the software (i.e., python, the packages you need, and your scripts) into a container on its servers in what they call a 'dyno'. That way, your users can run the software without having to load it on their own machine. Heroku is similar to Shiny Apps, if you're used to the R ecosystem. It's also worth noting that Heroku does not store data for you. In other words, you won't be able to access the .csv files or .png files you have been passing to your program. You'll need to find a cloud-based hosting service like Google Drive or Amazon S3 for those assets.","title":"Heroku"},{"location":"deployment/heroku/#tutorial","text":"This tutorial will walk you through the process of deploying a streamlit app on Heroku. You'll need to set up a virtual environment , understand the basics of git, and be comfortable working from the command line. You should also review the Google API tutorial to set up Google Sheets as a back-end.","title":"Tutorial"},{"location":"deployment/heroku/#step-by-step","text":"Create a new project folder Open an Anaconda Prompt window within the project folder Initialize git ( git init ) Create a .gitignore file ( code .gitignore ) Add (one per line): *.pyc , secrets/ , and any other files or folders you want ignored (e.g., notebooks/ , data/ if you have will be working in jupyter notebooks or with local data while you build your app) Create a virtual environment ( conda create -n <project-name> python ) Activate the environment ( conda activate <project-name> ) Install any packages you'll need using the conda package manager Build your project locally, working within the project's environment Create a requirements.txt file. To do this, you will first need to install pip into your environment ( install pip ). Then use the command pip freeze > requirements.txt . As an alternative, you can simply list the top-level libraries you are using (i.e., the packages you import in your script). If, when deploying to Heroku, you get an error that a package cannot be found, delete that package from requirements.txt . I've found fewer issues with just listing top-level libraries, but you risk version issues cropping up later. Optionally, create a runtime.txt file to tell Heroku which python version to install. Only certain versions are supported by Heroku, so check the documentation first. Use the command python -V > runtime.txt , or type in the python version you'd prefer Heroku install. Create a Procfile, which tells Heroku how to actually run your app. The Procfile is run when the app is initialized in each session. Type code Profile to create a new file and add the command Heroku will use to run your app. It's different for each technology, but for a streamlit app the text is sh setup.sh && streamlit run <my_app.py> . The app will run from the root folder, so make sure if your script is stored in a folder, you include it in the path (e.g, sh setup && streamlit run <scripts/my_app.py> ). Also, make sure that the relative paths in your script are relative to the root folder, NOT the script itself. For a streamlit app, you'll need to create a setup.sh file. Create a new file code setup.sh and paste the below into it (note language is bash): 1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir -p ~/.streamlit/ echo \"\\ [general]\\n\\ email = \\\"your-email@domain.com\\\"\\n\\ \" > ~/.streamlit/credentials.toml echo \"\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT \\n\\ \" > ~/.streamlit/config.toml Create a Procfile for local development, if needed: code Procfile.windows web: streamlit run scripts/<my_app.py> runserver 0.0.0.0:5000 Commit changes to git ( git add . , git commit -m \"message\" ) Create the Heroku app: heroku create <app name> where <app name> will serve as the base of the url and the name of the app on Heroku's dashboard Deploy the code to Heroku: git push heroku master Spin up one dyno: heroku ps:scale web=1 (use heroku ps:scale web=0 ) to shut it down. Use heroku ps to see how many dynos are running. Open the app: heroku open . If the app isn't up yet, or something looks wrong, check the logs with heroku logs --tail . You can close the logs with ctrl+c . If you need to run the app locally, you can use the command heroku local web -f Procfile.windows . If you make changes to the app, commit everything to git and then push it back up to Heroku. Every time you push to Heroku, it rebuilds the dyno from scratch, which can take some time depending on how many packages you need to load, so try to do that sparingly. If you have API tokens or other sensitive information, you'll need to configure Heroku's configuration variables. This is equivalent to setting an environment variable on your local machine. These variables will only be exposed at runtime, and won't be accessible to your users. Simply type heroku config:set <VAR_NAME> = <VAR_VALUE> . For JSON blobs, it's easiest to add them through Heroku's online interface (under the 'Settings' of the app.) You can paste in the prettified JSON as a new variable there. See the Google API page for more info on accessing those variables within your scripts. Your app should now be up and running on Heroku. If you're on a free tier, it may take 10-20 seconds for the app to load each time because the server needs to spin it up. It will be active for 30 minutes, but will sleep again after that time. You can upgrade to a Hobby license for $7 per dyno per month.","title":"Step-by-step"},{"location":"deployment/heroku/#update-an-existing-app","text":"Updating an existing app is as simple as running git push heroku master from the project's root directory. However, if you have cloned the repository (or you, say, deployed it last time from a different computer), you'll need to establish the remote connection with heroku first. Type heroku login to login (a browser window will open) and then heroku git:remote -a <app-name> to connect to the app. The <app-name> can be found on your heroku page .","title":"Update an existing app"},{"location":"deployment/heroku/#notes","text":"Note that in recent updates the setup.sh may not be required, instead you can use this command in the Procfile: web: streamlit run --server.enableCORS false my_script.py (but I haven't tested it).","title":"Notes"},{"location":"development/command-line-tools/","text":"Command Line Tools \u00b6 Once you get comfortable with the command line, why not create your own utilities for quick tasks? The easy way \u00b6 The easiest way to use the command line to run python programs is simply to write a script and run it from the CLI with python: 1 python <my_app.py> If you don't want to provide the fully-qualified path to your script, open the command prompt from within the project's directory, or cd into it. Nothing more to it than that. With arguments \u00b6 If you need to pass arguments to your application (e.g., a file to save output), you'll need to be able to handle those arguments. The command line will pass anything you type after python <my_app.py> as strings to the program, but without a way of parsing and validating those strings, those arguments just get ignored. There are a number of packages that you can explore for developing command line tools: fire argpars click fire may be the easiest to use right out of the box, while click has more control for validation and providing help at the command line. Check out these tutorials on command line interface apps to get started: How to write python command line interfaces like a pro Python command line tools","title":"Command Line Tools"},{"location":"development/command-line-tools/#command-line-tools","text":"Once you get comfortable with the command line, why not create your own utilities for quick tasks?","title":"Command Line Tools"},{"location":"development/command-line-tools/#the-easy-way","text":"The easiest way to use the command line to run python programs is simply to write a script and run it from the CLI with python: 1 python <my_app.py> If you don't want to provide the fully-qualified path to your script, open the command prompt from within the project's directory, or cd into it. Nothing more to it than that.","title":"The easy way"},{"location":"development/command-line-tools/#with-arguments","text":"If you need to pass arguments to your application (e.g., a file to save output), you'll need to be able to handle those arguments. The command line will pass anything you type after python <my_app.py> as strings to the program, but without a way of parsing and validating those strings, those arguments just get ignored. There are a number of packages that you can explore for developing command line tools: fire argpars click fire may be the easiest to use right out of the box, while click has more control for validation and providing help at the command line. Check out these tutorials on command line interface apps to get started: How to write python command line interfaces like a pro Python command line tools","title":"With arguments"},{"location":"development/environment-variables/","text":"Environment Variables \u00b6 Environment variables allow you to use sensitive or secret information (credentials, passwords, tokens, etc.) in a script without hard-coding those values into your script. Typically, you won't want to store something like a password as a string directly in your code. If you commit that code to GitHub it becomes a permanent part of your code's history and thus accessible to anyone who accesses your project on GitHub. There are web crawlers that sift through GitHub repositories looking for information like this, so be careful. Setting Environment Variables \u00b6 Environment variables are set differently on different operating systems. On Windows, just type 'environment variables' into the start menu and open the System Properties dialog box ('Advanced' tab). Click on 'Environment Variables' to edit. Adding as a user variable, rather than a system variable, is generally sufficient. Conventionally, both the name and the value are uppercase. Using Environment Variables \u00b6 Python reads the environment variables when os is imported. Environment variables are passed as a dictionary. Since this is simply a dictionary, you can use os.environ.get('<KEY>') to access the value with the name of the variable (where '<KEY>' is the name you specified for the environment variable). For example: 1 2 3 import os my_password = os . environ . get ( 'PASS_KEY' ) A caveat \u00b6 Environment variables are commonly used, but not always the best option for storing secrets. They can be subject to mangling by the operating system, and some access tokens don't conform to their limitations (e.g., my google access token had a '=' character, which is not allowed). Especially if your secrets are stored as a JSON blob, you might consider saving them in a file and reading them directly from the file. I will add a secrets/ folder to my projects and save credentials there. MAKE SURE TO INCLUDE secrets/ IN YOUR .gitignore FILE! Use the google-auth or oauthlib packages to access and use credentials stored as a JSON file. For the A/V Crowd \u00b6 This video captures the essentials of Environment Variables in 5 minutes. Worth a watch for anything I missed.","title":"Environment Variables"},{"location":"development/environment-variables/#environment-variables","text":"Environment variables allow you to use sensitive or secret information (credentials, passwords, tokens, etc.) in a script without hard-coding those values into your script. Typically, you won't want to store something like a password as a string directly in your code. If you commit that code to GitHub it becomes a permanent part of your code's history and thus accessible to anyone who accesses your project on GitHub. There are web crawlers that sift through GitHub repositories looking for information like this, so be careful.","title":"Environment Variables"},{"location":"development/environment-variables/#setting-environment-variables","text":"Environment variables are set differently on different operating systems. On Windows, just type 'environment variables' into the start menu and open the System Properties dialog box ('Advanced' tab). Click on 'Environment Variables' to edit. Adding as a user variable, rather than a system variable, is generally sufficient. Conventionally, both the name and the value are uppercase.","title":"Setting Environment Variables"},{"location":"development/environment-variables/#using-environment-variables","text":"Python reads the environment variables when os is imported. Environment variables are passed as a dictionary. Since this is simply a dictionary, you can use os.environ.get('<KEY>') to access the value with the name of the variable (where '<KEY>' is the name you specified for the environment variable). For example: 1 2 3 import os my_password = os . environ . get ( 'PASS_KEY' )","title":"Using Environment Variables"},{"location":"development/environment-variables/#a-caveat","text":"Environment variables are commonly used, but not always the best option for storing secrets. They can be subject to mangling by the operating system, and some access tokens don't conform to their limitations (e.g., my google access token had a '=' character, which is not allowed). Especially if your secrets are stored as a JSON blob, you might consider saving them in a file and reading them directly from the file. I will add a secrets/ folder to my projects and save credentials there. MAKE SURE TO INCLUDE secrets/ IN YOUR .gitignore FILE! Use the google-auth or oauthlib packages to access and use credentials stored as a JSON file.","title":"A caveat"},{"location":"development/environment-variables/#for-the-av-crowd","text":"This video captures the essentials of Environment Variables in 5 minutes. Worth a watch for anything I missed.","title":"For the A/V Crowd"},{"location":"development/scheduling-tasks/","text":"Scheduling Tasks \u00b6 If you have a task that needs to be completed regularly, and it's something you can automate with a python script, you can schedule the task to occur at set times to save you the hassle. It also helps guarantee that the task gets done when it should. Python has packages available for scheduling tasks, such as python-chrontab . However, on a Windows machine it's possibly easiest to use the Windows Task Scheduler . Alternatively, you can use schedule task to run when you start up your computer . Windows Task Scheduler \u00b6 1. Create a .bat file \u00b6 To use the Windows Task Scheduler to , you'll first need to create a .bat file. Open a new file on VS Code or notepad and type: 1 2 \"<Path to python executable>\" \"<Path to script>\" pause The path to the python executable for me is: \"C:\\Users\\Erik\\Anaconda3\\python.exe\" The path to your script is the fully qualified path to your script, don't forget to include the script name and .py at the end. Save the file and change the extension to .bat (e.g., run_me.bat ). You can run your script by double clicking on the file. Running a .bat file is a lot like typing python <my_script.py> into the CLI. 2. Schedule the task \u00b6 To schedule the task, type 'schedule task' in the Windows start menu and open the Task Scheduler. Select 'Create Basic Task...' and work through the interface. You can open the settings dialog box for more options (see below) before saving. Additional settings available \u00b6 General : Provide any name you want Triggers : Add a New Trigger and select the type of trigger you'd prefer. The task can be run at a specific time and day, but also upon logging on to the computer or other options. Actions : Add a New Action and select 'Start a Program'. Conditions : You can specify additional conditions here. Settings : Additional settings here. I recommend selecting 'Run task as soon as possible after a scheduled start is missed', in case the computer isn't on when the task should have been run. The Startup Folder \u00b6 Navigate to your .bat file, right-click and select copy. Use the Windows key + R keyboard shortcut to open the Run command. Type in shell:startup and click OK. Paste a shortcut to the .bat file in this folder and it will run on every startup (you'll need to restart for this to take effect). Note that if you shut down your computer more than once a day, this will run the script multiple times that day. If you just want a daily task to run, use the Task Scheduler. Read more about using batch files in Windows . Getting an error? \u00b6 In some cases (especially if you have multiple python versions installed on your computer), you may need to activate the base Anaconda environment before attempting to run your script in the .bat file. To do this, simply add a line at the beginning of your .bat file (edit the path as necessary to for your machine) so it looks like this: 1 2 call \"C:\\Users\\Erik\\Anaconda3\\Scripts\\activate.bat\" \"C:\\Users\\Erik\\Anaconda3\\python.exe\" \"<Path to script>\" The call function is used to execute another .bat file from within a .bat file, and in this case will activate the base anaconda environment which will allow your machine to find the packages you need to run your script. If you don't do this, you may get a DLL load error when attempting to run the .bat file.","title":"Scheduling Tasks"},{"location":"development/scheduling-tasks/#scheduling-tasks","text":"If you have a task that needs to be completed regularly, and it's something you can automate with a python script, you can schedule the task to occur at set times to save you the hassle. It also helps guarantee that the task gets done when it should. Python has packages available for scheduling tasks, such as python-chrontab . However, on a Windows machine it's possibly easiest to use the Windows Task Scheduler . Alternatively, you can use schedule task to run when you start up your computer .","title":"Scheduling Tasks"},{"location":"development/scheduling-tasks/#windows-task-scheduler","text":"","title":"Windows Task Scheduler"},{"location":"development/scheduling-tasks/#1-create-a-bat-file","text":"To use the Windows Task Scheduler to , you'll first need to create a .bat file. Open a new file on VS Code or notepad and type: 1 2 \"<Path to python executable>\" \"<Path to script>\" pause The path to the python executable for me is: \"C:\\Users\\Erik\\Anaconda3\\python.exe\" The path to your script is the fully qualified path to your script, don't forget to include the script name and .py at the end. Save the file and change the extension to .bat (e.g., run_me.bat ). You can run your script by double clicking on the file. Running a .bat file is a lot like typing python <my_script.py> into the CLI.","title":"1. Create a .bat file"},{"location":"development/scheduling-tasks/#2-schedule-the-task","text":"To schedule the task, type 'schedule task' in the Windows start menu and open the Task Scheduler. Select 'Create Basic Task...' and work through the interface. You can open the settings dialog box for more options (see below) before saving.","title":"2. Schedule the task"},{"location":"development/scheduling-tasks/#additional-settings-available","text":"General : Provide any name you want Triggers : Add a New Trigger and select the type of trigger you'd prefer. The task can be run at a specific time and day, but also upon logging on to the computer or other options. Actions : Add a New Action and select 'Start a Program'. Conditions : You can specify additional conditions here. Settings : Additional settings here. I recommend selecting 'Run task as soon as possible after a scheduled start is missed', in case the computer isn't on when the task should have been run.","title":"Additional settings available"},{"location":"development/scheduling-tasks/#the-startup-folder","text":"Navigate to your .bat file, right-click and select copy. Use the Windows key + R keyboard shortcut to open the Run command. Type in shell:startup and click OK. Paste a shortcut to the .bat file in this folder and it will run on every startup (you'll need to restart for this to take effect). Note that if you shut down your computer more than once a day, this will run the script multiple times that day. If you just want a daily task to run, use the Task Scheduler. Read more about using batch files in Windows .","title":"The Startup Folder"},{"location":"development/scheduling-tasks/#getting-an-error","text":"In some cases (especially if you have multiple python versions installed on your computer), you may need to activate the base Anaconda environment before attempting to run your script in the .bat file. To do this, simply add a line at the beginning of your .bat file (edit the path as necessary to for your machine) so it looks like this: 1 2 call \"C:\\Users\\Erik\\Anaconda3\\Scripts\\activate.bat\" \"C:\\Users\\Erik\\Anaconda3\\python.exe\" \"<Path to script>\" The call function is used to execute another .bat file from within a .bat file, and in this case will activate the base anaconda environment which will allow your machine to find the packages you need to run your script. If you don't do this, you may get a DLL load error when attempting to run the .bat file.","title":"Getting an error?"},{"location":"development/virtual-environments/","text":"Virtual Environments \u00b6 Virtual environments allow you to maintain a controlled development environment and are useful when deploying your application. They're actually quite easy to set up and use. See the basic workflow for a quick reference. Why use a virtual environment? \u00b6 Imagine you have built an app using Python v2.7. You decide to upgrade your Anaconda distribution, which upgrades all of your packages and upgrades Python to v3.6. All of the sudden, all of your print statements return an error because they are missing parentheses! To avoid this, you can create virtual environments for each of your projects. A virtual environment simply saves all of the packages you're using, as well as the Python version (in some cases), in a unique folder rather than with the base Anaconda packages. This way, you can always be sure that the packages you're using aren't upgraded (or downgraded) in the future when you upgrade or install new packages to your base environment. The base environment \u00b6 If you're not working in a virtual environment you have created, you're working in the base Anaconda environment (assuming you're working from an Anaconda distribution). You'll notice that every time you open an Anaconda Prompt window, it automatically activates the base environment with the command conda activate base . When you conda install a package from the base environment, it is stored in this base environment. Creating virtual environments \u00b6 To create a virtual environment, simply use the command conda create -n <name-of-environment> python . If you want a specific python version, specify it as python=3.6.3 for example. This environment is stored in Anaconda's environment directory ( C:\\Users\\User\\Anaconda3\\envs ), rather than the project's folder. Be sure to use the Anaconda Prompt window when creating virtual environments. If you're not using Anaconda, you can use the venv or virtualenv packages (however they do not work with the Anaconda distribution.) venv is the preferred package and should work fine, unless you need to specify a python version, then use virtualenv . Working in virtual environments \u00b6 To activate an environment, simply type conda activate <name-of-environment> . To deactivate, type conda deactivate . If you're using VS Code, you should tell it that you are working in a virtual environment so it can properly lint (highlight errors) based on the packages in your environment. Here's how: Ctrl+Shift+P to open the command palette Search python: select interpreter Select the environment from the dropdown list You will likely be prompted to install pylint into this environment, go ahead and do so using conda. Virtual environment workflow \u00b6 Here's a quick workflow reference for setting up and working with virtual environments: Open an Anaconda Prompt window Create the environment: conda create <name-of-environment> Activate the environment: conda activate <name-of-environment> Install necessary packages: conda install -c conda-forge <package-name> (Check Anaconda's package repo for the correct channel to install from) Build your project If you need to deactivate the environment, use conda deactivate or conda activate base to go back to the base environment (both have the same result). For packages not on Anaconda's package repo, you may need to install pip before using a pip install command. Sharing virtual environments \u00b6 Another great thing about virtual environments is that they are easy to share. And, if you're planning to deploy an app on a platform like Heroku, you must specify the environment (by specifying the necessary packages). Virtual environments are shared with a requirements.txt or environment.yml file. Others can use this to recreate the environment on their own machines. To create this file, make sure the environment is activated and type conda env export > environment.yml (for Anaconda) or pip freeze > requirements.txt (if using pip). This file will be saved in the current working directory. It will include not just the top level libraries you are using (e.g., the libraries you import at the top of your script), but also the dependencies for the current version of those libraries. My 2-cents \u00b6 I recommend creating a new environment for any long-lived project using conda create -n <name-of-environment> python and saving the name of the environment as the same as the root folder of the project. Install pip within it only if needed. The base conda environment ( conda activate base ) is useful for quick projects or ongoing-development type projects that you don't mind breaking from time to time, but plan to always keep up to date. If you code a project without a virtual environment and don't come back to it for a year, there's a good chance you'll find the code doesn't work anymore, or worse yet behaves unexpectedly, and you'll need to go through and update it--which might be more trouble than it's worth. So use virtual environments to save you the hassle. At the very least, keep a requirements.txt or environment.yml file of the current state of the base environment for later reference.","title":"Virtual Environments"},{"location":"development/virtual-environments/#virtual-environments","text":"Virtual environments allow you to maintain a controlled development environment and are useful when deploying your application. They're actually quite easy to set up and use. See the basic workflow for a quick reference.","title":"Virtual Environments"},{"location":"development/virtual-environments/#why-use-a-virtual-environment","text":"Imagine you have built an app using Python v2.7. You decide to upgrade your Anaconda distribution, which upgrades all of your packages and upgrades Python to v3.6. All of the sudden, all of your print statements return an error because they are missing parentheses! To avoid this, you can create virtual environments for each of your projects. A virtual environment simply saves all of the packages you're using, as well as the Python version (in some cases), in a unique folder rather than with the base Anaconda packages. This way, you can always be sure that the packages you're using aren't upgraded (or downgraded) in the future when you upgrade or install new packages to your base environment.","title":"Why use a virtual environment?"},{"location":"development/virtual-environments/#the-base-environment","text":"If you're not working in a virtual environment you have created, you're working in the base Anaconda environment (assuming you're working from an Anaconda distribution). You'll notice that every time you open an Anaconda Prompt window, it automatically activates the base environment with the command conda activate base . When you conda install a package from the base environment, it is stored in this base environment.","title":"The base environment"},{"location":"development/virtual-environments/#creating-virtual-environments","text":"To create a virtual environment, simply use the command conda create -n <name-of-environment> python . If you want a specific python version, specify it as python=3.6.3 for example. This environment is stored in Anaconda's environment directory ( C:\\Users\\User\\Anaconda3\\envs ), rather than the project's folder. Be sure to use the Anaconda Prompt window when creating virtual environments. If you're not using Anaconda, you can use the venv or virtualenv packages (however they do not work with the Anaconda distribution.) venv is the preferred package and should work fine, unless you need to specify a python version, then use virtualenv .","title":"Creating virtual environments"},{"location":"development/virtual-environments/#working-in-virtual-environments","text":"To activate an environment, simply type conda activate <name-of-environment> . To deactivate, type conda deactivate . If you're using VS Code, you should tell it that you are working in a virtual environment so it can properly lint (highlight errors) based on the packages in your environment. Here's how: Ctrl+Shift+P to open the command palette Search python: select interpreter Select the environment from the dropdown list You will likely be prompted to install pylint into this environment, go ahead and do so using conda.","title":"Working in virtual environments"},{"location":"development/virtual-environments/#virtual-environment-workflow","text":"Here's a quick workflow reference for setting up and working with virtual environments: Open an Anaconda Prompt window Create the environment: conda create <name-of-environment> Activate the environment: conda activate <name-of-environment> Install necessary packages: conda install -c conda-forge <package-name> (Check Anaconda's package repo for the correct channel to install from) Build your project If you need to deactivate the environment, use conda deactivate or conda activate base to go back to the base environment (both have the same result). For packages not on Anaconda's package repo, you may need to install pip before using a pip install command.","title":"Virtual environment workflow"},{"location":"development/virtual-environments/#sharing-virtual-environments","text":"Another great thing about virtual environments is that they are easy to share. And, if you're planning to deploy an app on a platform like Heroku, you must specify the environment (by specifying the necessary packages). Virtual environments are shared with a requirements.txt or environment.yml file. Others can use this to recreate the environment on their own machines. To create this file, make sure the environment is activated and type conda env export > environment.yml (for Anaconda) or pip freeze > requirements.txt (if using pip). This file will be saved in the current working directory. It will include not just the top level libraries you are using (e.g., the libraries you import at the top of your script), but also the dependencies for the current version of those libraries.","title":"Sharing virtual environments"},{"location":"development/virtual-environments/#my-2-cents","text":"I recommend creating a new environment for any long-lived project using conda create -n <name-of-environment> python and saving the name of the environment as the same as the root folder of the project. Install pip within it only if needed. The base conda environment ( conda activate base ) is useful for quick projects or ongoing-development type projects that you don't mind breaking from time to time, but plan to always keep up to date. If you code a project without a virtual environment and don't come back to it for a year, there's a good chance you'll find the code doesn't work anymore, or worse yet behaves unexpectedly, and you'll need to go through and update it--which might be more trouble than it's worth. So use virtual environments to save you the hassle. At the very least, keep a requirements.txt or environment.yml file of the current state of the base environment for later reference.","title":"My 2-cents"},{"location":"git/initializing-git/","text":"This section describes how to begin a new project and track it on Github. We have two options: Create the project locally and push it up to Github* Create the project on Github first and clone it down to your computer *If you already have a project started, you've already started locally. Starting Locally \u00b6 If you have already started a project or prefer to work from your local machine first, follow these steps: 1. Create the project directory \u00b6 In Windows Explorer, navigate to the folder in which you will create your project directory (or the root of an existing project directory). Right-click and select 'GitBash Here' to open an instance of Bash. Skip the following paragraph if you already have a project started . Use the command mkdir <project-name> (replace <project-name> with the name of the directory for the project) to create a folder. Next, use the command cd <project-name> to move into that folder. 2. Initialize git \u00b6 Use the command git init to initialize git. A new repository named .git will be created within the directory which stores all of the git version control. You don't need to have GitBash open during your editing sessions, this folder will take care of all of your version control. If for some reason you want to stop tracking with git and lose all of your previous versions, just delete this folder. 3. Create a .gitignore and README.md file \u00b6 Use the command touch .gitignore to create a new file named .gitignore. This file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. Use the command touch README.md to create a README file. This file will automatically be displayed on the repository page on Github. Using the Markdown language, populate this README file with important info as necessary. 4. Commit your initial file(s) \u00b6 First, we'll need to add all files to be tracked by git. Use the command: 1 git add -A Next we'll use a commit to get the first batch of files/folders into our git repository. Use the following command: 1 git commit -m \"Initial Commit\" You will learn more about commits and other features of git in the Using Git section. 5. Create a Github repository \u00b6 Navigate to your Github.com page and login. Click the green Create New Repository button. Name it with the same name you used in step 1 for the project directory. Provide a description. The repository will be public. DO NOT create a README or .gitignore file. After you click 'Ok', Github will provide instructions for importing your project files into the repository. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. You'll use two commands in GitBash to accomplish this: 1 2 git remote add origin <url> git push -u origin master --tags The first line establishes a remote connection to the repository. Replace the url with the url provided by github to your project repository. Note that you can change the name 'origin' to anything you'd like, but origin is used by convention. You will be prompted to sign into your Github account. The second line 'pushes' your files/folders up to Github. The -u flag tells Github to track these files/folders along with master, while the --tags flag will move any messages from previous commits. If this fails, it may be because you already have a remote established named 'origin'. Try git remove rm origin to remove any existing connections, or change the name origin to something unique. To confirm your remote connection was established, use the command git remote -v . 6. Create your project structure and begin coding! \u00b6 At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work. Starting Remote \u00b6 In this option, we'll start by creating a repository on Github and then clone that down to our computer. You can copy files/folders into this newly created repository if you'd like. 1. Create a Gihub repository \u00b6 Navigate to your Github.com page and login. Click the green Create New Repository button. Provide a description. The repository will be public. Create a README and .gitignore file. Optionally add a license. 2. Clone the remote repository \u00b6 Within the project repository on Github, click the 'Clone or Download' button. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. In Windows Explorer, navigate to the folder in which you will create your project directory. Right-click and select 'GitBash Here' to open an instance of Bash. Clone the repository into this folder with the command: 1 git clone <url> where you replace <url> with the copied url (use Shift+Insert to paste into Bash). You may provide a name for the project following the url if you'd like the folder on your local drive to have a different name than the repository on Github. Run ls to confirm that the repository was created. Type the command cd <repository-name> to move into the repository folder (replace <repository name> with the correct name). If you need to copy files/folders into the repository, use the command cp -R <~/path/> . . Replace <~/path/> with the correct path of the source folder/file. The -R flag signifies recursive and copies everything from within the folders as well. The . simply means the current directory, so your copying everything from the path provided to the current directory. 3. Populate the .gitignore file \u00b6 The .gitignore file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. 4. Create your project structure and begin coding! \u00b6 At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work. Additional Resources \u00b6 Corey Shafer's excellent series on Git","title":"Create Project"},{"location":"git/initializing-git/#starting-locally","text":"If you have already started a project or prefer to work from your local machine first, follow these steps:","title":"Starting Locally"},{"location":"git/initializing-git/#1-create-the-project-directory","text":"In Windows Explorer, navigate to the folder in which you will create your project directory (or the root of an existing project directory). Right-click and select 'GitBash Here' to open an instance of Bash. Skip the following paragraph if you already have a project started . Use the command mkdir <project-name> (replace <project-name> with the name of the directory for the project) to create a folder. Next, use the command cd <project-name> to move into that folder.","title":"1. Create the project directory"},{"location":"git/initializing-git/#2-initialize-git","text":"Use the command git init to initialize git. A new repository named .git will be created within the directory which stores all of the git version control. You don't need to have GitBash open during your editing sessions, this folder will take care of all of your version control. If for some reason you want to stop tracking with git and lose all of your previous versions, just delete this folder.","title":"2. Initialize git"},{"location":"git/initializing-git/#3-create-a-gitignore-and-readmemd-file","text":"Use the command touch .gitignore to create a new file named .gitignore. This file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. Use the command touch README.md to create a README file. This file will automatically be displayed on the repository page on Github. Using the Markdown language, populate this README file with important info as necessary.","title":"3. Create a .gitignore and README.md file"},{"location":"git/initializing-git/#4-commit-your-initial-files","text":"First, we'll need to add all files to be tracked by git. Use the command: 1 git add -A Next we'll use a commit to get the first batch of files/folders into our git repository. Use the following command: 1 git commit -m \"Initial Commit\" You will learn more about commits and other features of git in the Using Git section.","title":"4. Commit your initial file(s)"},{"location":"git/initializing-git/#5-create-a-github-repository","text":"Navigate to your Github.com page and login. Click the green Create New Repository button. Name it with the same name you used in step 1 for the project directory. Provide a description. The repository will be public. DO NOT create a README or .gitignore file. After you click 'Ok', Github will provide instructions for importing your project files into the repository. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. You'll use two commands in GitBash to accomplish this: 1 2 git remote add origin <url> git push -u origin master --tags The first line establishes a remote connection to the repository. Replace the url with the url provided by github to your project repository. Note that you can change the name 'origin' to anything you'd like, but origin is used by convention. You will be prompted to sign into your Github account. The second line 'pushes' your files/folders up to Github. The -u flag tells Github to track these files/folders along with master, while the --tags flag will move any messages from previous commits. If this fails, it may be because you already have a remote established named 'origin'. Try git remove rm origin to remove any existing connections, or change the name origin to something unique. To confirm your remote connection was established, use the command git remote -v .","title":"5. Create a Github repository"},{"location":"git/initializing-git/#6-create-your-project-structure-and-begin-coding","text":"At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work.","title":"6. Create your project structure and begin coding!"},{"location":"git/initializing-git/#starting-remote","text":"In this option, we'll start by creating a repository on Github and then clone that down to our computer. You can copy files/folders into this newly created repository if you'd like.","title":"Starting Remote"},{"location":"git/initializing-git/#1-create-a-gihub-repository","text":"Navigate to your Github.com page and login. Click the green Create New Repository button. Provide a description. The repository will be public. Create a README and .gitignore file. Optionally add a license.","title":"1. Create a Gihub repository"},{"location":"git/initializing-git/#2-clone-the-remote-repository","text":"Within the project repository on Github, click the 'Clone or Download' button. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. In Windows Explorer, navigate to the folder in which you will create your project directory. Right-click and select 'GitBash Here' to open an instance of Bash. Clone the repository into this folder with the command: 1 git clone <url> where you replace <url> with the copied url (use Shift+Insert to paste into Bash). You may provide a name for the project following the url if you'd like the folder on your local drive to have a different name than the repository on Github. Run ls to confirm that the repository was created. Type the command cd <repository-name> to move into the repository folder (replace <repository name> with the correct name). If you need to copy files/folders into the repository, use the command cp -R <~/path/> . . Replace <~/path/> with the correct path of the source folder/file. The -R flag signifies recursive and copies everything from within the folders as well. The . simply means the current directory, so your copying everything from the path provided to the current directory.","title":"2. Clone the remote repository"},{"location":"git/initializing-git/#3-populate-the-gitignore-file","text":"The .gitignore file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore.","title":"3. Populate the .gitignore file"},{"location":"git/initializing-git/#4-create-your-project-structure-and-begin-coding","text":"At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work.","title":"4. Create your project structure and begin coding!"},{"location":"git/initializing-git/#additional-resources","text":"Corey Shafer's excellent series on Git","title":"Additional Resources"},{"location":"git/notes/","text":"HELP ME PLEASE! Setting Up Git & GitHub \u00b6 Download Git \u00b6 git-scm.com Download & run installer git --version on cmd to make sure its successfully installed Configure Git \u00b6 git config --global user.name <name> git config --global user.email <email> git config --list to see all settings Git Log git log <- all commits git show <- last commit git ls-files <- lists all files that git is tracking git log --oneline --graph --decorate --all <-more detailed history of commits To create an alias for the git history command \u00b6 git config --global alias.hist \"log --oneline --graph --decorate --all\" now use git hist to see the same command, note it still accepts additional arguments (for example, provide --filename to see history for one file) History will be served line by line, type q to quit at any time Setup editor (VSCode should be already available as 'code') Add full folder path that includes executable to system path environment variable, separate with semi-colon Restart Bash Create alias for editor notepad++ ~/.bash_profile alias npp='notepad++ -mulitInst - nosession' git config --global core.editor \"notepad++ -multiInst -nosession\" git config --global -e Setup Diff & Merge Tool This section describes how to use p4merge. Use VS Code instead git config --global diff.tool p4merge git config --gloabl difftool.p4merge.path \"C:/... /p4merge.exe git config --global difftool.prompt false git config --global merge.tool p4merge git config --global mergetool.p4merge.path \"C:/.../p4merge.exe git config --global mergetool.prompt false Git Workflow \u00b6 Common workflow on master \u00b6 Navigate to root project folder in Windows Explorer Right-click and select 'Git Bash Here' from context menu git status commit and push any changes if found, but usually wouldn't be Work on a feature You can open, add, and remove files using the Windows Explorer GUI or using unix commands in Git Bash Commit changes locally If new files were added, git add -A then git commit -m \"Message\" Otherwise, express commit git commit -am \"Message\" Push to GitHub git pull origin master in case others are working on it git push origin master Common workflow on branch \u00b6 Creating and committing a branch \u00b6 git branch <name> <- creates branch git checkout <branch name> <- checks out make changes git add -A git commit -m \"Message\" git push -u origin <branch name> (first time only) git branch -a (to confirm) Merging to master \u00b6 git checkout master git pull origin master git branch --merged (which branches have been merged?) git merge <branch name> git push origin master git branch -d <branch name> (to delete branch locally) git push origin --delete <branch name> (to delete the remote branch) Workflow Commands \u00b6 Adding files to staging area Use git add <filename> to add file Use git add -A to add everything in the working directory (Use git reset <filename> to remove from the staging area or git reset to remove everything) Regular commit git commit -m \"Message\" If -m \"Message\" is excluded, the default editor will be opened and a message should be inputted. No need to add quotes or anything else. Line breaks will be ignored. Express commit git commit -am \"Message\" For any files already tracked (use git ls-files if unsure or git status to see what's in staging area) Renaming files git mv <file1> <file2> git commit -m \"message\" also can use git add -A if changes (renames, deletions) made outside of git Delete files (from git tracking) git rm filename git commit -m \"message\" also can use git add -u if files deleted outside of git Cloning remote repositories git clone <url> git remote -v <- shows remote connection git branch -a <- shows branches in repository Pushing git diff <- shows changes Example workflow Other Common Commands \u00b6 Help git <action> --help Remove file after adding to .gitignore 1 2 3 git rm -r --cached <file or folder name> git commit -m \"Removed files message\" git push origin master List files git ls-files Open file with default editor start <file name> Open file with VSCode code <file name> (set up default editors with aliases or use start and ensure preferred editor is system default) Diffs & Merges \u00b6 If you encounter a merge issue after pulling from the repo, and it can't be automatically merged, open the file in VSCode and accept/reject changes (will be highlighted in VSCode). code <mergefile> Then save and close the editor. You can now add the file to the staging area and push all changes to the repo ( git commit -m \"Fixed merge conflict\" > git push ). Working Together \u00b6 There are two common ways of collaborating: Fork the repo and submit pull requests to the repo owner Add Collaborators to your repo to give others push authority Forking the repo and submitting pull requests is the safest, as the repo owner is in charge of reviewing all proposed changes before integrating them into the repo. However, that can create a lot of work for the repo owner depending on the frequency of commits. Adding Collaborators can be done in the Settings tab of a repo. This allows anyone listed as a collaborator to work on the repo as if it was their own. This will streamline the workflow, but you risk missing simple mistakes, severe mistakes, and malicious intent. https://kbroman.org/github_tutorial/pages/fork.html Removing files \u00b6 If you want to remove a file that has already been committed: Add the file to the .gitignore file Use the command git rm --cached <filename> Adding empty folders \u00b6 Empty folders are automatically ignored by git, since it only stores files. Sometimes you want to include an empty folder if you want to commit a folder structure before populating it with files or add a folder for temporarily storing data. To do this you need to add an empty file to the folder. By convention, we can call this .keep or .gitkeep . Create a new text document within the folder you want to keep, leave it empty, and save it as .keep or .gitkeep . If you also want to ignore the contents of this folder, but keep the folder (e.g., for log files or temporary data that isn't deleted), add the following to the .gitignore file: 1 2 3 4 5 # ignore the files in the folder foo foo /* # but keep the folder !foo/.keep","title":"Notes"},{"location":"git/notes/#setting-up-git-github","text":"","title":"Setting Up Git &amp; GitHub"},{"location":"git/notes/#download-git","text":"git-scm.com Download & run installer git --version on cmd to make sure its successfully installed","title":"Download Git"},{"location":"git/notes/#configure-git","text":"git config --global user.name <name> git config --global user.email <email> git config --list to see all settings Git Log git log <- all commits git show <- last commit git ls-files <- lists all files that git is tracking git log --oneline --graph --decorate --all <-more detailed history of commits","title":"Configure Git"},{"location":"git/notes/#to-create-an-alias-for-the-git-history-command","text":"git config --global alias.hist \"log --oneline --graph --decorate --all\" now use git hist to see the same command, note it still accepts additional arguments (for example, provide --filename to see history for one file) History will be served line by line, type q to quit at any time Setup editor (VSCode should be already available as 'code') Add full folder path that includes executable to system path environment variable, separate with semi-colon Restart Bash Create alias for editor notepad++ ~/.bash_profile alias npp='notepad++ -mulitInst - nosession' git config --global core.editor \"notepad++ -multiInst -nosession\" git config --global -e Setup Diff & Merge Tool This section describes how to use p4merge. Use VS Code instead git config --global diff.tool p4merge git config --gloabl difftool.p4merge.path \"C:/... /p4merge.exe git config --global difftool.prompt false git config --global merge.tool p4merge git config --global mergetool.p4merge.path \"C:/.../p4merge.exe git config --global mergetool.prompt false","title":"To create an alias for the git history command"},{"location":"git/notes/#git-workflow","text":"","title":"Git Workflow"},{"location":"git/notes/#common-workflow-on-master","text":"Navigate to root project folder in Windows Explorer Right-click and select 'Git Bash Here' from context menu git status commit and push any changes if found, but usually wouldn't be Work on a feature You can open, add, and remove files using the Windows Explorer GUI or using unix commands in Git Bash Commit changes locally If new files were added, git add -A then git commit -m \"Message\" Otherwise, express commit git commit -am \"Message\" Push to GitHub git pull origin master in case others are working on it git push origin master","title":"Common workflow on master"},{"location":"git/notes/#common-workflow-on-branch","text":"","title":"Common workflow on branch"},{"location":"git/notes/#creating-and-committing-a-branch","text":"git branch <name> <- creates branch git checkout <branch name> <- checks out make changes git add -A git commit -m \"Message\" git push -u origin <branch name> (first time only) git branch -a (to confirm)","title":"Creating and committing a branch"},{"location":"git/notes/#merging-to-master","text":"git checkout master git pull origin master git branch --merged (which branches have been merged?) git merge <branch name> git push origin master git branch -d <branch name> (to delete branch locally) git push origin --delete <branch name> (to delete the remote branch)","title":"Merging to master"},{"location":"git/notes/#workflow-commands","text":"Adding files to staging area Use git add <filename> to add file Use git add -A to add everything in the working directory (Use git reset <filename> to remove from the staging area or git reset to remove everything) Regular commit git commit -m \"Message\" If -m \"Message\" is excluded, the default editor will be opened and a message should be inputted. No need to add quotes or anything else. Line breaks will be ignored. Express commit git commit -am \"Message\" For any files already tracked (use git ls-files if unsure or git status to see what's in staging area) Renaming files git mv <file1> <file2> git commit -m \"message\" also can use git add -A if changes (renames, deletions) made outside of git Delete files (from git tracking) git rm filename git commit -m \"message\" also can use git add -u if files deleted outside of git Cloning remote repositories git clone <url> git remote -v <- shows remote connection git branch -a <- shows branches in repository Pushing git diff <- shows changes Example workflow","title":"Workflow Commands"},{"location":"git/notes/#other-common-commands","text":"Help git <action> --help Remove file after adding to .gitignore 1 2 3 git rm -r --cached <file or folder name> git commit -m \"Removed files message\" git push origin master List files git ls-files Open file with default editor start <file name> Open file with VSCode code <file name> (set up default editors with aliases or use start and ensure preferred editor is system default)","title":"Other Common Commands"},{"location":"git/notes/#diffs-merges","text":"If you encounter a merge issue after pulling from the repo, and it can't be automatically merged, open the file in VSCode and accept/reject changes (will be highlighted in VSCode). code <mergefile> Then save and close the editor. You can now add the file to the staging area and push all changes to the repo ( git commit -m \"Fixed merge conflict\" > git push ).","title":"Diffs &amp; Merges"},{"location":"git/notes/#working-together","text":"There are two common ways of collaborating: Fork the repo and submit pull requests to the repo owner Add Collaborators to your repo to give others push authority Forking the repo and submitting pull requests is the safest, as the repo owner is in charge of reviewing all proposed changes before integrating them into the repo. However, that can create a lot of work for the repo owner depending on the frequency of commits. Adding Collaborators can be done in the Settings tab of a repo. This allows anyone listed as a collaborator to work on the repo as if it was their own. This will streamline the workflow, but you risk missing simple mistakes, severe mistakes, and malicious intent. https://kbroman.org/github_tutorial/pages/fork.html","title":"Working Together"},{"location":"git/notes/#removing-files","text":"If you want to remove a file that has already been committed: Add the file to the .gitignore file Use the command git rm --cached <filename>","title":"Removing files"},{"location":"git/notes/#adding-empty-folders","text":"Empty folders are automatically ignored by git, since it only stores files. Sometimes you want to include an empty folder if you want to commit a folder structure before populating it with files or add a folder for temporarily storing data. To do this you need to add an empty file to the folder. By convention, we can call this .keep or .gitkeep . Create a new text document within the folder you want to keep, leave it empty, and save it as .keep or .gitkeep . If you also want to ignore the contents of this folder, but keep the folder (e.g., for log files or temporary data that isn't deleted), add the following to the .gitignore file: 1 2 3 4 5 # ignore the files in the folder foo foo /* # but keep the folder !foo/.keep","title":"Adding empty folders"},{"location":"metrics-design/overview/","text":"Metrics Design Philosophy \u00b6 Our metrics design philosophy has been developed and tested through many years building solutions for clients and internal initiatives alike. It is based on a variety of different frameworks, from the Open Standards for the Practice of Conservation to Systems Thinking to Human-Centered Design (see list of inspirations below). To be honest, very little in this philosophy is new. What is unique about it, however, is its synthesis of these various frameworks. Human-centered design typically assumes you already have your solution, and are simply adding a new feature. Open Standards/Systems Thinking are focused on identifying the right place to intervene - but are, in my opinion, lacking in specifics as to how to design and implement a successful solution. Bringing the concepts within these processes together creates a more holistic, and hopefully impactful, approach to conservation. So how is it unique? \u00b6 There's a few key characteristics of this philosophy that make it unique. People-first : This philosophy is unapologetically people-centric. We seek to change people's behavior to achieve our conservation targets. We do that by solving people's problems or creating new opportunities for people. If changing human behavior isn't at the core of your problem or central to the success of your strategy, this isn't the right process for you. Evidence-driven : Prepare to test every assumption! We seek evidence from the very beginning of the process - we do not wait until we've implemented the program to start testing our assumptions. Importantly, we do not even entertain solutions until we have strong evidence that the problem exists and that solving it will help achieve our conservation target. And we never commit to our solution unless we see results in the real world. Inside-out : If you're used to spending years developing the perfect program, creating a monitoring plan, and then seeing what happens, this might be disorienting. Instead, we'll start with the most core functionality - the smallest solution we could build while still meeting the need - and use that to test our assumptions. Once we have some positive evidence that our solution is working for our intended users, we'll build the next piece. We'll greatly reduce the chances of wasting our time (or even making the problem worse) if we fail fast and fail small , as some like to say. Once we've found success, we iterate and scale up. The Process \u00b6 We've codified our philosophy into a seven step process. Each step is described in more detail on the pages linked through here. Also, note that there are templates, additional guidance, and other resources available in Additional Resources , many of which are referenced within these linked pages. Process Steps \u00b6 \u200b Step 1 : Start at the End \u200b Step 2 : Map It \u200b Step 3 : Develop Empathy \u200b Step 4 : Define the Problem \u200b Step 5 : Identify a Solution \u200b Step 6 : Implement \u200b Step 7 : Iterate & Adapt Additional Resources \u00b6 \u200b Scoping Canvas \u200b Product Definition \u200b Persona Development Guide \u200b Interview Guide Inspiration \u00b6 These resources and frameworks were leaned on heavily in the development of this approach. Open Standards for the Practice of Conservation Outcomes Mapping ( 1 ) ( 2 ) ( 3 ) & EI Speed Training Systems Thinking ( Thinking in Systems , Systems Thinking for Social Change , Omidyar Systems Practice Workbook) Design Thinking & Lean UX ( Design Sprint , IDEO , Venture Design , The Lean Startup ) Transformative Scenario Planning (Adam Kahane) Structured Decision Making","title":"Overview"},{"location":"metrics-design/overview/#metrics-design-philosophy","text":"Our metrics design philosophy has been developed and tested through many years building solutions for clients and internal initiatives alike. It is based on a variety of different frameworks, from the Open Standards for the Practice of Conservation to Systems Thinking to Human-Centered Design (see list of inspirations below). To be honest, very little in this philosophy is new. What is unique about it, however, is its synthesis of these various frameworks. Human-centered design typically assumes you already have your solution, and are simply adding a new feature. Open Standards/Systems Thinking are focused on identifying the right place to intervene - but are, in my opinion, lacking in specifics as to how to design and implement a successful solution. Bringing the concepts within these processes together creates a more holistic, and hopefully impactful, approach to conservation.","title":"Metrics Design Philosophy"},{"location":"metrics-design/overview/#so-how-is-it-unique","text":"There's a few key characteristics of this philosophy that make it unique. People-first : This philosophy is unapologetically people-centric. We seek to change people's behavior to achieve our conservation targets. We do that by solving people's problems or creating new opportunities for people. If changing human behavior isn't at the core of your problem or central to the success of your strategy, this isn't the right process for you. Evidence-driven : Prepare to test every assumption! We seek evidence from the very beginning of the process - we do not wait until we've implemented the program to start testing our assumptions. Importantly, we do not even entertain solutions until we have strong evidence that the problem exists and that solving it will help achieve our conservation target. And we never commit to our solution unless we see results in the real world. Inside-out : If you're used to spending years developing the perfect program, creating a monitoring plan, and then seeing what happens, this might be disorienting. Instead, we'll start with the most core functionality - the smallest solution we could build while still meeting the need - and use that to test our assumptions. Once we have some positive evidence that our solution is working for our intended users, we'll build the next piece. We'll greatly reduce the chances of wasting our time (or even making the problem worse) if we fail fast and fail small , as some like to say. Once we've found success, we iterate and scale up.","title":"So how is it unique?"},{"location":"metrics-design/overview/#the-process","text":"We've codified our philosophy into a seven step process. Each step is described in more detail on the pages linked through here. Also, note that there are templates, additional guidance, and other resources available in Additional Resources , many of which are referenced within these linked pages.","title":"The Process"},{"location":"metrics-design/overview/#process-steps","text":"\u200b Step 1 : Start at the End \u200b Step 2 : Map It \u200b Step 3 : Develop Empathy \u200b Step 4 : Define the Problem \u200b Step 5 : Identify a Solution \u200b Step 6 : Implement \u200b Step 7 : Iterate & Adapt","title":"Process Steps"},{"location":"metrics-design/overview/#additional-resources","text":"\u200b Scoping Canvas \u200b Product Definition \u200b Persona Development Guide \u200b Interview Guide","title":"Additional Resources"},{"location":"metrics-design/overview/#inspiration","text":"These resources and frameworks were leaned on heavily in the development of this approach. Open Standards for the Practice of Conservation Outcomes Mapping ( 1 ) ( 2 ) ( 3 ) & EI Speed Training Systems Thinking ( Thinking in Systems , Systems Thinking for Social Change , Omidyar Systems Practice Workbook) Design Thinking & Lean UX ( Design Sprint , IDEO , Venture Design , The Lean Startup ) Transformative Scenario Planning (Adam Kahane) Structured Decision Making","title":"Inspiration"},{"location":"metrics-design/step1-start-at-the-end/","text":"Step 1: Start at the End \u00b6 Describe the world as it would be if the problem were solved. Be agnostic about the solution, just describe your vision for world. A vision statement is a good place to start. A vision statement should meet the following criteria: Idealistic About the future Observable Not about the intervention Don\u2019t wordsmith the vision statement quite yet. It\u2019s just intended to provide a guiding light or goal post throughout this process. You may choose to re-write it later once you better understand the problem. If using the Open Standards, include in your vision the conservation targets . Don\u2019t worry about defining measurable goals yet, unless they are already defined. If desired, write down design principles to guide you through this process. These principles should reflect your values and any constraints on the solution; they will guide decision making throughout the process. See examples here . Other Techniques \u00b6 Ask a question: If you\u2019re not quite sure how you think the world could be improved, but sense that there is room for improvement, just ask yourself the question: how might this be better? Could we do this in a different (e.g., more efficient/equitable) way? Keep asking yourself questions until you have a good sense of what can be changed. Asking the right design question is essential to solving the right problem. Bug list: another approach to visualizing a better world is writing down all of the things that bug you about the current situation. Is one of those \u2018bugs\u2019 sufficiently problematic that it\u2019s worth investing time and resources in addressing? More Info on This Step \u00b6 See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets. They define a vision statement as relatively general, visionary, and brief. See Omidyar Group\u2019s Systems Practice section Set Your Goals.","title":"Step 1: Start at the End"},{"location":"metrics-design/step1-start-at-the-end/#step-1-start-at-the-end","text":"Describe the world as it would be if the problem were solved. Be agnostic about the solution, just describe your vision for world. A vision statement is a good place to start. A vision statement should meet the following criteria: Idealistic About the future Observable Not about the intervention Don\u2019t wordsmith the vision statement quite yet. It\u2019s just intended to provide a guiding light or goal post throughout this process. You may choose to re-write it later once you better understand the problem. If using the Open Standards, include in your vision the conservation targets . Don\u2019t worry about defining measurable goals yet, unless they are already defined. If desired, write down design principles to guide you through this process. These principles should reflect your values and any constraints on the solution; they will guide decision making throughout the process. See examples here .","title":"Step 1: Start at the End"},{"location":"metrics-design/step1-start-at-the-end/#other-techniques","text":"Ask a question: If you\u2019re not quite sure how you think the world could be improved, but sense that there is room for improvement, just ask yourself the question: how might this be better? Could we do this in a different (e.g., more efficient/equitable) way? Keep asking yourself questions until you have a good sense of what can be changed. Asking the right design question is essential to solving the right problem. Bug list: another approach to visualizing a better world is writing down all of the things that bug you about the current situation. Is one of those \u2018bugs\u2019 sufficiently problematic that it\u2019s worth investing time and resources in addressing?","title":"Other Techniques"},{"location":"metrics-design/step1-start-at-the-end/#more-info-on-this-step","text":"See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets. They define a vision statement as relatively general, visionary, and brief. See Omidyar Group\u2019s Systems Practice section Set Your Goals.","title":"More Info on This Step"},{"location":"metrics-design/step2-map-it/","text":"Step 2: Map It \u00b6 Documenting your understanding of the problem space in a visual way is an impactful way of capturing your understanding and can be a good way of sharing it with others. A Situation Model , Systems Map , or Journey Map are all great tools. The Situation Model focuses on relationships between human causes (drivers) of threats to your vision (conservation targets); whereas the Systems Map describes feedback loops between relevant elements of the system. Journey Maps capture the chain of behaviors required to solve a problem or do a job. If you can, start by defining the scope of the system that you will be exploring, such as geographic boundaries or levels (e.g., federal vs. state gov\u2019t). This will help focus your search to just that area of the problem space in which you have a manageable interest . Alternatively, use a framing question to focus your interrogation of the system. Developing these maps typically requires formative research and immersion in the problem space. Start by listing the factors (i.e., threats, drivers and enabling conditions ) that you know and building out the map using a breadth-first search of the problem space. Add to the map anything that seems relevant, but don\u2019t spend too much time understanding how. Group items using affinity mapping until the relationships become clear. Next, focus on what appear to be the most important elements with an in-depth search. Ask an expert: experts can quickly help parse what is important and what is not, but be aware of the bias any expert brings. When interviewing experts, research enough that they don\u2019t feel obligated to spend all of their time educating you on the basics\u2014you want them to really focus on the nuance that they have come to understand by becoming an expert. Write down your findings and document your sources in brief narrative format; don\u2019t waste time developing a long report. Finally, connect the elements based on their relationships. Identify central nodes, leverage points, etc. Explore the feedback loops present between factors, including both positive (reinforcing) and negative (balancing) loops. The map you create can be a good communication tool, but it is often only helpful to those who created it. To really communicate your understanding, you must tell a story for the relevant relationships you have identified. In a Situation Model, describe the key pathways from drivers to threats and impacts on targets. In a Systems Map, describe the major balancing and reinforcing feedback loops. Transformative Scenario Planning relies almost entirely on story-telling to shape understanding. If you can tell a convincing story about what is causing the problem, you will be better able to test and communicate that understanding. More on This Step \u00b6 See Thinking in Systems by Donella Meadows, Chapter 5 for a discussion on the challenges of appropriately defining scope. See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets for a general discussion of scope. Scopes are both \u201cplace-based\u201d and \u201cthematic-based\u201d.See CMP-Open Standards v3.0 section 1C-1D for more on Situation Models. See publication Building Ecosystem Services Conceptual Models (Olander et al) for helpful guidance. See Systems Practice by Omidyar Group, section Gaining Clarity . Context Mapping (Historical, Environmental, Societal/Cultural, Technological, Political) Systems Maps or Conceptual Models? \u00b6 One drawback of simple causal conceptual models typically depicted by Open Standards frameworks is that they imply single and unidirectional causalities between specific pressures and ecosystem conditions (Schwartz et al 2012; Niemeijer and de Groot 2008; Smeets and Weterings 1999). In reality, a specific pressure may affect multiple states and a specific state may be affected by multiple pressures. Instead of simple causal chain frameworks, Niemeijer and de Groot (2008) proposed that complex systems are better represented by causal networks, in which multiple causal chains interact and interconnect.","title":"Step 2: Map It"},{"location":"metrics-design/step2-map-it/#step-2-map-it","text":"Documenting your understanding of the problem space in a visual way is an impactful way of capturing your understanding and can be a good way of sharing it with others. A Situation Model , Systems Map , or Journey Map are all great tools. The Situation Model focuses on relationships between human causes (drivers) of threats to your vision (conservation targets); whereas the Systems Map describes feedback loops between relevant elements of the system. Journey Maps capture the chain of behaviors required to solve a problem or do a job. If you can, start by defining the scope of the system that you will be exploring, such as geographic boundaries or levels (e.g., federal vs. state gov\u2019t). This will help focus your search to just that area of the problem space in which you have a manageable interest . Alternatively, use a framing question to focus your interrogation of the system. Developing these maps typically requires formative research and immersion in the problem space. Start by listing the factors (i.e., threats, drivers and enabling conditions ) that you know and building out the map using a breadth-first search of the problem space. Add to the map anything that seems relevant, but don\u2019t spend too much time understanding how. Group items using affinity mapping until the relationships become clear. Next, focus on what appear to be the most important elements with an in-depth search. Ask an expert: experts can quickly help parse what is important and what is not, but be aware of the bias any expert brings. When interviewing experts, research enough that they don\u2019t feel obligated to spend all of their time educating you on the basics\u2014you want them to really focus on the nuance that they have come to understand by becoming an expert. Write down your findings and document your sources in brief narrative format; don\u2019t waste time developing a long report. Finally, connect the elements based on their relationships. Identify central nodes, leverage points, etc. Explore the feedback loops present between factors, including both positive (reinforcing) and negative (balancing) loops. The map you create can be a good communication tool, but it is often only helpful to those who created it. To really communicate your understanding, you must tell a story for the relevant relationships you have identified. In a Situation Model, describe the key pathways from drivers to threats and impacts on targets. In a Systems Map, describe the major balancing and reinforcing feedback loops. Transformative Scenario Planning relies almost entirely on story-telling to shape understanding. If you can tell a convincing story about what is causing the problem, you will be better able to test and communicate that understanding.","title":"Step 2: Map It"},{"location":"metrics-design/step2-map-it/#more-on-this-step","text":"See Thinking in Systems by Donella Meadows, Chapter 5 for a discussion on the challenges of appropriately defining scope. See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets for a general discussion of scope. Scopes are both \u201cplace-based\u201d and \u201cthematic-based\u201d.See CMP-Open Standards v3.0 section 1C-1D for more on Situation Models. See publication Building Ecosystem Services Conceptual Models (Olander et al) for helpful guidance. See Systems Practice by Omidyar Group, section Gaining Clarity . Context Mapping (Historical, Environmental, Societal/Cultural, Technological, Political)","title":"More on This Step"},{"location":"metrics-design/step2-map-it/#systems-maps-or-conceptual-models","text":"One drawback of simple causal conceptual models typically depicted by Open Standards frameworks is that they imply single and unidirectional causalities between specific pressures and ecosystem conditions (Schwartz et al 2012; Niemeijer and de Groot 2008; Smeets and Weterings 1999). In reality, a specific pressure may affect multiple states and a specific state may be affected by multiple pressures. Instead of simple causal chain frameworks, Niemeijer and de Groot (2008) proposed that complex systems are better represented by causal networks, in which multiple causal chains interact and interconnect.","title":"Systems Maps or Conceptual Models?"},{"location":"metrics-design/step3-develop-empathy/","text":"Step 3: Develop Empathy \u00b6 To really achieve the change that will bring about your vision, you must first change peoples\u2019 behavior. You can\u2019t arm rhinos with bazookas or convince sage-grouse to move to the city. The first step is to identify stakeholders\u2014all of the people that can affect or will be affected by the change you want to see. Then, you will seek to develop empathy for key stakeholders and their needs/problems. This will provide insight into a solution that is more likely to be adopted, supported, persist, and create real change. Create a stakeholder list . Search or ask around to build out your list. Consider government agencies, NGOs, academia, individual businesses, industry groups, segments of the population, etc. Group stakeholders to create a manageable list if possible, but try not to obscure the efforts of individual organizations. For example, if Molson-Coors is very active in your area of interest, keep them separate. If you're more worried about the impact of beverage companies in general, group them together. As you're building your list, capture a sentence or two about why they are relevant so you don\u2019t forget. You can categorize stakeholders based on how directly you can influence them. Stakeholders that you can directly influence are Boundary Partners . Stakeholders that you can work with directly, but can\u2019t influence, are Strategic Partners . Stakeholders that you can influence, but not directly, are your boundary partners\u2019 boundary partners. Stakeholders that are not influenceable and can\u2019t be targeted directly are not relevant (yet). Think of your boundary partners as customers, you need to \u2018sell\u2019 your solution to your boundary partners. Your strategic partners support you and provide components of your solution that you don\u2019t focus on. For each of your boundary partners (potential customers), develop a persona profile . If necessary, you can create multiple personas for a single boundary partner. For example, if you included farmers as a boundary partner, you may need to distinguish between farmers who derive their entire income from farming and hobby farmers. If you are trying to influence a very targeted set of stakeholders, you may simply research the specific people you are targeting, rather than generalized personas. In either case, the process is largely the same. The challenge is drafting personas that contain relevant facts based on truth. While short, personas must be well researched. To do persona research , you can use ethnography and psychographic research methods. Read relevant literature, studies, or surveys that have been conducted, search out their blogs, read their periodicals, imagine a day in the life . Use the framework of thinks/sees/feels/does . Understand their mental models \u2014how do they think about the problem space? Consider their primary interests which most drive their behavior. Develop persona hypotheses \u2014expectations that you have about their character, how they see the world, what their background is, etc. Conduct exploratory interviews (develop an interview guide first) to test your hypotheses (you might combine this with interviews to explore problem hypotheses at the same time, see step 4). Focus on common, not idiosyncratic, characteristics. Refine. Repeat. Don\u2019t stop until you have a clear mental picture of the persona, and you are confident that you have segmented your stakeholders into the right personas. Consider this a process, not a product. Keep updating personas as you continue to understand the problem space and your solution. Good personas are real, exact, actionable, clear, and testable ( REACT ). What do they like to do on the weekend? More on This Step \u00b6 Alex Cowan\u2019s Venture Design process is an excellent resource for persona development. See the persona development guidance for more on personas. EI has also developed an Ideal Client Persona Template for external marketing, which can be adapted. See Systems Thinking for Social Change , for more on mapping mental models into a systems map. From Our Work \u00b6 In designing the Pollinator Scorecard, we developed personas around three groups: \u2018Wingtips\u2019, \u2018Steel Toes\u2019, and \u2018Tevas\u2019. These user groups divided our target industry in a way that best reflected their uses for, and ability to use, the Pollinator Scorecard. It was helpful to both the project team in designing the Scorecard (by putting us in our users\u2019 \u2018shoes\u2019, so to speak) and when presenting the Scorecard to users to help them understand why we made the design decisions we did.","title":"Step 3: Develop Empathy"},{"location":"metrics-design/step3-develop-empathy/#step-3-develop-empathy","text":"To really achieve the change that will bring about your vision, you must first change peoples\u2019 behavior. You can\u2019t arm rhinos with bazookas or convince sage-grouse to move to the city. The first step is to identify stakeholders\u2014all of the people that can affect or will be affected by the change you want to see. Then, you will seek to develop empathy for key stakeholders and their needs/problems. This will provide insight into a solution that is more likely to be adopted, supported, persist, and create real change. Create a stakeholder list . Search or ask around to build out your list. Consider government agencies, NGOs, academia, individual businesses, industry groups, segments of the population, etc. Group stakeholders to create a manageable list if possible, but try not to obscure the efforts of individual organizations. For example, if Molson-Coors is very active in your area of interest, keep them separate. If you're more worried about the impact of beverage companies in general, group them together. As you're building your list, capture a sentence or two about why they are relevant so you don\u2019t forget. You can categorize stakeholders based on how directly you can influence them. Stakeholders that you can directly influence are Boundary Partners . Stakeholders that you can work with directly, but can\u2019t influence, are Strategic Partners . Stakeholders that you can influence, but not directly, are your boundary partners\u2019 boundary partners. Stakeholders that are not influenceable and can\u2019t be targeted directly are not relevant (yet). Think of your boundary partners as customers, you need to \u2018sell\u2019 your solution to your boundary partners. Your strategic partners support you and provide components of your solution that you don\u2019t focus on. For each of your boundary partners (potential customers), develop a persona profile . If necessary, you can create multiple personas for a single boundary partner. For example, if you included farmers as a boundary partner, you may need to distinguish between farmers who derive their entire income from farming and hobby farmers. If you are trying to influence a very targeted set of stakeholders, you may simply research the specific people you are targeting, rather than generalized personas. In either case, the process is largely the same. The challenge is drafting personas that contain relevant facts based on truth. While short, personas must be well researched. To do persona research , you can use ethnography and psychographic research methods. Read relevant literature, studies, or surveys that have been conducted, search out their blogs, read their periodicals, imagine a day in the life . Use the framework of thinks/sees/feels/does . Understand their mental models \u2014how do they think about the problem space? Consider their primary interests which most drive their behavior. Develop persona hypotheses \u2014expectations that you have about their character, how they see the world, what their background is, etc. Conduct exploratory interviews (develop an interview guide first) to test your hypotheses (you might combine this with interviews to explore problem hypotheses at the same time, see step 4). Focus on common, not idiosyncratic, characteristics. Refine. Repeat. Don\u2019t stop until you have a clear mental picture of the persona, and you are confident that you have segmented your stakeholders into the right personas. Consider this a process, not a product. Keep updating personas as you continue to understand the problem space and your solution. Good personas are real, exact, actionable, clear, and testable ( REACT ). What do they like to do on the weekend?","title":"Step 3: Develop Empathy"},{"location":"metrics-design/step3-develop-empathy/#more-on-this-step","text":"Alex Cowan\u2019s Venture Design process is an excellent resource for persona development. See the persona development guidance for more on personas. EI has also developed an Ideal Client Persona Template for external marketing, which can be adapted. See Systems Thinking for Social Change , for more on mapping mental models into a systems map.","title":"More on This Step"},{"location":"metrics-design/step3-develop-empathy/#from-our-work","text":"In designing the Pollinator Scorecard, we developed personas around three groups: \u2018Wingtips\u2019, \u2018Steel Toes\u2019, and \u2018Tevas\u2019. These user groups divided our target industry in a way that best reflected their uses for, and ability to use, the Pollinator Scorecard. It was helpful to both the project team in designing the Scorecard (by putting us in our users\u2019 \u2018shoes\u2019, so to speak) and when presenting the Scorecard to users to help them understand why we made the design decisions we did.","title":"From Our Work"},{"location":"metrics-design/step4-define-the-problem/","text":"Step 4: Define the Problem \u00b6 \u201cGiven one hour to save the planet, I would spend 59 minutes understanding the problem and one minute resolving it.\u201d - Attributed to Albert Einstein (probably incorrectly) You may have started this process thinking you know what the problem is, or what strategy you\u2019ll employ. Do you still? It will do no good to solve the wrong problem; it might actually do more harm! There are many examples of well-meaning groups attempting to solve a problem with their \u2018cure-all\u2019 and seen it fall flat--or worse, exacerbate existing problems. To a hammer, the whole world's a nail \u00b6 Many of the frameworks that inspired this approach suggest defining your group's mission at the outset. We prefer to be fully agnostic as to the solution until the problem is clearly defined and evidence supports your understanding. We recognize that your group has unique strengths and experience with some solutions more than others. If the problem doesn't call for your solution, that's fine, you can go solve a different one. The temptation is simply too great to frame the problem in terms of your solution if you come with preconceptions as to what your strategy will be. So how do we define a problem and develop our solution? Let\u2019s go back to our map. If you\u2019ve developed a systems map, look for leverage points . If you\u2019ve developed a situation model, identify key intervention points --which factor will you focus on? If you haven\u2019t already, map other stakeholders\u2019 interventions onto your map. Are there factors that aren\u2019t receiving enough attention? Where could other interventions be leveraged? Don\u2019t worry about the how (i.e., your strategy) yet. Once you\u2019ve identified the factor you\u2019d like to address, review your stakeholder list. Which of your boundary partners are relevant to this factor? Refer back to their personas and describe the problem scenarios that they face relevant to your understanding of the situation scope. These can be either problems or simply jobs to be done: Farmers aren\u2019t sure where to find information on conservation programs. Agency staff need to develop restoration project specifications but aren\u2019t experts in restoration. CSR managers need to meet biodiversity targets but don\u2019t have time to develop habitat projects. You get the picture. Depending on how broad your scope, this task may seem nebulous. Here are a few options to help clarify: For each persona, draft an outcome challenge --the ideal behavior you want each persona to adopt in a world in which your vision is achieved. You can break this down into progress markers by defining what you would expect to see, like to see, and love to see from each persona. Create a journey map . A journey map illustrates the steps the persona takes to complete a relevant task or solve a relevant problem. For example, if you envision restaurants sustainably sourcing ingredients, map out a typical process for creating procurement policies. Where do they begin? Who else is involved? Within the journey map, highlight pain points --areas where the process is difficult. Those pain points may just turn into the problems you choose to tackle. A behavior chain is similar to a journey map, and outlines the unique, self-contained behaviors necessary to complete a more complex behavior. To identify problems that are actually solvable, consider organizing them as parent, anchor and child scenarios. The parent problem is too big; the child scenario to small. The anchor problem , just right. For each anchor problem, consider alternatives --what is the persona doing now instead of what you want them to do; what other options currently exist that they\u2019re not using? Understanding the alternatives is necessary to understand the baseline and to specify conditions of satisfaction --in other words, how you\u2019ll know your solution is sufficiently better than the alternatives that the user will consider it. Craft problem hypotheses for the problems you most want to address or think are most important. These testable propositions will be evaluated through persona interviews or focus groups (you may want to combine this step with the persona interviews in step 3). Before moving on to the next step, where you will identify potential strategies, it is critical that you have a clear understanding of the problem space and have evidence supporting your hypotheses. By now you should have talked to experts and interviewed at least 5 relevant people. For more substantial endeavors, shoot for 30 people. There is no guarantee that this process will deliver a solution that will help you reach your vision for the world, but the chances of having an impact in a system you don\u2019t understand with people you can\u2019t relate to are slim. Research all at once? \u00b6 To maximize the efficiency of your research process, and avoid having to interview people twice, you may want to hold off on interviews until you've developed both persona hypotheses and problem hypotheses. If you have been working in this space for some time and have accumulated sufficient evidence that you are comfortable proposing a solution at this time, you can even hold off on interviews until the next step, once you have a proposal for a solution.","title":"Step 4: Define the Problem"},{"location":"metrics-design/step4-define-the-problem/#step-4-define-the-problem","text":"\u201cGiven one hour to save the planet, I would spend 59 minutes understanding the problem and one minute resolving it.\u201d - Attributed to Albert Einstein (probably incorrectly) You may have started this process thinking you know what the problem is, or what strategy you\u2019ll employ. Do you still? It will do no good to solve the wrong problem; it might actually do more harm! There are many examples of well-meaning groups attempting to solve a problem with their \u2018cure-all\u2019 and seen it fall flat--or worse, exacerbate existing problems.","title":"Step 4: Define the Problem"},{"location":"metrics-design/step4-define-the-problem/#to-a-hammer-the-whole-worlds-a-nail","text":"Many of the frameworks that inspired this approach suggest defining your group's mission at the outset. We prefer to be fully agnostic as to the solution until the problem is clearly defined and evidence supports your understanding. We recognize that your group has unique strengths and experience with some solutions more than others. If the problem doesn't call for your solution, that's fine, you can go solve a different one. The temptation is simply too great to frame the problem in terms of your solution if you come with preconceptions as to what your strategy will be. So how do we define a problem and develop our solution? Let\u2019s go back to our map. If you\u2019ve developed a systems map, look for leverage points . If you\u2019ve developed a situation model, identify key intervention points --which factor will you focus on? If you haven\u2019t already, map other stakeholders\u2019 interventions onto your map. Are there factors that aren\u2019t receiving enough attention? Where could other interventions be leveraged? Don\u2019t worry about the how (i.e., your strategy) yet. Once you\u2019ve identified the factor you\u2019d like to address, review your stakeholder list. Which of your boundary partners are relevant to this factor? Refer back to their personas and describe the problem scenarios that they face relevant to your understanding of the situation scope. These can be either problems or simply jobs to be done: Farmers aren\u2019t sure where to find information on conservation programs. Agency staff need to develop restoration project specifications but aren\u2019t experts in restoration. CSR managers need to meet biodiversity targets but don\u2019t have time to develop habitat projects. You get the picture. Depending on how broad your scope, this task may seem nebulous. Here are a few options to help clarify: For each persona, draft an outcome challenge --the ideal behavior you want each persona to adopt in a world in which your vision is achieved. You can break this down into progress markers by defining what you would expect to see, like to see, and love to see from each persona. Create a journey map . A journey map illustrates the steps the persona takes to complete a relevant task or solve a relevant problem. For example, if you envision restaurants sustainably sourcing ingredients, map out a typical process for creating procurement policies. Where do they begin? Who else is involved? Within the journey map, highlight pain points --areas where the process is difficult. Those pain points may just turn into the problems you choose to tackle. A behavior chain is similar to a journey map, and outlines the unique, self-contained behaviors necessary to complete a more complex behavior. To identify problems that are actually solvable, consider organizing them as parent, anchor and child scenarios. The parent problem is too big; the child scenario to small. The anchor problem , just right. For each anchor problem, consider alternatives --what is the persona doing now instead of what you want them to do; what other options currently exist that they\u2019re not using? Understanding the alternatives is necessary to understand the baseline and to specify conditions of satisfaction --in other words, how you\u2019ll know your solution is sufficiently better than the alternatives that the user will consider it. Craft problem hypotheses for the problems you most want to address or think are most important. These testable propositions will be evaluated through persona interviews or focus groups (you may want to combine this step with the persona interviews in step 3). Before moving on to the next step, where you will identify potential strategies, it is critical that you have a clear understanding of the problem space and have evidence supporting your hypotheses. By now you should have talked to experts and interviewed at least 5 relevant people. For more substantial endeavors, shoot for 30 people. There is no guarantee that this process will deliver a solution that will help you reach your vision for the world, but the chances of having an impact in a system you don\u2019t understand with people you can\u2019t relate to are slim.","title":"To a hammer, the whole world's a nail"},{"location":"metrics-design/step4-define-the-problem/#research-all-at-once","text":"To maximize the efficiency of your research process, and avoid having to interview people twice, you may want to hold off on interviews until you've developed both persona hypotheses and problem hypotheses. If you have been working in this space for some time and have accumulated sufficient evidence that you are comfortable proposing a solution at this time, you can even hold off on interviews until the next step, once you have a proposal for a solution.","title":"Research all at once?"},{"location":"metrics-design/step5-identify-a-solution/","text":"Step 5: Identify a Solution \u00b6 This is the fun part! In terms of the design \u2018 double diamond ,\u2019 we are at the central convergence point. Starting from our vision, we\u2019ve diverged to explore the problem space and its stakeholders, and have since converged around a problem worth solving. Now, we get to come up with a solution. Call it brainstorming or ideation or, my personal favorite, spitballing . Just don\u2019t take it too seriously and don\u2019t do it alone. There are a plethora of techniques for working in groups to create solutions. Can you invest in a weeklong design sprint ? If not, try this one day version. Use sketching to help you visualize the solution. Timed idea generation , mind mapping , storyboarding , journey maps are all great options to get a creative buzz on. Reframe your problem scenarios into opportunities by asking \u2018 how might we? \u2019. Think about the problem from multiple angles. Create frameworks to help you see the problem with a different lens. Use many model thinking . Consider both extremes and mainstreams . Try coming up with your worst idea . Look at similar projects or other people\u2019s interventions and describe their challenges, insights, and opportunities . Consider the problem and define success --what would a good solution look like? You\u2019re looking for a solution that is all three: desirable, feasible, viable . Ideally one that also gets people in your group excited and overlaps with your unique strengths . Once you\u2019ve arrived at a potential conceptual solution, it\u2019s time to put it to the test. For the people for whom you\u2019re trying to solve this problem, craft a value proposition for each of the problem scenarios you\u2019ve already defined. Also include the current alternative so you know what you\u2019re competing against. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Does the solution appear to be better than the alternative? How do you know? Do you need to test the value proposition to know for sure? Craft value hypotheses for each value proposition that should be tested. You can state a value hypothesis as \u201cIf we do X for Y, they will Z\u201d. Make sure the behavior you want to see is observable. Some value hypotheses can be tested without building any of the proposed solution, others will require a minimum viable product to test--we\u2019ll work on that in the next step. Finally, it\u2019s good practice to summarize the value of your solution in a positioning statement . Fill in the blanks: For [persona A] who need to [problem scenario], our [solution name] is a [product category] that [value proposition]. Unlike [current alternative], our product [key differentiation]. Draft one for each of your primary personas. Now is also the time to develop a Results Chain to clarify assumptions as to why solving this problem will improve the conservation target and identify metrics for evaluating progress.","title":"Step 5: Identify a Solution"},{"location":"metrics-design/step5-identify-a-solution/#step-5-identify-a-solution","text":"This is the fun part! In terms of the design \u2018 double diamond ,\u2019 we are at the central convergence point. Starting from our vision, we\u2019ve diverged to explore the problem space and its stakeholders, and have since converged around a problem worth solving. Now, we get to come up with a solution. Call it brainstorming or ideation or, my personal favorite, spitballing . Just don\u2019t take it too seriously and don\u2019t do it alone. There are a plethora of techniques for working in groups to create solutions. Can you invest in a weeklong design sprint ? If not, try this one day version. Use sketching to help you visualize the solution. Timed idea generation , mind mapping , storyboarding , journey maps are all great options to get a creative buzz on. Reframe your problem scenarios into opportunities by asking \u2018 how might we? \u2019. Think about the problem from multiple angles. Create frameworks to help you see the problem with a different lens. Use many model thinking . Consider both extremes and mainstreams . Try coming up with your worst idea . Look at similar projects or other people\u2019s interventions and describe their challenges, insights, and opportunities . Consider the problem and define success --what would a good solution look like? You\u2019re looking for a solution that is all three: desirable, feasible, viable . Ideally one that also gets people in your group excited and overlaps with your unique strengths . Once you\u2019ve arrived at a potential conceptual solution, it\u2019s time to put it to the test. For the people for whom you\u2019re trying to solve this problem, craft a value proposition for each of the problem scenarios you\u2019ve already defined. Also include the current alternative so you know what you\u2019re competing against. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Does the solution appear to be better than the alternative? How do you know? Do you need to test the value proposition to know for sure? Craft value hypotheses for each value proposition that should be tested. You can state a value hypothesis as \u201cIf we do X for Y, they will Z\u201d. Make sure the behavior you want to see is observable. Some value hypotheses can be tested without building any of the proposed solution, others will require a minimum viable product to test--we\u2019ll work on that in the next step. Finally, it\u2019s good practice to summarize the value of your solution in a positioning statement . Fill in the blanks: For [persona A] who need to [problem scenario], our [solution name] is a [product category] that [value proposition]. Unlike [current alternative], our product [key differentiation]. Draft one for each of your primary personas. Now is also the time to develop a Results Chain to clarify assumptions as to why solving this problem will improve the conservation target and identify metrics for evaluating progress.","title":"Step 5: Identify a Solution"},{"location":"metrics-design/step6-implement/","text":"Coming soon... \u00b6","title":"Step 6: Implement"},{"location":"metrics-design/step6-implement/#coming-soon","text":"","title":"Coming soon..."},{"location":"metrics-design/step7-adapt/","text":"Coming later... \u00b6","title":"Step 7: Iterate & Adapt"},{"location":"metrics-design/step7-adapt/#coming-later","text":"","title":"Coming later..."},{"location":"metrics-services/how-we-work/","text":"We've refined our metrics design philosophy to a simple, scalable process to support you in the design and development of your tools and programs. Whether you've identified a new need within an existing project, are setting expectations with a client for a new tool, or simply replacing an old tool with a new one, we can help. Expect to meet at least three times as we work to understand the problem, define a solution, and develop a prototype. Our role can be minimal or more involved, from simply helping you think through the important details to building and supporting the product over the long term. Our process can be scaled to fit your needs large or small, supporting projects including: Building an internal tool Conducting a data analysis Creating a map, data visualization or dashboard Evaluating options for a technology solution Scoping a tool as a product/service for you practice area Developing a prototype prior to working with a technology partner Interfacing with technology partners Building a client-facing tool as a deliverable We'll work with you through the process of scoping , product definition , specification , implementation and adaptation . What's described below is optimized for developing a medium- to large-effort tool, but will be similar, though less involved, for lesser engagements. Scoping \u00b6 Scoping is all about understanding the situation and exploring the problem. We often start by asking you to describe your vision. If its our first time working on this program, we'd like to know your vision for the program, your conservation targets, etc. If we've already engaged, then we'll focus on your vision for the world in which this solution exists. Understanding your vision helps us know what a successful solution will look like. If that's too optimistic for you, we can instead focus on the problems you face, decisions you need to make, or jobs to be done. We'll have you list the problems and needs so we can zero in on the key problems, who is involved, and the overall situation. While you're describing the vision or problem, we'll be listening for three things. First, the people involved. Who are the users and what are their roles? Who are the stakeholders (those affected by the tool)? What are the users' capabilities? The list of people will be refined to a list of users and persona descriptions that describe their motivations and constraints. Second, their problems or needs. Which can be addressed? What are people doing instead? How will our solution improve upon the alternatives? Problems will be catalogued and we'll ultimately seek to identify the problem, or set of problems, that our solution will address first. Finally, context. How will this solution integrate with existing tools or processes? What are the requirements, constraints, and design considerations we must keep in mind? If we come up with questions you don't have an answer for, its helpful to have an expert (whether a true expert or just another member of the project team) available to get clarity before we move on. While we're not committing to a particular solution yet, we will want to work with you to identify priority problems to address by asking 'How Might We?'. This question allows us to transform problems into opportunities and get creative about potential solutions. After this meeting, we'll refine our understanding of what you told us, develop testable hypotheses for people and problems, and craft a research plan. We'll do background research to fill in gaps. We may even interview potential users to confirm our persona hypotheses and problem hypotheses. Product Definition \u00b6 Utilizing our research and what you've told us, we'll develop options for solutions and develop a recommendation. We'll return with a Product Definition and, potentially, a 'Minimum Viable Product' (MVP) - some tangible way for you to understand the recommended solution. The MVP is the minimum build sufficient to test key hypotheses we've developed. It may not even function at all (e.g., a 'wizard of oz' approach), but will simulate the experience of using the tool. It might be as simple as a drawing, a wireframe, or even a small version of what we'll eventually develop. We find it's better to have something tangible for you to react to so that you are fully aware of the look/feel and limitations of any solution. Otherwise you might be imaging a cure all that is impossible! Also, we may not be able to solve all of the problems (or meet all of the needs) all at once, if not, we'll pick an 'anchor problem' and start there. We'll keep track of other problems to be solved in future features. This is your chance to give us direction on the scope and solution. Did we overestimate the users capabilities? Forget a key requirement? It's our job to make sure you fully understand the implications of the proposed solution - who will manage it, what is its expected lifetime, what will it cost, etc. We'll also review roles, timeline, and necessary resources. Specification \u00b6 We'll document the specifications for the solution in our template Specifications Document , create our project plan and roadmap, and develop the solution or support you to get the product built with a technology provider. Before working with a technology provider, we may develop a simple prototype that will allow you to test and interact with the product so you can be very clear about what you want and how you want it to work. Implementation \u00b6 Implementation - the development and deployment of the solution- is simply iteration. We strongly recommend implementing the smallest workable subset of a solution possible. Don't wait for the perfect build out before putting it in the hands of your users. Also, don't overlook the importance of outreach to and education of both users and stakeholders. We can help you navigate the cycle of building and testing that is central to solution implementation. Adaptation \u00b6 Almost every tool requires some revision in the first year. We can provide support for the first year and recommend a structured process for adapting the product as needed after a full year of use. The adaptation process should, of course, be scaled to the size of the tool. Why have we developed this process? \u00b6 A common component of the delivery of metrics services is the design of tools. We have developed over the past few years a unique perspective and approach to developing tools to support metrics projects. Our tools can include a technology component (such as a scripting language like R or Python), but many do not. The design and development of tools, whether technological or not, is a unique skillset developed within the Metrics service line over the past few years. We have built out processes and products to facilitate the efficient development of tools that can be used both internally and externally to improve EI services and internal processes. In addition, some EI staff not explicitly included in the Metrics service line have also been developing skills and experience in overlapping areas (e.g., human-centered design). Our aim is to build EI's capacity for effective tool development by promoting the processes and products we have while incorporating the learning of other EI staff in those products and processes. We are not pretending to be a tech company when we are not, nor develop a competency in the development of software or hardware. EI will continue to work with technology providers to deliver robust technology solutions. Instead, this is an effort to describe and promote, internally and externally, a subset of the Metrics service line's skillset that is unique to Metrics service line experts. Tool design is central to this, but it can also include data analytics and visualization as stand-alone services. We also explore the opportunity and requirements to deliver Metrics services outside the typical program design and implementation package to new clients/markets, to existing clients, and to internal initiatives. The Metrics service line can support an effort as small as designing a new budget template to as large as developing an entire information management system for a large agency. We leverage our skills in consulting, design, development, and deployment created through past internal and external work. We work to improve EI's internal practices, expand our staff's understanding of what is possible, and deliver robust, user-friendly tools to clients. Additionally, we seek to increase the capacity of all EI staff to develop lasting solutions.","title":"Metrics Services"},{"location":"metrics-services/how-we-work/#scoping","text":"Scoping is all about understanding the situation and exploring the problem. We often start by asking you to describe your vision. If its our first time working on this program, we'd like to know your vision for the program, your conservation targets, etc. If we've already engaged, then we'll focus on your vision for the world in which this solution exists. Understanding your vision helps us know what a successful solution will look like. If that's too optimistic for you, we can instead focus on the problems you face, decisions you need to make, or jobs to be done. We'll have you list the problems and needs so we can zero in on the key problems, who is involved, and the overall situation. While you're describing the vision or problem, we'll be listening for three things. First, the people involved. Who are the users and what are their roles? Who are the stakeholders (those affected by the tool)? What are the users' capabilities? The list of people will be refined to a list of users and persona descriptions that describe their motivations and constraints. Second, their problems or needs. Which can be addressed? What are people doing instead? How will our solution improve upon the alternatives? Problems will be catalogued and we'll ultimately seek to identify the problem, or set of problems, that our solution will address first. Finally, context. How will this solution integrate with existing tools or processes? What are the requirements, constraints, and design considerations we must keep in mind? If we come up with questions you don't have an answer for, its helpful to have an expert (whether a true expert or just another member of the project team) available to get clarity before we move on. While we're not committing to a particular solution yet, we will want to work with you to identify priority problems to address by asking 'How Might We?'. This question allows us to transform problems into opportunities and get creative about potential solutions. After this meeting, we'll refine our understanding of what you told us, develop testable hypotheses for people and problems, and craft a research plan. We'll do background research to fill in gaps. We may even interview potential users to confirm our persona hypotheses and problem hypotheses.","title":"Scoping"},{"location":"metrics-services/how-we-work/#product-definition","text":"Utilizing our research and what you've told us, we'll develop options for solutions and develop a recommendation. We'll return with a Product Definition and, potentially, a 'Minimum Viable Product' (MVP) - some tangible way for you to understand the recommended solution. The MVP is the minimum build sufficient to test key hypotheses we've developed. It may not even function at all (e.g., a 'wizard of oz' approach), but will simulate the experience of using the tool. It might be as simple as a drawing, a wireframe, or even a small version of what we'll eventually develop. We find it's better to have something tangible for you to react to so that you are fully aware of the look/feel and limitations of any solution. Otherwise you might be imaging a cure all that is impossible! Also, we may not be able to solve all of the problems (or meet all of the needs) all at once, if not, we'll pick an 'anchor problem' and start there. We'll keep track of other problems to be solved in future features. This is your chance to give us direction on the scope and solution. Did we overestimate the users capabilities? Forget a key requirement? It's our job to make sure you fully understand the implications of the proposed solution - who will manage it, what is its expected lifetime, what will it cost, etc. We'll also review roles, timeline, and necessary resources.","title":"Product Definition"},{"location":"metrics-services/how-we-work/#specification","text":"We'll document the specifications for the solution in our template Specifications Document , create our project plan and roadmap, and develop the solution or support you to get the product built with a technology provider. Before working with a technology provider, we may develop a simple prototype that will allow you to test and interact with the product so you can be very clear about what you want and how you want it to work.","title":"Specification"},{"location":"metrics-services/how-we-work/#implementation","text":"Implementation - the development and deployment of the solution- is simply iteration. We strongly recommend implementing the smallest workable subset of a solution possible. Don't wait for the perfect build out before putting it in the hands of your users. Also, don't overlook the importance of outreach to and education of both users and stakeholders. We can help you navigate the cycle of building and testing that is central to solution implementation.","title":"Implementation"},{"location":"metrics-services/how-we-work/#adaptation","text":"Almost every tool requires some revision in the first year. We can provide support for the first year and recommend a structured process for adapting the product as needed after a full year of use. The adaptation process should, of course, be scaled to the size of the tool.","title":"Adaptation"},{"location":"metrics-services/how-we-work/#why-have-we-developed-this-process","text":"A common component of the delivery of metrics services is the design of tools. We have developed over the past few years a unique perspective and approach to developing tools to support metrics projects. Our tools can include a technology component (such as a scripting language like R or Python), but many do not. The design and development of tools, whether technological or not, is a unique skillset developed within the Metrics service line over the past few years. We have built out processes and products to facilitate the efficient development of tools that can be used both internally and externally to improve EI services and internal processes. In addition, some EI staff not explicitly included in the Metrics service line have also been developing skills and experience in overlapping areas (e.g., human-centered design). Our aim is to build EI's capacity for effective tool development by promoting the processes and products we have while incorporating the learning of other EI staff in those products and processes. We are not pretending to be a tech company when we are not, nor develop a competency in the development of software or hardware. EI will continue to work with technology providers to deliver robust technology solutions. Instead, this is an effort to describe and promote, internally and externally, a subset of the Metrics service line's skillset that is unique to Metrics service line experts. Tool design is central to this, but it can also include data analytics and visualization as stand-alone services. We also explore the opportunity and requirements to deliver Metrics services outside the typical program design and implementation package to new clients/markets, to existing clients, and to internal initiatives. The Metrics service line can support an effort as small as designing a new budget template to as large as developing an entire information management system for a large agency. We leverage our skills in consulting, design, development, and deployment created through past internal and external work. We work to improve EI's internal practices, expand our staff's understanding of what is possible, and deliver robust, user-friendly tools to clients. Additionally, we seek to increase the capacity of all EI staff to develop lasting solutions.","title":"Why have we developed this process?"},{"location":"portfolio/project-portfolio/","text":"Project Portfolio \u00b6 These are a few of our projects. Peruse at your leisure. Monarch Registry \u00b6 We worked with a web design firm to develop the Monarch Registry for the Monarch Habitat Exchange. Our role was to design the database, identify reporting metrics, and provide input on visual aesthetic. # custom web app, registry, database Nevada Credit System Site Screening Tool \u00b6 The Nevada Site Screening Tool allows landowners to quickly screen their private lands for potential to participate in the Nevada Credit System based on a number of factors relevant to sage-grouse habitat quality. We partnered with Sitka to develop the web app, its algorithms, and outputs. # custom web app, spatial analysis Roadside Managers Monarch Tool \u00b6 We partnered with UX design students to design an interface for a mobile field data collection app for roadside managers to quickly assess the quality of monarch habitat on roadsides. The design was used to spur interest from funders and as the basis for the final mobile field data collection app design, which was build on ESRI's Survey123. # UX design, Survey123, mobile app Pollinator Habitat Quantification Tool \u00b6 The Pollinator Habitat Quantification Tool evaluates habitat quality in agricultural landscapes for a user-specified list of native pollinators. The tool implements an algorithm described here . The tool is presented in this ESRI StoryMap , which can be viewed below (scroll). # ESRI StoryMap, ArcGIS Pollinator Scorecard \u00b6 The Pollinator Scorecard is designed for electric utilities and other rights-of-way managers to rapidly assess pollinator and monarch habitat on their rights-of-way. It is the required assessment method for the USFWS Monarch Candidate Conservation Agreement with Assurances. Data may be collected through a fillable PDF form or ESRI's Survey123 app. # Low-tech, PDF, Survey123, field protocol, datasheets San Diego Incentives Demographics \u00b6 We created a mapping application to inform the design of rebates and incentives program in San Diego County by evaluating basic demographic information. # ESRI StoryMap, Spatial Analysis, Mapping Monarch Habitat Quantification Tool \u00b6 The Monarch Habitat Quantification Tool is designed to assess monarch habitat quality in agricultural landscapes. We led a team of monarch experts to develop an assessment protocol and habitat quantification approach for EDF. The tool includes an Excel-based spreadsheet calculator, field datasheets, and field data collection protocol. # Excel, Monitoring Protocol, Datasheets Sage-Steppe Habitat Quantification Tool \u00b6 The Sage-Steppe Habitat Quantification Tool facilitates habitat assessment for greater sage-grouse and mule deer in multiple states around the West. Multiple state mitigation programs use a regionalized version of the tool. The tool has been used to inform conservation spending and mitigation of over $1 million (USD). The tool consists of a set of custom script tools using ESRI's ArcGIS API, arcpy and Excel spreadsheet calculators. # ArcGIS, arcpy, python, spatial analysis, Excel, VBA, Excel forms Idaho Greater-Sage Grouse Project Calculator \u00b6 As part of the Sage-Steppe Habitat Quantification Tool, we developed a custom Excel form with VBA scripting to facilitate data entry into the Project Calculator. # Excel, VBA, Excel Forms Aliso Creek Results Chain Primer \u00b6 The Water Team developed a great introduction to Results Chains that can be tailored to any audience. Check it out below, or download here . # Open Standards, Results Chain, Performance Measures Performance Measure Design \u00b6 This slide deck introduces a five step process to performance measure design. It's a useful resource when introducing performance measures to new clients or developing proposals. # Open Standards, Results Chains, Performance Measures Qualitative Monitoring Dashboard \u00b6 The Qualitative Monitoring Dashboard reports progress markers for sustainable WASH (water, sanitation and hygiene) systems in Africa. The interactive dashboard allows users to select their municipality and view qualitative indicators and a sustainability scorecard. The dashboard was developed using InDesign before converting to HTML. It is regularly updated and hosted on the EI website (Note it contains sensitive information and is thus password protected). # dashboard, PDF, InDesign, qualitative indicators Central Valley Multi-Species HQT \u00b6 The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes. The HQT scoring for giant garter-snake is broken down in the diagram below. # Excel, field protocol","title":"Portfolio"},{"location":"portfolio/project-portfolio/#project-portfolio","text":"These are a few of our projects. Peruse at your leisure.","title":"Project Portfolio"},{"location":"portfolio/project-portfolio/#monarch-registry","text":"We worked with a web design firm to develop the Monarch Registry for the Monarch Habitat Exchange. Our role was to design the database, identify reporting metrics, and provide input on visual aesthetic. # custom web app, registry, database","title":"Monarch Registry"},{"location":"portfolio/project-portfolio/#nevada-credit-system-site-screening-tool","text":"The Nevada Site Screening Tool allows landowners to quickly screen their private lands for potential to participate in the Nevada Credit System based on a number of factors relevant to sage-grouse habitat quality. We partnered with Sitka to develop the web app, its algorithms, and outputs. # custom web app, spatial analysis","title":"Nevada Credit System Site Screening Tool"},{"location":"portfolio/project-portfolio/#roadside-managers-monarch-tool","text":"We partnered with UX design students to design an interface for a mobile field data collection app for roadside managers to quickly assess the quality of monarch habitat on roadsides. The design was used to spur interest from funders and as the basis for the final mobile field data collection app design, which was build on ESRI's Survey123. # UX design, Survey123, mobile app","title":"Roadside Managers Monarch Tool"},{"location":"portfolio/project-portfolio/#pollinator-habitat-quantification-tool","text":"The Pollinator Habitat Quantification Tool evaluates habitat quality in agricultural landscapes for a user-specified list of native pollinators. The tool implements an algorithm described here . The tool is presented in this ESRI StoryMap , which can be viewed below (scroll). # ESRI StoryMap, ArcGIS","title":"Pollinator Habitat Quantification Tool"},{"location":"portfolio/project-portfolio/#pollinator-scorecard","text":"The Pollinator Scorecard is designed for electric utilities and other rights-of-way managers to rapidly assess pollinator and monarch habitat on their rights-of-way. It is the required assessment method for the USFWS Monarch Candidate Conservation Agreement with Assurances. Data may be collected through a fillable PDF form or ESRI's Survey123 app. # Low-tech, PDF, Survey123, field protocol, datasheets","title":"Pollinator Scorecard"},{"location":"portfolio/project-portfolio/#san-diego-incentives-demographics","text":"We created a mapping application to inform the design of rebates and incentives program in San Diego County by evaluating basic demographic information. # ESRI StoryMap, Spatial Analysis, Mapping","title":"San Diego Incentives Demographics"},{"location":"portfolio/project-portfolio/#monarch-habitat-quantification-tool","text":"The Monarch Habitat Quantification Tool is designed to assess monarch habitat quality in agricultural landscapes. We led a team of monarch experts to develop an assessment protocol and habitat quantification approach for EDF. The tool includes an Excel-based spreadsheet calculator, field datasheets, and field data collection protocol. # Excel, Monitoring Protocol, Datasheets","title":"Monarch Habitat Quantification Tool"},{"location":"portfolio/project-portfolio/#sage-steppe-habitat-quantification-tool","text":"The Sage-Steppe Habitat Quantification Tool facilitates habitat assessment for greater sage-grouse and mule deer in multiple states around the West. Multiple state mitigation programs use a regionalized version of the tool. The tool has been used to inform conservation spending and mitigation of over $1 million (USD). The tool consists of a set of custom script tools using ESRI's ArcGIS API, arcpy and Excel spreadsheet calculators. # ArcGIS, arcpy, python, spatial analysis, Excel, VBA, Excel forms","title":"Sage-Steppe Habitat Quantification Tool"},{"location":"portfolio/project-portfolio/#idaho-greater-sage-grouse-project-calculator","text":"As part of the Sage-Steppe Habitat Quantification Tool, we developed a custom Excel form with VBA scripting to facilitate data entry into the Project Calculator. # Excel, VBA, Excel Forms","title":"Idaho Greater-Sage Grouse Project Calculator"},{"location":"portfolio/project-portfolio/#aliso-creek-results-chain-primer","text":"The Water Team developed a great introduction to Results Chains that can be tailored to any audience. Check it out below, or download here . # Open Standards, Results Chain, Performance Measures","title":"Aliso Creek Results Chain Primer"},{"location":"portfolio/project-portfolio/#performance-measure-design","text":"This slide deck introduces a five step process to performance measure design. It's a useful resource when introducing performance measures to new clients or developing proposals. # Open Standards, Results Chains, Performance Measures","title":"Performance Measure Design"},{"location":"portfolio/project-portfolio/#qualitative-monitoring-dashboard","text":"The Qualitative Monitoring Dashboard reports progress markers for sustainable WASH (water, sanitation and hygiene) systems in Africa. The interactive dashboard allows users to select their municipality and view qualitative indicators and a sustainability scorecard. The dashboard was developed using InDesign before converting to HTML. It is regularly updated and hosted on the EI website (Note it contains sensitive information and is thus password protected). # dashboard, PDF, InDesign, qualitative indicators","title":"Qualitative Monitoring Dashboard"},{"location":"portfolio/project-portfolio/#central-valley-multi-species-hqt","text":"The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes. The HQT scoring for giant garter-snake is broken down in the diagram below. # Excel, field protocol","title":"Central Valley Multi-Species HQT"},{"location":"portfolio/ggs-hqt/ggs-hqt/","text":"Giant Garter Snake Habitat Quantification Tool \u00b6 Excel-based Tool for Understanding Habitat Functionality \u00b6 The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes. Giant Garter Snakes \u00b6 Giant garter snakes were included in the original tool. This species, once common in the seasonal wetlands of the Central Valley, has seen drastic habitat reductions and is now listed as endangered under the Endangered Species Act and the California Endangered Species Act. These snakes now live in agricultural canals and rice fields, and are often impacted by standard maintenance activities, like dredging canals or grading levees. Because this species relies on human-created ecosystems and is heavily regulated by federal and state wildlife agencies, there is a great need to understand their habitat, be able to compare habitat sites, and demonstrate improvement of habitat for the snake. Benefits of the HQT \u00b6 The giant garter snake HQT was developed by Environmental Defense Fund and a Technical Advisory Committee of giant garter snake experts. EI helped to ensure the tool was specifically designed to be used by conservation organizations, agencies, and technically savvy farmers and landowners. As such, the tool is * Excel-based, with 13 attributes needed * Minor spatial analysis can be completed in ArcGIS or Google Earth * Includes management strategies that will likely lead to improved habitat * Combines landscape, regional, and site-specific attributes into one clear metric (% functionality) * Can be used on working lands and protected wetlands and everything in between We believe tools are only useful if they are used. Given the identified users of this tool, having a relatively simple excel sheet that completes the complicating weighting of attributes behind-the-scenes was critical for tool success. Adaptive Management & Proliferation \u00b6 We at EI were using the tool to help the California Department of Water Resources, who manages hundreds of acres of GGS habitat, demonstrate habitat improvements. Through this process, we identified several modification that would make the tool more usable and useful to meet its goals. Our team 1. Identified consequences of specific attribute weights in the tool that were leading to misaligned incentives 2. Developed a scenario analysis to understand the true impact of identified attributes 3. Underwent adaptive management process with the TAC to improve the tool so it properly weighted specific attributed 4. Trained DWR staff on use of the tool, including in-field data collection and desktop analyses 5. Worked with DWR to integrate the HQT into their standard practices","title":"Giant Garter Snake Habitat Quantification Tool"},{"location":"portfolio/ggs-hqt/ggs-hqt/#giant-garter-snake-habitat-quantification-tool","text":"","title":"Giant Garter Snake Habitat Quantification Tool"},{"location":"portfolio/ggs-hqt/ggs-hqt/#excel-based-tool-for-understanding-habitat-functionality","text":"The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes.","title":"Excel-based Tool for Understanding Habitat Functionality"},{"location":"portfolio/ggs-hqt/ggs-hqt/#giant-garter-snakes","text":"Giant garter snakes were included in the original tool. This species, once common in the seasonal wetlands of the Central Valley, has seen drastic habitat reductions and is now listed as endangered under the Endangered Species Act and the California Endangered Species Act. These snakes now live in agricultural canals and rice fields, and are often impacted by standard maintenance activities, like dredging canals or grading levees. Because this species relies on human-created ecosystems and is heavily regulated by federal and state wildlife agencies, there is a great need to understand their habitat, be able to compare habitat sites, and demonstrate improvement of habitat for the snake.","title":"Giant Garter Snakes"},{"location":"portfolio/ggs-hqt/ggs-hqt/#benefits-of-the-hqt","text":"The giant garter snake HQT was developed by Environmental Defense Fund and a Technical Advisory Committee of giant garter snake experts. EI helped to ensure the tool was specifically designed to be used by conservation organizations, agencies, and technically savvy farmers and landowners. As such, the tool is * Excel-based, with 13 attributes needed * Minor spatial analysis can be completed in ArcGIS or Google Earth * Includes management strategies that will likely lead to improved habitat * Combines landscape, regional, and site-specific attributes into one clear metric (% functionality) * Can be used on working lands and protected wetlands and everything in between We believe tools are only useful if they are used. Given the identified users of this tool, having a relatively simple excel sheet that completes the complicating weighting of attributes behind-the-scenes was critical for tool success.","title":"Benefits of the HQT"},{"location":"portfolio/ggs-hqt/ggs-hqt/#adaptive-management-proliferation","text":"We at EI were using the tool to help the California Department of Water Resources, who manages hundreds of acres of GGS habitat, demonstrate habitat improvements. Through this process, we identified several modification that would make the tool more usable and useful to meet its goals. Our team 1. Identified consequences of specific attribute weights in the tool that were leading to misaligned incentives 2. Developed a scenario analysis to understand the true impact of identified attributes 3. Underwent adaptive management process with the TAC to improve the tool so it properly weighted specific attributed 4. Trained DWR staff on use of the tool, including in-field data collection and desktop analyses 5. Worked with DWR to integrate the HQT into their standard practices","title":"Adaptive Management &amp; Proliferation"},{"location":"portfolio/utilization-report/utilization-report/","text":"Utilization Report \u00b6 The EI Utilization Report helps inform weekly planning by providing up-to-date utilization data for all EI staff. # python, heroku, streamlit, pandas, matplotlib, google api, google sheets, data viz","title":"Utilization Report"},{"location":"portfolio/utilization-report/utilization-report/#utilization-report","text":"The EI Utilization Report helps inform weekly planning by providing up-to-date utilization data for all EI staff. # python, heroku, streamlit, pandas, matplotlib, google api, google sheets, data viz","title":"Utilization Report"},{"location":"project-organization/project-planning/","text":"Getting Started \u00b6 Before beginning any project, start by thinking it through. What technologies are required? What existing frameworks or packages are available? How will the project be deployed? Who are the users? Is the project worth the effort? How long will it take? How will the project be supported? These are the high-level questions that will shape the direction and scope of the project. Use the EI Data Driven Product - Product Definition to get started. Next, think about the general approach. What will the architecture of the project be? Which specific packages will be used? Should you set up a virtual environment? What will the user interface look like? How will the backend be managed? What testing approach will be used? What conventions will be used for file and folder naming, code style, etc.? Draft a Tool Specifications document if warranted (i.e., for large, billable projects). See the draft Tool Specifications outline in the EI Data Driven Product - Product Definition. You should have a clear plan in writing before starting with the first line of code. Example Project Plan \u00b6 Here's an example project plan for this project: Solution Description \u00b6 A web-based wiki for capturing and organizing information important to developing metrics products with a focus on performance-driven conservation programs. Goal & Objectives \u00b6 Goal \u00b6 Create a single source for documenting and sharing EI's approach to metrics product development and best practices for current and future staff. Objectives \u00b6 Compile all existing resources for data product development and create single platform for accumulating new resources. Present information in user-friendly format that balances instructional content with requirements for illustrating in-line code. Require staff focused on product development to work with technologies that will be used in deploying EI data products. Users \u00b6 Primary \u00b6 EI's metrics staff and other technical staff (e.g., Erik, Kristen, Maso) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Metrics staff have developed standard processes and must learn new technologies frequently to deliver metrics products. Until now, this information has been scattered in multiple locations or lost entirely. New information that is learned or \u200b Value Propositions: This product will provide a common location for storing information and allow metrics staff to quickly access said information and re-familiarize themselves with important information quickly. Secondary \u00b6 Non-technical staff working with metrics staff to develop a data product (e.g., Kelsey) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Non-technical staff are unaware of the capabilities available to them from metrics service line staff or are uncertain as to how to utilize those capabilities for their projects, both internal and external. \u200b Value Propositions: This product will include overviews of available services and technologies, as well as project examples, that non-technical staff can peruse at their leisure. Metrics staff can also use the wiki to introduce options and explain our engagement process. Use Cases \u00b6 User Stories \u00b6 As the metrics service line lead, I want to ______ so I can ______: Direct staff to a common resource so I can limit the time needed to train new staff on commonly used technologies and best practices. Encourage metrics staff to work with technologies like git and the command line so I can better integrate them into our workflows. As a metrics/technical staff, I want to ______ so I can ______: Quickly relearn previously used technologies so I can employ them in new projects with minimal spin up time. Communicate to non-metrics staff the options they have for a specific product, visualization or analysis so I can more quickly scope their projects. Capture and share new technologies, ideas, and other information as I discover it so I can reference it later and promote its use. As a non-metrics EI staff, I want to ______ so I can ______: Better understand the capabilities of EI staff to support on technology-based products so I can pitch them to clients or develop them for internal uses. Better understand the process for working with metrics service line staff so I can assess the feasibility of the product I'm considering. Product Sketch \u00b6 [Story Board, what did you learn from storyboarding?] Design Principles & Constraints \u00b6 (list 'em. Consider integration, hosting, support, adaptive management, deployment but don't get into details yet) Conditions of Satisfaction \u00b6 (How will we know this tool is successful?) Approach \u00b6 (Roles, Timeline, Resources required) As you can see, lots to be done! The folder order is roughly the prioritization for these pages. Thus, my approach will be to work through these pages in roughly this order. One objective for this project is to provide a place to store new information as it becomes available, so I'll create the above folders initially as markdown files of the same name within a 'tbd' folder where I can store links and other references as I come across them. I've also built some of the pages above in other formats (Evernote, Google Docs, Jupyter Notebooks, etc.) so I can now pull everything together into one place. Optional: Situation Model \u00b6 Optional: Results Chain \u00b6 \u00b6 (Move to Specs) \u00b6 Technologies: \u00b6 MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE Git - version control Github - Repository Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc. Architecture \u00b6 The MkDocs package will create the basic architecture when creating the project . After creating the project, a mkdocs.yml file will be created. A docs folder will also be created with an index.md file within it. The index.md file manages the site outline; the mkdocs.yml file manages the settings. I'll add a README.md file in the root folder that will show up on the Github repo page. Files and folders can be created within the docs folder to create the project pages. Here's the file structure proposed within the root folder; the folder structure will mirror the site outline: mkdocs.yml # describes how the site is organized, enables features README.md # instructions for accessing and publishing docs/ index.md # home page how-we-work/ consulting-process.md program-requirements.md conservation-design/ project-planning/ file-organization-and-naming.md project-planning.md specifications-outline.md skills-and-training.md git/ installing-git.md initializing-git.md using-git.md development/ virtual-environments.md IDEs.md data-science-workflow.md deployment/ deployment-overview.md jupyter.md heroku.md linux.md aws.md docker.md data-management/ database-overview.md data-science/ workflow-overview.md data-exploration.md data-analysis.md data-visualization.md (e.g., inline exploratory) spatial-analysis/ earth-observation.md google-earth-engine.md gdal.md arcpy.md land-use-land-cover.md dashboards/ visualization/ (e.g. report quality) packages/ dash pandas seaborn folium sqlite rasterio Other Considerations \u00b6 Keep track of concerns and other considerations as you go and revisit the specifications periodically to ensure the best approach has been taken. How will this tech stack allow for illustrating using code? Can code be run within the deployment environment, or will static code blocks and outputs be needed? How often should links to a Jupyter Notebook, for example, be used as opposed to illustrating static code? Is there a good way to surface content for non-technical staff that are interested in these services or are asked by the metrics staff to, for example, complete a product definition? Or should the users be limited to technical staff only?","title":"Project Planning"},{"location":"project-organization/project-planning/#getting-started","text":"Before beginning any project, start by thinking it through. What technologies are required? What existing frameworks or packages are available? How will the project be deployed? Who are the users? Is the project worth the effort? How long will it take? How will the project be supported? These are the high-level questions that will shape the direction and scope of the project. Use the EI Data Driven Product - Product Definition to get started. Next, think about the general approach. What will the architecture of the project be? Which specific packages will be used? Should you set up a virtual environment? What will the user interface look like? How will the backend be managed? What testing approach will be used? What conventions will be used for file and folder naming, code style, etc.? Draft a Tool Specifications document if warranted (i.e., for large, billable projects). See the draft Tool Specifications outline in the EI Data Driven Product - Product Definition. You should have a clear plan in writing before starting with the first line of code.","title":"Getting Started"},{"location":"project-organization/project-planning/#example-project-plan","text":"Here's an example project plan for this project:","title":"Example Project Plan"},{"location":"project-organization/project-planning/#solution-description","text":"A web-based wiki for capturing and organizing information important to developing metrics products with a focus on performance-driven conservation programs.","title":"Solution Description"},{"location":"project-organization/project-planning/#goal-objectives","text":"","title":"Goal &amp; Objectives"},{"location":"project-organization/project-planning/#goal","text":"Create a single source for documenting and sharing EI's approach to metrics product development and best practices for current and future staff.","title":"Goal"},{"location":"project-organization/project-planning/#objectives","text":"Compile all existing resources for data product development and create single platform for accumulating new resources. Present information in user-friendly format that balances instructional content with requirements for illustrating in-line code. Require staff focused on product development to work with technologies that will be used in deploying EI data products.","title":"Objectives"},{"location":"project-organization/project-planning/#users","text":"","title":"Users"},{"location":"project-organization/project-planning/#primary","text":"EI's metrics staff and other technical staff (e.g., Erik, Kristen, Maso) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Metrics staff have developed standard processes and must learn new technologies frequently to deliver metrics products. Until now, this information has been scattered in multiple locations or lost entirely. New information that is learned or \u200b Value Propositions: This product will provide a common location for storing information and allow metrics staff to quickly access said information and re-familiarize themselves with important information quickly.","title":"Primary"},{"location":"project-organization/project-planning/#secondary","text":"Non-technical staff working with metrics staff to develop a data product (e.g., Kelsey) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Non-technical staff are unaware of the capabilities available to them from metrics service line staff or are uncertain as to how to utilize those capabilities for their projects, both internal and external. \u200b Value Propositions: This product will include overviews of available services and technologies, as well as project examples, that non-technical staff can peruse at their leisure. Metrics staff can also use the wiki to introduce options and explain our engagement process.","title":"Secondary"},{"location":"project-organization/project-planning/#use-cases","text":"","title":"Use Cases"},{"location":"project-organization/project-planning/#user-stories","text":"As the metrics service line lead, I want to ______ so I can ______: Direct staff to a common resource so I can limit the time needed to train new staff on commonly used technologies and best practices. Encourage metrics staff to work with technologies like git and the command line so I can better integrate them into our workflows. As a metrics/technical staff, I want to ______ so I can ______: Quickly relearn previously used technologies so I can employ them in new projects with minimal spin up time. Communicate to non-metrics staff the options they have for a specific product, visualization or analysis so I can more quickly scope their projects. Capture and share new technologies, ideas, and other information as I discover it so I can reference it later and promote its use. As a non-metrics EI staff, I want to ______ so I can ______: Better understand the capabilities of EI staff to support on technology-based products so I can pitch them to clients or develop them for internal uses. Better understand the process for working with metrics service line staff so I can assess the feasibility of the product I'm considering.","title":"User Stories"},{"location":"project-organization/project-planning/#product-sketch","text":"[Story Board, what did you learn from storyboarding?]","title":"Product Sketch"},{"location":"project-organization/project-planning/#design-principles-constraints","text":"(list 'em. Consider integration, hosting, support, adaptive management, deployment but don't get into details yet)","title":"Design Principles &amp; Constraints"},{"location":"project-organization/project-planning/#conditions-of-satisfaction","text":"(How will we know this tool is successful?)","title":"Conditions of Satisfaction"},{"location":"project-organization/project-planning/#approach","text":"(Roles, Timeline, Resources required) As you can see, lots to be done! The folder order is roughly the prioritization for these pages. Thus, my approach will be to work through these pages in roughly this order. One objective for this project is to provide a place to store new information as it becomes available, so I'll create the above folders initially as markdown files of the same name within a 'tbd' folder where I can store links and other references as I come across them. I've also built some of the pages above in other formats (Evernote, Google Docs, Jupyter Notebooks, etc.) so I can now pull everything together into one place.","title":"Approach"},{"location":"project-organization/project-planning/#optional-situation-model","text":"","title":"Optional: Situation Model"},{"location":"project-organization/project-planning/#optional-results-chain","text":"","title":"Optional: Results Chain"},{"location":"project-organization/project-planning/#move-to-specs","text":"","title":"(Move to Specs)"},{"location":"project-organization/project-planning/#technologies","text":"MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE Git - version control Github - Repository Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc.","title":"Technologies:"},{"location":"project-organization/project-planning/#architecture","text":"The MkDocs package will create the basic architecture when creating the project . After creating the project, a mkdocs.yml file will be created. A docs folder will also be created with an index.md file within it. The index.md file manages the site outline; the mkdocs.yml file manages the settings. I'll add a README.md file in the root folder that will show up on the Github repo page. Files and folders can be created within the docs folder to create the project pages. Here's the file structure proposed within the root folder; the folder structure will mirror the site outline: mkdocs.yml # describes how the site is organized, enables features README.md # instructions for accessing and publishing docs/ index.md # home page how-we-work/ consulting-process.md program-requirements.md conservation-design/ project-planning/ file-organization-and-naming.md project-planning.md specifications-outline.md skills-and-training.md git/ installing-git.md initializing-git.md using-git.md development/ virtual-environments.md IDEs.md data-science-workflow.md deployment/ deployment-overview.md jupyter.md heroku.md linux.md aws.md docker.md data-management/ database-overview.md data-science/ workflow-overview.md data-exploration.md data-analysis.md data-visualization.md (e.g., inline exploratory) spatial-analysis/ earth-observation.md google-earth-engine.md gdal.md arcpy.md land-use-land-cover.md dashboards/ visualization/ (e.g. report quality) packages/ dash pandas seaborn folium sqlite rasterio","title":"Architecture"},{"location":"project-organization/project-planning/#other-considerations","text":"Keep track of concerns and other considerations as you go and revisit the specifications periodically to ensure the best approach has been taken. How will this tech stack allow for illustrating using code? Can code be run within the deployment environment, or will static code blocks and outputs be needed? How often should links to a Jupyter Notebook, for example, be used as opposed to illustrating static code? Is there a good way to surface content for non-technical staff that are interested in these services or are asked by the metrics staff to, for example, complete a product definition? Or should the users be limited to technical staff only?","title":"Other Considerations"},{"location":"python-packages/dash/","text":"Dash \u00b6 Dash is a Python framework for building analytic web applications. As it's name suggests, it's especially good for building interactive dashboards. The strengths of this package lie in how it bridges the gaps between your app's analytic functionality, the web, and your user. Dash obviously integrates with Python and all of its functionality, but also provides a fairly intuitive interface to interact with the DOM (more on that below) and includes user interface components (i.e., widgets ) that invite interactivity. I recommend taking a day to work through the examples in the User Guide and exploring what's possible in the gallery . Once you've done that and built your first app, you can come back here for a quick refresher. Also worth noting that Dash 1.0 is relatively new, and a lot of the guidance you'll find in forums and on YouTube is outdated, so start with the official documentation and the very helpful Dash community . The DOM DOM is short for document-object-model. The DOM is how web browsers display information on web pages. The DOM is represented by HTML (Hyper-Text Markup Language). Having a decent grasp on HTML will help you as you customize your Dash app. Check out w3schools or codeacademy for a crash course. You don't need to know everything, but it's helpful to know how Dash is converting your Python code to HTML, and the options you have through Dash's HTML components . Installation \u00b6 Dash isn't distributed through conda (yet), so you'll need to install using pip . Might as well initialize a git repository and create a virtual environment while you're at it. Use virtualenv since this is a pip environment and that's what the Dash documentation suggests. Make sure your prompt is in the root folder of your project directory (or mkdir and cd into it). 1 2 3 4 virtualenv venv source venv / bin / activate pip install dash == 1 . 8 . 0 # check the docs for the latest version pip install plotly Here are the files they recommend including in your .gitignore file. 1 2 3 4 venv *.pyc .DS_Store .env Patterns \u00b6 A Dash app consists of a layout, which describes the organization of the app, and callbacks, which create interactive functionality. Layout \u00b6 The Dash layout consists of both Core Components and HTML Components . Core components represent your user interface widgets. HTML components expose the HTML tags used in the DOM as Python classes. Here's a basic template layout to get you started. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # -*- coding: utf-8 -*- import dash import dash_core_components as dcc import dash_html_components as html external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ( children = [ # YOUR APP GOES HERE ]) if __name__ == '__main__' : app . run_server ( debug = True ) Within the app.layout variable, you'll define all of your core components and html components within a div HTML element as a list. app.layout essentially defines the body of the DOM. Everything you want on your webpage must be included here. The external stylesheet provides styling through CSS to make your app look a bit better. You can also provide custom CSS and custom JavaScript . Core Components \u00b6 Dash's core components facilitate user interaction and include dropdowns, sliders, a variety of inputs, checkboxes, radio items, buttons, date pickers, file uploaders, tabs, dialogs, and graphs (i.e., charts). The tables component , recently released, allows for interacting with and even editing tables. As well, you can pass Markdown through the Markdown component. There are a few additional libraries of components available, check out the list under Open Source Component Library (I recommend checking out Dash DAQ library). Each component also has an array of properties that can be set (or updated through callbacks), refer to each component's documentation. HTML Components \u00b6 The HTML components provide all of the other functionality you'll need to create a web page by exposing a majority of the available html tags. You'll most often be using html.Div to create areas within the web page to show widgets, content, and even to store information without showing it. Styling \u00b6 The CSS provided by the external stylesheet offers some basic styling and a grid feature. Check out the codepen to see the documentation and example styles available to you. To use the CSS, you need to assign a class to the component. You can do this by including a variable className in the list of component properties. Convert any dots ( . ) in the CSS to spaces in your code. For example, a primary button ( primary.button ) is created with: 1 2 3 4 5 html . Button ( children = 'Click me!' , id = 'my_button' , className = 'primary button' ) Grids \u00b6 To create side-by-side elements, access the grid. First, create a div where you will include the side-by-side elements. Assign to each component the number of 'columns' you want the component to occupy out of a possible twelve. Assign to the div element the class name of 'row'. This example would create two side-by-side buttons, each occupying one half of the screen: 1 2 3 4 5 6 7 8 9 html . Div ([ html . Button ( 'Submit' , id = 'submit-button' , className = 'six rows' ), html . Button ( 'Undo' , id = 'undo-button' , className = 'six rows' ) ], className = 'row' ) Bootstrap \u00b6 Another stylesheet makes available Bootstrap (a popular user interface library for the web), called dash-bootstrap-components . This stylesheet changes the development patterns with new syntax for the layout, so decide if you want to use Bootstrap before getting too far in development. Callbacks \u00b6 Callbacks handle the interactivity of the app. Callbacks listen for events, like clicking on a button, and then handle the action required to make the button do it's job. You can update almost any property of any component through a callback. For example, you might want to update the figure property of a dcc.Graph component based on the value of a slider or other input. In other words, you will change the way the graph looks whenever the user interacts with the relevant widget. To create a callback, import Input and Output from dash.dependencies , use the callback decorator, provide one or more outputs and one or more inputs, and then define a function that handles the interaction. 1 2 3 4 5 6 7 8 9 from dash.dependencies import Input , Output # Put callbacks below the layout or in a separate file @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): return first_input ^ 2 If you have more than one output, they should be passed as a list. The inputs need to be included in a list, regardless of how many there are. The inputs are passed to the function in the order they are listed. Two parameters are passed to the Output or Input , first the id of the component and second the property of interest. Thus, any component that supports interactivity must have an id property assigned (also, all components in your callbacks must also be represented in the layout, or you'll get an error). Callbacks can also be chained to create a cascade of changes to create dynamic UIs where one input component updates the available options of another input component. Provide the Output of one callback as the Input of another callback. Note that you won't provide the value for the Output components you will be updating within app.layout . This is because Dash automatically fires all of the callbacks upon page load, so anything you provide up front will simply be overwritten. For example, if you have a callback that updates the figure of a dcc.Graph component, don't include a figure= property for that graph in the app.layout section, you only need provide it an id= property. Warning Don't update variables that are outside of the callback's scope, as this can introduce unusual behavior. State \u00b6 Callbacks fire whenever there is a change to the input component(s). You might instead want to, say, wait for the user to finish updating a form before you fire the callbacks associated with those input fields. For this, you can use a State variable. Include State variables as you would Input variables, in a list below the Input variables. 1 2 3 4 5 6 7 8 9 from dash.dependencies import Input , Output , State @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )], [ State ( 'my-input-state' , 'value' )] ) def my_function ( first_input , state_input ): return first_input ^ state_input This example raises the value of the my-input component by the my-input-state component, which allows the user to specify the power to raise to without re-calculating all of the outputs. The result is used to update the children property of the my-output component (children properties are common for text outputs, so this would print the result to the screen). PreventUpdate & NoUpdate \u00b6 If you want to include logic that prevents updating a component based on the property of some input, you can use PreventUpdate . 1 2 3 4 5 6 7 8 9 10 11 from dash.exceptions import PreventUpdate @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): if first_input is None : raise PreventUpdate else : return first_input ^ 2 Here, if the input is not filled in, the output component will not be updated. Alternatively, you can use dash.no_update to tell Dash to skip updating a specific output based on an input value. For example, if you want to update one output, but not another, based on an input, return dash.no_update for that output. Interactive Graphing \u00b6 One really great feature of Dash is its ability to update multiple graphs based on the built-in interactivity of Plotly figures. Plotly figures support hovering, clicking, selecting, and zooming on charts and their data. Using Dash, you can update any component based on the user's interaction with the graph. This is probably best illustrated. Note the charts on the right are updated based on where the cursor hovers. See the documentation for details. Running the app \u00b6 Run the app in the terminal with: 1 python < app . py > where <app.py> is the path to your python script containing the application. By custom, this is called app.py . Visit localhost:8050 in your browser (or whichever port the terminal states). Deployment \u00b6 While Dash does provide a paid hosting platform, it's better to host on Heroku. Warning Don't deploy with Debut set to True (In app.py, set app.run_server(debug=False)) Heroku \u00b6 Check out our Heroku tutorial for general guidance and the step-by-step tutorial in the official documentation (under Heroku example ). There are a few configuration options and gotchas specific to Dash you'll need to know about that are covered in the tutorial. Flask \u00b6 Dash is written on top of flask , and so integrates fairly easily with Flask applications. Deploying your Flask application is exactly the same as deploying your Dash application. Embedding in existing websites \u00b6 Dash apps (and any other apps hosted through Heroku) can be inserted into a webpage using the iframe tag. 1 < iframe src = \"<path-to-my-app.com>\" , height = 800, width = 700 ></ iframe > Check out the Utilization Report embedded in this site for an example. Note that with the free tier, the page may timeout before the app is spun up and loaded. You'll need to reload the site, which isn't the best user experience, so consider upgrading any app that you want to embed in this way. Tips & Tricks \u00b6 Storing data & performance \u00b6 If your app includes expensive analytics, you may want to separate that process from the interactivity of your app to avoid expensive computations with every change of inputs. One option is to store the data in a hidden div . This will store the data in the user's browser for the duration of their session. Data has to be converted to a string like JSON for storage. You can either store the fully computed data or you can perform some computation, save the intermediate results, and then with a chained callback access that information to complete the interaction. The pre-processed data will be available to subsequent callbacks where you stored it. Finally, you can cache the data using flask . This will also allow for persisting the data between sessions. Once you're ready to worry about optimizing performance, see the docs . Multi-page Apps \u00b6 If you need basic functionality for multiple pages, check out the Tabs core component. If you really need a multi-page app, you can either embed multiple Dash apps in a Django or Flask app, or check out this guidance . I'd also recommend this helpful Towards Data Science article. Extending \u00b6 You can write your own Dash components in React JS, a popular JavaScript library for building user interfaces. You'll need to fork the sample component library and, of course, be comfortable working with React JS. Custom CSS \u00b6 To add your own CSS, save the CSS files in a folder named assets in the project's root directory. Dash will automatically serve any CSS it finds here. Note that the CSS files are added in alphanumeric order, so if the order matters, prepend numeric values to the filenames to get the loading order you want. You can also copy the provided CSS into this folder and edit it. Custom JavaScript \u00b6 To add custom JavaScript, simply include a JavaScript file in the assets folder in the project's root directory. Dash will automatically include it. However, they recommend building new Dash components for any significant functionality you want to add.","title":"Dash"},{"location":"python-packages/dash/#dash","text":"Dash is a Python framework for building analytic web applications. As it's name suggests, it's especially good for building interactive dashboards. The strengths of this package lie in how it bridges the gaps between your app's analytic functionality, the web, and your user. Dash obviously integrates with Python and all of its functionality, but also provides a fairly intuitive interface to interact with the DOM (more on that below) and includes user interface components (i.e., widgets ) that invite interactivity. I recommend taking a day to work through the examples in the User Guide and exploring what's possible in the gallery . Once you've done that and built your first app, you can come back here for a quick refresher. Also worth noting that Dash 1.0 is relatively new, and a lot of the guidance you'll find in forums and on YouTube is outdated, so start with the official documentation and the very helpful Dash community . The DOM DOM is short for document-object-model. The DOM is how web browsers display information on web pages. The DOM is represented by HTML (Hyper-Text Markup Language). Having a decent grasp on HTML will help you as you customize your Dash app. Check out w3schools or codeacademy for a crash course. You don't need to know everything, but it's helpful to know how Dash is converting your Python code to HTML, and the options you have through Dash's HTML components .","title":"Dash"},{"location":"python-packages/dash/#installation","text":"Dash isn't distributed through conda (yet), so you'll need to install using pip . Might as well initialize a git repository and create a virtual environment while you're at it. Use virtualenv since this is a pip environment and that's what the Dash documentation suggests. Make sure your prompt is in the root folder of your project directory (or mkdir and cd into it). 1 2 3 4 virtualenv venv source venv / bin / activate pip install dash == 1 . 8 . 0 # check the docs for the latest version pip install plotly Here are the files they recommend including in your .gitignore file. 1 2 3 4 venv *.pyc .DS_Store .env","title":"Installation"},{"location":"python-packages/dash/#patterns","text":"A Dash app consists of a layout, which describes the organization of the app, and callbacks, which create interactive functionality.","title":"Patterns"},{"location":"python-packages/dash/#layout","text":"The Dash layout consists of both Core Components and HTML Components . Core components represent your user interface widgets. HTML components expose the HTML tags used in the DOM as Python classes. Here's a basic template layout to get you started. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # -*- coding: utf-8 -*- import dash import dash_core_components as dcc import dash_html_components as html external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ( children = [ # YOUR APP GOES HERE ]) if __name__ == '__main__' : app . run_server ( debug = True ) Within the app.layout variable, you'll define all of your core components and html components within a div HTML element as a list. app.layout essentially defines the body of the DOM. Everything you want on your webpage must be included here. The external stylesheet provides styling through CSS to make your app look a bit better. You can also provide custom CSS and custom JavaScript .","title":"Layout"},{"location":"python-packages/dash/#core-components","text":"Dash's core components facilitate user interaction and include dropdowns, sliders, a variety of inputs, checkboxes, radio items, buttons, date pickers, file uploaders, tabs, dialogs, and graphs (i.e., charts). The tables component , recently released, allows for interacting with and even editing tables. As well, you can pass Markdown through the Markdown component. There are a few additional libraries of components available, check out the list under Open Source Component Library (I recommend checking out Dash DAQ library). Each component also has an array of properties that can be set (or updated through callbacks), refer to each component's documentation.","title":"Core Components"},{"location":"python-packages/dash/#html-components","text":"The HTML components provide all of the other functionality you'll need to create a web page by exposing a majority of the available html tags. You'll most often be using html.Div to create areas within the web page to show widgets, content, and even to store information without showing it.","title":"HTML Components"},{"location":"python-packages/dash/#styling","text":"The CSS provided by the external stylesheet offers some basic styling and a grid feature. Check out the codepen to see the documentation and example styles available to you. To use the CSS, you need to assign a class to the component. You can do this by including a variable className in the list of component properties. Convert any dots ( . ) in the CSS to spaces in your code. For example, a primary button ( primary.button ) is created with: 1 2 3 4 5 html . Button ( children = 'Click me!' , id = 'my_button' , className = 'primary button' )","title":"Styling"},{"location":"python-packages/dash/#grids","text":"To create side-by-side elements, access the grid. First, create a div where you will include the side-by-side elements. Assign to each component the number of 'columns' you want the component to occupy out of a possible twelve. Assign to the div element the class name of 'row'. This example would create two side-by-side buttons, each occupying one half of the screen: 1 2 3 4 5 6 7 8 9 html . Div ([ html . Button ( 'Submit' , id = 'submit-button' , className = 'six rows' ), html . Button ( 'Undo' , id = 'undo-button' , className = 'six rows' ) ], className = 'row' )","title":"Grids"},{"location":"python-packages/dash/#bootstrap","text":"Another stylesheet makes available Bootstrap (a popular user interface library for the web), called dash-bootstrap-components . This stylesheet changes the development patterns with new syntax for the layout, so decide if you want to use Bootstrap before getting too far in development.","title":"Bootstrap"},{"location":"python-packages/dash/#callbacks","text":"Callbacks handle the interactivity of the app. Callbacks listen for events, like clicking on a button, and then handle the action required to make the button do it's job. You can update almost any property of any component through a callback. For example, you might want to update the figure property of a dcc.Graph component based on the value of a slider or other input. In other words, you will change the way the graph looks whenever the user interacts with the relevant widget. To create a callback, import Input and Output from dash.dependencies , use the callback decorator, provide one or more outputs and one or more inputs, and then define a function that handles the interaction. 1 2 3 4 5 6 7 8 9 from dash.dependencies import Input , Output # Put callbacks below the layout or in a separate file @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): return first_input ^ 2 If you have more than one output, they should be passed as a list. The inputs need to be included in a list, regardless of how many there are. The inputs are passed to the function in the order they are listed. Two parameters are passed to the Output or Input , first the id of the component and second the property of interest. Thus, any component that supports interactivity must have an id property assigned (also, all components in your callbacks must also be represented in the layout, or you'll get an error). Callbacks can also be chained to create a cascade of changes to create dynamic UIs where one input component updates the available options of another input component. Provide the Output of one callback as the Input of another callback. Note that you won't provide the value for the Output components you will be updating within app.layout . This is because Dash automatically fires all of the callbacks upon page load, so anything you provide up front will simply be overwritten. For example, if you have a callback that updates the figure of a dcc.Graph component, don't include a figure= property for that graph in the app.layout section, you only need provide it an id= property. Warning Don't update variables that are outside of the callback's scope, as this can introduce unusual behavior.","title":"Callbacks"},{"location":"python-packages/dash/#state","text":"Callbacks fire whenever there is a change to the input component(s). You might instead want to, say, wait for the user to finish updating a form before you fire the callbacks associated with those input fields. For this, you can use a State variable. Include State variables as you would Input variables, in a list below the Input variables. 1 2 3 4 5 6 7 8 9 from dash.dependencies import Input , Output , State @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )], [ State ( 'my-input-state' , 'value' )] ) def my_function ( first_input , state_input ): return first_input ^ state_input This example raises the value of the my-input component by the my-input-state component, which allows the user to specify the power to raise to without re-calculating all of the outputs. The result is used to update the children property of the my-output component (children properties are common for text outputs, so this would print the result to the screen).","title":"State"},{"location":"python-packages/dash/#preventupdate-noupdate","text":"If you want to include logic that prevents updating a component based on the property of some input, you can use PreventUpdate . 1 2 3 4 5 6 7 8 9 10 11 from dash.exceptions import PreventUpdate @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): if first_input is None : raise PreventUpdate else : return first_input ^ 2 Here, if the input is not filled in, the output component will not be updated. Alternatively, you can use dash.no_update to tell Dash to skip updating a specific output based on an input value. For example, if you want to update one output, but not another, based on an input, return dash.no_update for that output.","title":"PreventUpdate &amp; NoUpdate"},{"location":"python-packages/dash/#interactive-graphing","text":"One really great feature of Dash is its ability to update multiple graphs based on the built-in interactivity of Plotly figures. Plotly figures support hovering, clicking, selecting, and zooming on charts and their data. Using Dash, you can update any component based on the user's interaction with the graph. This is probably best illustrated. Note the charts on the right are updated based on where the cursor hovers. See the documentation for details.","title":"Interactive Graphing"},{"location":"python-packages/dash/#running-the-app","text":"Run the app in the terminal with: 1 python < app . py > where <app.py> is the path to your python script containing the application. By custom, this is called app.py . Visit localhost:8050 in your browser (or whichever port the terminal states).","title":"Running the app"},{"location":"python-packages/dash/#deployment","text":"While Dash does provide a paid hosting platform, it's better to host on Heroku. Warning Don't deploy with Debut set to True (In app.py, set app.run_server(debug=False))","title":"Deployment"},{"location":"python-packages/dash/#heroku","text":"Check out our Heroku tutorial for general guidance and the step-by-step tutorial in the official documentation (under Heroku example ). There are a few configuration options and gotchas specific to Dash you'll need to know about that are covered in the tutorial.","title":"Heroku"},{"location":"python-packages/dash/#flask","text":"Dash is written on top of flask , and so integrates fairly easily with Flask applications. Deploying your Flask application is exactly the same as deploying your Dash application.","title":"Flask"},{"location":"python-packages/dash/#embedding-in-existing-websites","text":"Dash apps (and any other apps hosted through Heroku) can be inserted into a webpage using the iframe tag. 1 < iframe src = \"<path-to-my-app.com>\" , height = 800, width = 700 ></ iframe > Check out the Utilization Report embedded in this site for an example. Note that with the free tier, the page may timeout before the app is spun up and loaded. You'll need to reload the site, which isn't the best user experience, so consider upgrading any app that you want to embed in this way.","title":"Embedding in existing websites"},{"location":"python-packages/dash/#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"python-packages/dash/#storing-data-performance","text":"If your app includes expensive analytics, you may want to separate that process from the interactivity of your app to avoid expensive computations with every change of inputs. One option is to store the data in a hidden div . This will store the data in the user's browser for the duration of their session. Data has to be converted to a string like JSON for storage. You can either store the fully computed data or you can perform some computation, save the intermediate results, and then with a chained callback access that information to complete the interaction. The pre-processed data will be available to subsequent callbacks where you stored it. Finally, you can cache the data using flask . This will also allow for persisting the data between sessions. Once you're ready to worry about optimizing performance, see the docs .","title":"Storing data &amp; performance"},{"location":"python-packages/dash/#multi-page-apps","text":"If you need basic functionality for multiple pages, check out the Tabs core component. If you really need a multi-page app, you can either embed multiple Dash apps in a Django or Flask app, or check out this guidance . I'd also recommend this helpful Towards Data Science article.","title":"Multi-page Apps"},{"location":"python-packages/dash/#extending","text":"You can write your own Dash components in React JS, a popular JavaScript library for building user interfaces. You'll need to fork the sample component library and, of course, be comfortable working with React JS.","title":"Extending"},{"location":"python-packages/dash/#custom-css","text":"To add your own CSS, save the CSS files in a folder named assets in the project's root directory. Dash will automatically serve any CSS it finds here. Note that the CSS files are added in alphanumeric order, so if the order matters, prepend numeric values to the filenames to get the loading order you want. You can also copy the provided CSS into this folder and edit it.","title":"Custom CSS"},{"location":"python-packages/dash/#custom-javascript","text":"To add custom JavaScript, simply include a JavaScript file in the assets folder in the project's root directory. Dash will automatically include it. However, they recommend building new Dash components for any significant functionality you want to add.","title":"Custom JavaScript"}]}